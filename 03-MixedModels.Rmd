# Linear Mixed Models

## ANOVA with random factors

The simplest linear mixed models are used to analyze linear models for one-way ANOVA, randomized complete block designs, and two-way ANOVA.  *Mixed* models as opposed to *fixed* models (the linear models you have heretofore studied) are needed when factors have levels that are **random**.  Random levels occur whenever the units making up those levels behave like random samples from a population.  Two examples are given below.  And, we discuss how to perform ANOVA-like tests for factors that are random rather than fixed.  The upshot is that (at least for balanced experiments/datasets) the tests for fixed effects are identical to those for random effects, only the interpretation is different (and importantly so!).

### Strength of metallic bonds

The dataset below, called "Bonds", contains responses for 21 samples of metals, 7 each for iron, nickel, and copper, that quantify the strength of metallic bonds.  One sample from each metal was extracted from each of 7 ingots.  We expect ingots to act like blocks---differences in ingots account for a substantial amount of variability in the responses, but the precise block effects are of no inferential/scientific inference.  We only include the blocks in order to reduce the residual variance after accounting for block variance.  A randomized controlled block design describes how this data was collected, but, if we repeated the experiment, the blocks (ingots) would be completely different.  That is, the blocks are not *fixed* but *random*.  Rather than estimating block effects that would surely change experiment to experiment, we should focus on estimating the amount of variability explained by the blocks, which should remain about the same experiment to experiment.  This suggests a different model than used to analyze RCBD experiments with fixed blocks.<br><br>

The usual linear model for fixed blocks is
\[y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ij},\]
where $y_{ij}$ is the response for treatment (metal) $i$ in block (ingot) $j$; the $\alpha_i$'s are the metal (treatment) effects; the $\beta_j$'s are the ingot (block) effects; and, $\epsilon_{ij}\stackrel{iid}{\sim}N(0,\sigma^2)$ are the random residuals.<br><br>

The above linear model is the wrong model for this data because the block effects (and, hence, also the interaction effects) are meaningless outside of the given data set; these are not population-level parameters because the blocks are random rather than fixed.  The appropriate model (given normality and independence of random residuals is reasonable) is the following *mixed effects model*:

\begin{equation}
y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij},
 (\#eq:fullmodel)
\end{equation}
where $\beta_j\stackrel{iid}{\sim}N(0, \sigma_b^2)$ and, independently, $\epsilon_{ij}\stackrel{iid}{\sim}N(0,\sigma^2)$.  <br><br>

For *balanced* experiments (the number of replicates is equal across each combination of factor levels) we can test for block and treatment effects by comparing nested/aggregated models.  Let $\overline Y_{i\cdot}$ denote the mean response for metal $i$ averaged over ingots.  We can write down the following aggregated model from \@ref(eq:fullmodel) as
\begin{equation}
\overline y_{i\cdot} = \mu + \alpha_i + \epsilon_{i},
 (\#eq:aggmodel1)
\end{equation}
where $\epsilon_i = \frac{1}{J}\sum_{j=1}^j \epsilon_{ij}$.  Then, $\epsilon_j$ has variance $\sigma_b^2 + \sigma^2/J$.  The F statistic
\[F = \frac{J\cdot MSE_{agg}}{MSE_{full}}\]
where $MSE_{agg}$ and $MSE_{full}$ are the mean squared errors from the models in \@ref(eq:aggmodel1)  and \@ref(eq:fullmodel) can be used to test the hypothesis $H_0:\sigma_b^2 = 0$.   

### Machine productivity

The dataset given below contains the results of a designed experiment to evaluate worker productivity using 3 different industrial machines.  The goal is to determine which machine is most productive while controlling for natural variation in worker productivity.  The observed workers represent a random sample from a population of workers (blocks), analogous to the ingots in the previous example.  The difference between the two examples (besides the context) is that the machine treatments are replicated wihtin workers, so that there are three observations of a productivity score fore each worker for each type of machine.  This means that we can fit a model with *interaction terms* capable of capturing changes in machine effects on productivity between different workers (if those changes are present): 
\begin{equation}
y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk},
 (\#eq:fullmodel2)
\end{equation}
where $k$ denotes the $k^{\text{th}}$ replicate within machine $i$ and worker $j$; and where $(\alpha\beta)_{ij}$ denote the machine-worker interaction effects.  <br><br>

Let $\overline Y_{ij\cdot}$ be the mean response averaging over replicates for the treatment $i$ and block $j$ combination.  Then, 
\begin{align*}
V(\overline Y_{ij\cdot}) &= V\left(K^{-1}\sum_{k=1}^K Y_{ijk}\right) \\
&= \frac{1}{K^2}V\left(\sum_{k=1}^K \{\mu+\alpha_i+\beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}\}\right)\\
& = \frac{1}{K^2}V\left(K\mu + K\alpha_i + K\beta_j + K(\alpha\beta)_{ij} + \sum_{k=1}^K \epsilon_{ijk}\right)\\
& = \sigma_b^2 + \sigma_{ab}^2 + K^{-1}\sigma^2.
\end{align*}
If we rewrite the model for the cell mean responses as
\begin{equation}
\overline y_{ij\cdot} = \mu + \alpha_i + \beta_j + \epsilon_{ij},
 (\#eq:aggmodel)
\end{equation}
then the aggregated error term follows $\epsilon_{ij}\stackrel{iid}{\sim}N(0, \sigma_{ab}^2 + \sigma^2/K)$.  The residual mean square (or called the mean squared error) of model \@ref(eq:fullmodel2) (let's call it $MSE_{\text{full}}$) has mean $\sigma^2$ with $n-p_1$ degrees of freedom where $n$ is the sample size and $p$ is the number of coefficients in the fitted model ($p_1$ equals the number of crossed factor levels, the number of blocks times the number of treatments).  The residual mean square for the aggregated model in \@ref(eq:aggmodel) (let's call it $MSE_{\text{agg}}$) has mean $\sigma_{ab}^2 + \sigma^2/K$ with $n/K-p_2$ degrees of freedom where $p_2$ is the number of treatments plus the number of blocks minus 1. An unbiased estimate of $\sigma_{ab}^2$ is given by  $MSE_{\text{agg}} - \frac{1}{K}MSE_{\text{full}}$.  Consider testing the null hypothesis $H_0:\sigma_{ab}^2 = 0$.  The statistic
\[F := \frac{K\cdot MSE_{agg}}{MSE_{full}}\stackrel{H_0}{\sim}F_{n/K-p_2, n-p_1},\]
that is, under the null hypothesis.  The test that rejects $H_0$ for $F > F_{1-\alpha,n/K-p_2, n-p_1}$ is exactly equivalent to the partial F test between the full model and the full model without the interaction terms (the additive model).  <br><br>

Below we use R to compute ANOVA tables for the full model, full model without interaction, and the aggregated model.  The F test statistic for the aggregated model is about 46.13 on 10 and 36 degrees of freedom, which exactly matches the partial F test between the full and additive models.

```{r}
library(nlme)

# aggregated model
Mach.agg <- aggregate(score~Machine*Worker, data = Machines, FUN=mean)

m2 <- lm(score~Machine+Worker, data = Mach.agg)

anova(m2)

# full model with interaction
m0 <- lm(score~Machine*Worker, data = Machines)

anova(m0)

(142.18*3/10)/(33.29/36)
1-pf((142.18*3/10)/(33.29/36), 10, 36)

# additive model (no interaction)
m1 <- lm(score~Machine+Worker, data = Machines)

anova(m1)

anova(m1,m0)



```



## A general linear mixed model

For experiments comparing responses between factors the ANOVA-type analyses above are sufficient.  But, for more general models with random effects, e.g., those including continuous covariates, general-purpose methods are needed.  The general mixed effects model may be written
\[Y = X\beta+ Z\alpha + \epsilon\]
where Y is an $n\times 1$ response, $X$ is an $n \times p$ design matrix for fixed (non-random) effects; $Z$ is and $n\times a$ matrix for random effects; $\beta$ is a $p\times 1$ non-random coefficient vector; $\alpha\sim N_a(0, \psi_\theta)$ is an $a\times 1$ multivariate normal random coefficient vector with mean 0 and covariance matrix $\psi_\theta$ indexed by a parameter $\theta$; and $\epsilon\sim N_n(0, \Lambda_\theta)$ is a multivariate normal random residual vector with covariance matrix $\Lambda_\theta$. An alternative way of writing the model (quite succintly) is
\begin{equation}
(\#eq:linmix)
Y\sim N_n(X\beta, Z \psi_\theta Z^\top + \Lambda_\theta).
\end{equation}


### Parameter estimation using PML

The linear mixed model in \@ref(eq:linmix) may be fit using maximum likelihood estimation (MLE), but the (potentially) complicated covariance structure poses challenges to numerical maximization of the likelihood function.  Rather than straightforward MLE, linear mixed models are usually fit using either *restricted maximum likelihood estimation* (REML, also called residual MLE) or *profile maximum likelihood estimation* (PML).  Both approaches aim to simplify the computation of the maximum while retaining the good asymptotic properties of maximum likelihood estimators.  While MLE is a frequentist concept, it turns out that REML can be derived most intuitively using a Bayesian approach.

If you have previously covered general linear models, i.e., $Y = X\beta + \epsilon$ where $Cov(\epsilon) = \Sigma$ for some known $\Sigma$ or some matrix $\Sigma = \sigma^2 V$ for an unknown scalar $\sigma^2$ and a known matrix $V$, then you are familiar with weighted least squares (WLS) estimation.  If the linear mixed model covariance parameter $\theta$ were known, then the model could be fit using WLS:
\[\hat\beta(\theta) = (X^\top W^{-1}X)^{-1}X^\top W^{-1}Y, \quad \text{where} \quad W = Z \psi_\theta Z^\top + \Lambda_\theta.\]

The above WLS solution inspires the PML technique: maximize the likelihood with respect to $\theta$ after plugging in $\beta = \hat\beta(\theta)$.  PML reproduces MLEs exactly; its advantage is its simplified formulation of the likelihood maximization problem. 

Nevertheless there are two problems with the PML strategy suggested above: one computational and the other statistical.  

The first problem is that PML/WLS requires inverting the $n\times n$ matrix $W = Z \psi_\theta Z^\top + \Lambda_\theta$.  For even moderate sample sizes this matrix inversion can be both computationally demanding and computationally unstable.  Fortunately, some clever linear algebra resolves this problem.  Define matrix $A:=\psi^{-1} + Z^\top \Lambda^{-1}Z$.  Observe that the matrix $\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1}$ equals the inverse of $Z\psi Z^\top + \Lambda$:
\begin{align*}
&(\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1})(Z\psi Z^\top + \Lambda) \\
& = \Lambda^{-1}Z\psi Z^\top + I - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1}Z\psi Z^\top - \Lambda^{-1}ZA^{-1}Z^\top \\
& = I + \Lambda^{-1}Z(\psi - A^{-1}(Z^\top \Lambda^{-1}Z\psi + I))Z^\top \\
& = I+\Lambda^{-1}Z(\psi - \psi(Z^\top \Lambda^{-1}A\psi + I)^{-1}(Z^\top \Lambda^{-1}A\psi + I))Z^\top \\
& = I
\end{align*}
The key property of this inverse is that it only requires computing the inverse of $p\times p$ and $a\times a$ matrices, and does not require the computation of any $n\times n$ matrix inverse.  In many practical applications both $p$ and $a$ are much smaller than $n$, and the matrices $\Lambda$ and $\psi$ often have simple structures, so that the above matrix inversions may be performed very quickly and reliably.  

The second issue with PML cannot as easily be overcome.  Think back to the Gauss-Markov model $Y = X\beta+\epsilon$ where $Cov(\epsilon) = \sigma^2 I_n$.  Recall that the MLE $\hat\sigma^2$ is biased, i.e., $E(\hat\sigma^2) = \frac{n-p}{n}\sigma^2$.  In practice we may have a substantial number of covariates relative to sample size so that this bias can be quite significant; and, importantly, the bias results in an underestimate which causes over-optimism with respect to inferences concerning $\beta$.  The same phenomenon occurs in ML/PML estimation: the PML estimate of $\theta$ is biased.  The desire for an estimation strategy that constructively produces unbiased estimates is the motivation for REML, covered next.

### REML - frequentist approach

There are two formulations both leading to the residual (also called restircted/reduced) maximum likelihood approach (REML), one frequentist (non-Bayesian) and the other Bayesian.  Let's explore both approaches.

The motivation for REML is to produce unbiased (or at least less biased than MLE) estimates of the covariance parameters.  The REML approach to doing this may be interpreted as a likelihood approach based on residuals, or as a marginal likelihood approach to covariance parameter estimation.

Begin with the linear mixed model in \@ref(eq:linmix).  Assume $X$ has full rank $p$ and let $L = [L_1 \,\,L_2]$ denote a block matrix of $n\times p$ and $n\times (n-p)$ blocks with the properties
\[L_1^\top X = I_p \quad\text{and}\quad L_2^\top X = 0_{n-p},\]
such that $L$ has full rank $n$.  

This setup is a bit abstract, so it helps to find at least one concrete example of such an $L$.  Start with the projection matrix $P_X := X(X^\top X)^{-1}X^\top$.  Since $P_X$ is a symmetric, idempotent matrix of rank $p$ it has exactly $p$ eigenvalues of value 1 and $n-p$ 0 eigenvalues, and its eigenvectors are orthonormal.  Hence, 
\[P_X = \begin{bmatrix}V_1 & V_2\\
V_3 &V_4\end{bmatrix}\begin{bmatrix}1_{p\times p} & 0_{p\times n-p}\\
0_{n-p \times p} & 0_{n-p \times n-p}\end{bmatrix}\begin{bmatrix}V_1 & V_2\\
V_3 &V_4\end{bmatrix}^\top.\]
It follows from this representation that 
\[P_X = \begin{bmatrix}V_1 \\
V_3\end{bmatrix}\begin{bmatrix}V_1 \\
V_3 \end{bmatrix}^\top=:L_1L_1^\top\]
Furthermore, if $v$ is an eigenvector of $P_X$ with eigenvalue 1, then $v$ is an eigenvector of $I-P_X$ with eigenvalue 0 just by the definition of an eigenvalue ($Av=\lambda v$).  It follows that
\[I-P_X = \begin{bmatrix}V_2 \\
V_4\end{bmatrix}\begin{bmatrix}V_2 \\
V_4 \end{bmatrix}^\top=:L_2 L_2^\top.\]
By construction, $L_1L_2^\top = 0$, $L_1^\top L_1 = I_p$ and $L_2^\top L_2 = I_{n-p}$.  The columns of $L$ are orthogonal, and so $L$ is full rank.

In what follows, note that the REML estimates are invariant to the choice of $L$, so long as it has the above three properties.

Now, consider the full rank linear transformation $Y \mapsto LY = [Y_1 \quad Y_2]^\top$.  By the properties of $L$, we have
\[LY = \begin{bmatrix} Y_1\\Y_2 \end{bmatrix} \sim N\left(\begin{bmatrix} \beta \\ 0\end{bmatrix}, \begin{bmatrix} L_1^\top \Sigma L_1 &L_1^\top \Sigma L_2\\ L_2^\top \Sigma L_1 & L_2^\top \Sigma L_2 \end{bmatrix}\right)\]
where $\Sigma = Cov(Y) = Z \psi_\theta Z^\top + \Lambda_\theta$.

The next step is to consider an equivalent characterization of the distribution of $(Y_1, Y_2)$ as the product of the conditional distribution of $Y_1|Y_2 = y_2$ and the marginal distribution of $Y_2$.  Using the general formulas for the conditional distribution of a multivariate normal random vector, we get
\[Y_1|y_2 \sim N(\beta - L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1}y_2, \, L_1^\top \Sigma L_1-L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1} L_2^\top \Sigma L_1).\]
Some tedious linear algebra (omitted here) can be used to show that the above covariance matrix is actually equal to $(X^\top \Sigma^{-1}X)^{-1}$, regardless of the specific choice of $L$.  

Therefore, the conditional likelihood is given by
\[\ell_c := \text{const.} - \tfrac12\log |(X^\top \Sigma^{-1}X)^{-1}| -\tfrac{1}{2}(y_1 - \beta - y_2^\star)X^\top \Sigma^{-1}X(y_1 - \beta - y_2^\star)^\top, \]
where $y_2^\star = L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1}y_2$.  

Again, some tedious linear algebra computations can be used to show that
\[L_2(L_2^\top \Sigma L_2)^{-1}L_2^\top = \Sigma^{-1} - \Sigma^{-1}X(X^\top\Sigma^{-1} X)^{-1} X^\top\Sigma^{-1},\]
which is an improvement computationally because it requires only inversion of a $p\times p$ matrix and $\Sigma^{-1}$, which has an efficient formula; see above.  Furthermore, the block matrix determinant identity given by
\[det\begin{bmatrix}A & B\\
B^\top & C\end{bmatrix} = |C||A - B^\top C^{-1}B|,\]
applied to $L^\top\Sigma L$ can be used to show that
\[\log |L_2^\top \Sigma L_2| = \log|L^\top\Sigma L| + \log |X^\top \Sigma^{-1}X|.\]
Further, rules for determinants imply
\[\log|L^\top\Sigma L| = \log|L^\top L\Sigma| = \log|L^\top L|+\log|\Sigma|.\]
Using these simplified expressions, and noting that $\log|L^\top L|$ is constant in parameters, we may write tge marginal (or residual) likelihood as
\[\ell_r:= \text{const}.-\tfrac12 \log |\Sigma| +\tfrac12 \log|X^\top \Sigma^{-1} X| -\tfrac12 y^\top(\Sigma^{-1} - \Sigma^{-1}X(X^\top\Sigma^{-1} X)^{-1} X^\top\Sigma^{-1})y.\]

What is the reason for referring to $\ell_r$ as a *residual loglikelihood*? Recall $\ell_r$ is the marginal loglikelihood of $L_2^\top Y$ where $L_2^\top X = 0$.  Therefore, 
\[E(L_2^\top Y) = L_2^\top X\beta = (L_2^\top X)\beta = 0\beta = 0.\]
We say $L_2^\top Y$ behaves as a residual because $L_2^\top X = 0$.  

The REML estimate of $\theta$ is given by the maximizer of $\ell_r$ with respect to $\theta$.  Note also that this is simple the MLE of $\theta$ with respect to the marginal likelihood of $L_2^\top Y$, i.e., the likelihood of ``part of the data".

Next, consider maximizing the conditional likelihood $\ell_c$ with respect to $\beta$ and where $\theta=\hat\theta$ is fixed at its REML estimate, i.e., $\Sigma = \hat\Sigma$.  Take the first derivative with respect to $\beta$, equate to zero, and observe
\begin{align*}
\hat\beta &= y_1 - L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1}y_2\\
&= L_1^\top y - L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1}L_2^\top y\\
&= L_1^\top \Sigma \Sigma^{-1}y - L_1^\top \Sigma L_2(L_2^\top \Sigma L_2)^{-1}L_2^\top \Sigma\Sigma^{-1}y\\
& = (L_1^\top \Sigma - (L_1^\top \Sigma L_2)(L_2^\top \Sigma L_2)^{-1}L_2^\top \Sigma)\Sigma^{-1}y\\
& = (L_1^\top \Sigma L_1^\top X - (L_1^\top \Sigma L_2)(L_2^\top \Sigma L_2)^{-1}L_2^\top \Sigma L_1^\top X)\Sigma^{-1}y\\
& = [(L_1^\top \Sigma L_1)-(L_1^\top \Sigma L_2)(L_2^\top \Sigma L_2)^{-1}(L_2^\top \Sigma L_1)]X\Sigma^{-1}y\\
& = (X^\top \Sigma^{-1}X)^{-1}X^\top \Sigma^{-1}y,
\end{align*}
using that $L_1^\top X = I$ and the equivalence between the conditional covariance of $Y_1|y_2$ and $(X^\top \Sigma^{-1}X)^{-1}$ shown above.

We conclude that the REML estimator of $\beta$ is a weighted least squares estimator in which the weight matrix is given by the REML-based plug-in estimator $\hat\Sigma^{-1}$.  

### REML - Bayesian approach

Suppose we model (in a Bayesian sense) the parameter $\beta$ with an improper constant prior, and the random component $\alpha$ with a multivariate normal prior with covariance $\psi$.  For whatever parameters define $\theta$, endow those with constant/improper priors as well.  Then, combining priors and multivariate normal likelihood we have the following posterior:
\[\log\Pi_n(\beta) = \text{const.} - \tfrac12 \left(y - X\beta - Z\alpha\right)^\top \Lambda^{-1}\left(y - X\beta - Z\alpha\right) - \tfrac12\alpha^\top \psi^{-1}\alpha . \]
A nice (but somewhat hard to find) factorization of the posterior is due to Searle et al. (Variance Components, Section 9.2).  Expand the exponent:
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top \Lambda^{-1}\left(y - X\beta\right)\\
&-\tfrac12 \alpha^\top (\psi^{-1} + Z^\top \Lambda^{-1}Z)\alpha + \alpha^\top Z^\top \Lambda^{-1}(y - X\beta)
\end{align*}
Let $A:=\psi^{-1} + Z^\top \Lambda^{-1}Z$ and complete the square in $\alpha$:
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top \Lambda^{-1}\left(y - X\beta\right)\\
&-\tfrac12(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))^\top A(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))\\
&+\tfrac12(y-X\beta)^\top\Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1} (y-X\beta). 
\end{align*}
Combine the terms quadratic in $y-X\beta$.  The resulting matrix $\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1}$ is equal to $(Z\psi Z^\top + \Lambda)^{-1}$, which is a fact we used above in the frequentist approach to REML.  Therefore, the posterior factorizes into
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top (Z\psi Z^\top +\Lambda)^{-1}(y - X\beta) \\
&- \tfrac12(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))^\top A(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta)) \end{align*}

Integrate over $\alpha$ to obtain the posterior for $\beta$.  Marginally, $\alpha$ is multivariate normal; therefore, we obtain
\[\Pi_n(\beta) = (2\Pi)^{-n/2}|\Lambda|^{-1/2}|\psi|^{-1/2}|A|^{-1/2}\exp\left\{-\tfrac12 \left(y - X\beta\right)^\top (Z\psi Z^\top +\Lambda)^{-1}(y - X\beta)\right\}.\]

It can be verified that $|Z\psi Z^\top +\Lambda| = |\Lambda||\psi||A|$ by using the following identity
\[|D^{-1} - CA^{-1}B| = |D||A^{-1}||A - BD^{-1}C|\]
where $Z\psi Z^\top +\Lambda = D^{-1} - CA^{-1}B = (\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1})^{-1}$; and, see, e.g., Appendix M equation 31 in Searle et al. (Variance Components).  As a result, the posterior for $\beta$ is multivariate normal with covariance $Z\psi Z^\top +\Lambda$.

Let $V := (Z\psi Z^\top +\Lambda)$.  In the exponent, add and subtract $X\hat\beta = X(X^\top V^{-1} X)^{-1}X^\top V^{-1}y$ (which depends on $\theta$) in the quadratic to obtain
\begin{align*}
\left(y - X\beta\right)^\top V^{-1}(y - X\beta) & = \left(y - X\hat\beta\right)^\top V^{-1}(y - X\hat\beta)\\
& -2 \left(y - X\hat\beta\right)^\top V^{-1}(X\hat\beta - X\beta)\\
& + \left(X\hat\beta - X\beta\right)^\top V^{-1}(X\hat\beta - X\beta).
\end{align*}
It is straightforward to show the middle term is zero.  Thereforen, the joint posterior for $(\beta, \theta)$ may be written
\[\log \Pi_n(\beta, \theta) = \text{const}. -\tfrac12\log|V|-\tfrac12(y-X\hat\beta)^\top V^{-1}(y-X\hat\beta) -\tfrac12(X\hat\beta - X\beta)V^{-1}(X\hat\beta - X\beta)\]

If we integrate out $\beta$ (w.r.t. a multivariate normal density), then we obtain the following marginal posterior for the variance parameters:
\[\log\Pi_n(\theta) = \text{const.} - \tfrac{1}{2}\log|V| + \tfrac{1}{2}\log|X^\top V^{-1}X|-\tfrac12 \left(y - X\hat\beta\right)^\top V^{-1}(y - X\hat\beta).\]

## Inference for fixed effects

The most common inferential questions concern the regression coefficient vector $\beta$.  Wald confidence regions for $\beta$ are defined by the sets
\[C_\alpha := \{b: (b - \hat\beta)(X^\top V^{-1}X)^{-1}(b - \hat\beta)<\chi^2_{1-\alpha, p}\}\]
where $\chi^2_{1-\alpha, p}$ is the upper $\alpha$ quantile of the Chi-squared distribution with $p$ degrees of freedom.  Unfortunately, these regions are only exact when $\theta$ is known and can severely undercover for small sample sizes.  Software packages typically perform an adjustment, such as the Kenward-Rogers scaling, which is a generalization of Satterthwaite's approximation.  

For a point null hypothesis $H_0: \beta = \beta_{0}$ the Wald test statistic is
\[W = (\beta_0 - \hat\beta)(X^\top V^{-1}X)^{-1}(\beta_0 - \hat\beta)\]
and $H_0$ is rejected if $W > \chi^2_{1-\alpha, p}$.  More generally, let $D$ denote an $r\times p$ matrix $r<p$ of full rank.  Reject the null hypothesis $H_0:D\beta = b_0$ if $W > \chi^2_{1-\alpha, r}$ where
\[W = (b_0 - D\hat\beta)(D X^\top V^{-1}X D^\top)^{-1}(b_0 - D\hat\beta)\]








