# Linear Mixed Models

## ANOVA with random factors

The simplest linear mixed models are used to analyze linear models for one-way ANOVA, randomized complete block designs, and two-way ANOVA.  *Mixed* models as opposed to *fixed* models (the linear models you have heretofore studied) are needed when factors have levels that are **random**.  Random levels occur whenever the units making up those levels behave like random samples from a population.  Two examples are given below.  And, we discuss how to perform ANOVA-like tests for factors that are random rather than fixed.  The upshot is that (at least for balanced experiments/datasets) the tests for fixed effects are identical to those for random effects, only the interpretation is different (and importantly so!).

### Strength of metallic bonds

The dataset below, called "Bonds", contains responses for 21 samples of metals, 7 each for iron, nickel, and copper, that quantify the strength of metallic bonds.  One sample from each metal was extracted from each of 7 ingots.  We expect ingots to act like blocks---differences in ingots account for a substantial amount of variability in the responses, but the precise block effects are of no inferential/scientific inference.  We only include the blocks in order to reduce the residual variance after accounting for block variance.  A randomized controlled block design describes how this data was collected, but, if we repeated the experiment, the blocks (ingots) would be completely different.  That is, the blocks are not *fixed* but *random*.  Rather than estimating block effects that would surely change experiment to experiment, we should focus on estimating the amount of variability explained by the blocks, which should remain about the same experiment to experiment.  This suggests a different model than used to analyze RCBD experiments with fixed blocks.<br><br>

The usual linear model for fixed blocks is
\[y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ij},\]
where $y_{ij}$ is the response for treatment (metal) $i$ in block (ingot) $j$; the $\alpha_i$'s are the metal (treatment) effects; the $\beta_j$'s are the ingot (block) effects; and, $\epsilon_{ij}\stackrel{iid}{\sim}N(0,\sigma^2)$ are the random residuals.<br><br>

The above linear model is the wrong model for this data because the block effects (and, hence, also the interaction effects) are meaningless outside of the given data set; these are not population-level parameters because the blocks are random rather than fixed.  The appropriate model (given normality and independence of random residuals is reasonable) is the following *mixed effects model*:

\begin{equation}
y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij},
 (\#eq:fullmodel)
\end{equation}
where $\beta_j\stackrel{iid}{\sim}N(0, \sigma_b^2)$ and, independently, $\epsilon_{ij}\stackrel{iid}{\sim}N(0,\sigma^2)$.  <br><br>

For *balanced* experiments (the number of replicates is equal across each combination of factor levels) we can test for block and treatment effects by comparing nested/aggregated models.  Let $\overline Y_{i\cdot}$ denote the mean response for metal $i$ averaged over ingots.  We can write down the following aggregated model from \@ref(eq:fullmodel) as
\begin{equation}
\overline y_{i\cdot} = \mu + \alpha_i + \epsilon_{i},
 (\#eq:aggmodel1)
\end{equation}
where $\epsilon_i = \frac{1}{J}\sum_{j=1}^j \epsilon_{ij}$.  Then, $\epsilon_j$ has variance $\sigma_b^2 + \sigma^2/J$.  The F statistic
\[F = \frac{J\cdot MSE_{agg}}{MSE_{full}}\]
where $MSE_{agg}$ and $MSE_{full}$ are the mean squared errors from the models in \@ref(eq:aggmodel1)  and \@ref(eq:fullmodel) can be used to test the hypothesis $H_0:\sigma_b^2 = 0$.   

### Machine Productivity

The dataset given below contains the results of a designed experiment to evaluate worker productivity using 3 different industrial machines.  The goal is to determine which machine is most productive while controlling for natural variation in worker productivity.  The observed workers represent a random sample from a population of workers (blocks), analogous to the ingots in the previous example.  The difference between the two examples (besides the context) is that the machine treatments are replicated wihtin workers, so that there are three observations of a productivity score fore each worker for each type of machine.  This means that we can fit a model with *interaction terms* capable of capturing changes in machine effects on productivity between different workers (if those changes are present): 
\begin{equation}
y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk},
 (\#eq:fullmodel2)
\end{equation}
where $k$ denotes the $k^{\text{th}}$ replicate within machine $i$ and worker $j$; and where $(\alpha\beta)_{ij}$ denote the machine-worker interaction effects.  <br><br>

Let $\overline Y_{ij\cdot}$ be the mean response averaging over replicates for the treatment $i$ and block $j$ combination.  Then, 
\begin{align*}
V(\overline Y_{ij\cdot}) &= V\left(K^{-1}\sum_{k=1}^K Y_{ijk}\right) \\
&= \frac{1}{K^2}V\left(\sum_{k=1}^K \{\mu+\alpha_i+\beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}\}\right)\\
& = \frac{1}{K^2}V\left(K\mu + K\alpha_i + K\beta_j + K(\alpha\beta)_{ij} + \sum_{k=1}^K \epsilon_{ijk}\right)\\
& = \sigma_b^2 + \sigma_{ab}^2 + K^{-1}\sigma^2.
\end{align*}
If we rewrite the model for the cell mean responses as
\begin{equation}
\overline y_{ij\cdot} = \mu + \alpha_i + \beta_j + \epsilon_{ij},
 (\#eq:aggmodel)
\end{equation}
then the aggregated error term follows $\epsilon_{ij}\stackrel{iid}{\sim}N(0, \sigma_{ab}^2 + \sigma^2/K)$.  The residual mean square (or called the mean squared error) of model \@ref(eq:fullmodel2) (let's call it $MSE_{\text{full}}$) has mean $\sigma^2$ with $n-p_1$ degrees of freedom where $n$ is the sample size and $p$ is the number of coefficients in the fitted model ($p_1$ equals the number of crossed factor levels, the number of blocks times the number of treatments).  The residual mean square for the aggregated model in \@ref(eq:aggmodel) (let's call it $MSE_{\text{agg}}$) has mean $\sigma_{ab}^2 + \sigma^2/K$ with $n/K-p_2$ degrees of freedom where $p_2$ is the number of treatments plus the number of blocks minus 1. An unbiased estimate of $\sigma_{ab}^2$ is given by  $MSE_{\text{agg}} - \frac{1}{K}MSE_{\text{full}}$.  Consider testing the null hypothesis $H_0:\sigma_{ab}^2 = 0$.  The statistic
\[F := \frac{K\cdot MSE_{agg}}{MSE_{full}}\stackrel{H_0}{\sim}F_{n/K-p_2, n-p_1},\]
that is, under the null hypothesis.  The test that rejects $H_0$ for $F > F_{1-\alpha,n/K-p_2, n-p_1}$ is exactly equivalent to the partial F test between the full model and the full model without the interaction terms (the additive model).  <br><br>

Below we use R to compute ANOVA tables for the full model, full model without interaction, and the aggregated model.  The F test statistic for the aggregated model is about 46.13 on 10 and 36 degrees of freedom, which exactly matches the partial F test between the full and additive models.

```{r}
library(nlme)

# aggregated model
Mach.agg <- aggregate(score~Machine*Worker, data = Machines, FUN=mean)

m2 <- lm(score~Machine+Worker, data = Mach.agg)

anova(m2)

# full model with interaction
m0 <- lm(score~Machine*Worker, data = Machines)

anova(m0)

(142.18*3/10)/(33.29/36)
1-pf((142.18*3/10)/(33.29/36), 10, 36)

# additive model (no interaction)
m1 <- lm(score~Machine+Worker, data = Machines)

anova(m1)

anova(m1,m0)



```



## General Mixed Model Parameter Estimation

For experiments comparing responses between factors the ANOVA-type analyses above are sufficient.  But, for more general models with random effects, e.g., those including continuous covariates, general-purpose methods are needed.  The general mixed effects model may be written
\[Y = X\beta+ Z\alpha + \epsilon\]
where Y is an $n\times 1$ response, $X$ is an $n \times p$ design matrix for fixed (non-random) effects; $Z$ is and $n\times a$ matrix for random effects; $\beta$ is a $p\times 1$ non-random coefficient vector; $\alpha\sim N_a(0, \psi_\theta)$ is an $a\times 1$ multivariate normal random coefficient vector with mean 0 and covariance matrix $\psi_\theta$ indexed by a parameter $\theta$; and $\epsilon\sim N_n(0, \Lambda_\theta)$ is a multivariate normal random residual vector with covariance matrix $\Lambda_\theta$. An alternative way of writing the model (quite succintly) is
\[Y\sim N_n(X\beta, Z \psi_\theta Z^\top + \Lambda_\theta).\]

### Maximum Likelihood

If you are familiar with least squares (and weighted least squares) you may recognize the MLE of $\beta$ is given by the weighted least squares solution
\[\hat\beta_{WLS} = (X^\top W^{-1}X)^{-1}X^\top W^{-1}Y\]
where $W^{-1} = (Z \psi_\theta Z^\top + \Lambda_\theta)^{-1}$.  The WLS estimator is fine in theory, but hard to compute in practice because it requires inverting an $n\times n$ matrix.  For large $n$ this is computationally challenging---$O(n^3)$ computations---unless the matrix has some special "sparse" structure.  It's not just the number of computations (and time) needed to compute an $n\times n$ matrix inverse that is problematic.  For large matrices, algorithms for computing the inverse may not maintain adequate numerical precision, and, therefore, the resulting matrix may not truly equal the inverse desired.  These errors in floating-point arithmetic will compound for large matrix inversion problems and lead to an estimate $\hat\beta_{WLS}$ that is not actually equal to the true weighted least squares estimate, due to computational errors.<br><br>

Typically, the direct WLS approach to maximum likelihood estimation is replaced by an iterative procedure in order to avoid the computation of an inverse of a large matrix.  The WLS solution above is based on the joint density (likelihood) of the observations $Y$.  Instead, consider the joint density of $(Y,\alpha)$ parameterized by $(\beta, \theta)$.  This joint density is conveniently expressed by the product of conditional and marginal densities $f(y|\alpha;\beta,\theta)\cdot f(\alpha;\beta,\theta)$ given by
\[f(y|\alpha;\beta,\theta) = (2\pi)^{-2/n}|\Lambda_\theta|^{-1/2}\exp\{-\tfrac12\|y - X\beta - Z\alpha\|^2_{\lambda_\theta^{-1}}\},\]
where $\|a\|_B := a^\top Ba$, and
\[f(\alpha;\beta,\theta) = (2\pi)^{-p/2}|\psi_\theta|^{-1/2}\exp\{-\tfrac12\alpha^\top \psi_\theta^{-1}\alpha\}.\]

Now, the joint density $f(y,\alpha;\beta,\theta)$ cannot be used directly as a likelihood, i.e., $L(\beta, \theta| Y,\alpha)$ because $\alpha$ is an unobserved random variable.  Instead, we could average over $\alpha$ values (integrate out $\alpha$) to obtain the marginal density $f(y;\beta,\theta)$ and use this density with $Y=y$ fixed at the observed values as a likelihood for $(\beta, \theta)$.  To compute the this marginal density we begin with a tranformation and a Taylor expansion:
\begin{align*}
f(y;\beta, \theta) &= \int f(y,\alpha;\beta, \theta)d\alpha = \int \exp\{\log f(y\alpha;\beta,\theta)\}d\alpha \\ 
& = \int \exp\left\{\log f(y,\hat\alpha;\beta,\theta) + \tfrac12 (\alpha -\hat\alpha)^\top \frac{\partial^2 \log f(y,\alpha;\beta,\theta)}{\partial \alpha \partial\alpha^\top}(\alpha - \hat\alpha)\right\}d\alpha,
\end{align*}
where $\hat\alpha$ is the optimal *prediction* of $\alpha$ maximizing $f(y,\alpha;\beta,\theta)$ for a fixed $(\beta,\theta)$.  This maximizer has an explicit form: $\hat\alpha = (Z^\top \Lambda_\theta^{-1}Z + \psi_\theta^{-1})^{-1}Z^\top \Lambda_\theta^{-1}(Y - X\beta)$. The second line above contains a Taylor expansion of $\log f(y,\alpha;\beta,\theta)$  around $\hat\alpha$ which is exact, i.e., the higher-order terms are zero due to the Gaussian form of the densities.  Pulling out constant terms from the integration we have
\[f(y;\beta, \theta) = f(y, \hat\alpha;\beta, \theta)\int \exp\left\{-\tfrac12 (\alpha -\hat\alpha)^\top (Z^\top \lambda_\theta^{-1}Z + \psi_\theta^{-1})(\alpha - \hat\alpha)\right\}d\alpha.\]
Using the fact the integrand is the kernel of a multivariate Gaussian density, we see the integral evaluates to $(2\pi)^{p/2}|Z^\top \Lambda_\theta^{-1}Z+\psi_\theta^{-1}|^{-1/2}$, and, hence
\[f(y;\beta, \theta) = f(y, \hat\alpha;\beta, \theta)(2\pi)^{p/2}|Z^\top \Lambda_\theta^{-1}Z+\psi_\theta^{-1}|^{-1/2}.\]
Now, using the marginal density of $Y$ to define the loglikelihood we have
\[2\ell(\beta,\theta;y) = -\|y - X\beta - Z\hat\alpha\|_{\Lambda_\theta^{-1}}-\hat\alpha^\top \psi_\theta^{-1}\hat\alpha - \log|\Lambda_\theta|-\log|\psi_\theta|-\log|Z^\top\Lambda_\theta^{-1}Z+\psi_\theta^{-1}|-n\log 2\pi.\]
Given a fixed value of $\theta$, the MLE $\hat\beta$ may be found by maximizing
\[-\|y - X\beta - Z\hat\alpha\|_{\Lambda_\theta^{-1}}-\hat\alpha^\top \psi_\theta^{-1}\hat\alpha\]
where, recall, $\hat\alpha = (Z^\top \Lambda_\theta^{-1}Z)^{-1}Z^\top \Lambda_\theta^{-1}(Y - X\beta)$.  Plug in $\hat\alpha$ to see this becomes a WLS problem; hence, $\hat\beta$ is a WLS solution, and the weight matrix is given by $V = V_1 + V_2$, where (suppressing $\theta$ subscripts)
\begin{align*}
V_1 &= (I - Z(Z^\top \Lambda^{-1}Z)^{-1}Z^\top \Lambda^{-1})^\top \Lambda^{-1} (I - Z(Z^\top \Lambda^{-1}Z)^{-1}Z^\top \Lambda^{-1})\\
& = \Lambda^{-1} - \Lambda^{-1}Z(Z^\top \Lambda^{-1}Z)^{-1}Z^\top\Lambda^{-1}\\
V_2 &= ((Z^\top \Lambda^{-1}Z)^{-1}Z^\top \Lambda^{-1})^\top \psi^{-1} ((Z^\top \Lambda^{-1}Z)^{-1}Z^\top \Lambda^{-1})\\
& = \Lambda^{-1}Z(Z^\top \Lambda^{-1}Z)^{-1}\psi^{-1}(Z^\top \Lambda^{-1}Z)^{-1}Z^\top \Lambda^{-1}
\end{align*}

Computing the covariance of $\hat\beta$ still is computationally intensive.  Computing the covariance of
\[(X^\top VX)^{-1}X^\top V Y\]
still requires inversion of the covariance of $Y$, which is the problematic component.  Curiously, a Bayesian point of view provides a shortcut to the computation of the MLE covariance.  Suppose we model (in a Bayesian sense) the parameter $\beta$ with an improper constant prior, and the random component $\alpha$ with a multivariate normal prior with covariance $\psi$.  Then, combining priors and multivariate normal likelihood we have the following posterior:
\[\log\Pi_n(\beta) = \text{const.} - \tfrac12 \left(y - X\beta - Z\alpha\right)^\top \Lambda^{-1}\left(y - X\beta - Z\alpha\right) - \tfrac12\alpha^\top \psi^{-1}\alpha . \]
A nice (but otherwise hard to find) factorization of the posterior is due to Searle et al. (Variance Components, Section 9.2).  Expand the exponent:
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top \Lambda^{-1}\left(y - X\beta\right)\\
&-\tfrac12 \alpha^\top (\psi^{-1} + Z^\top \Lambda^{-1}Z)\alpha + \alpha^\top Z^\top \Lambda^{-1}(y - X\beta)
\end{align*}
Let $A:=\psi^{-1} + Z^\top \Lambda^{-1}Z$ and complete the square in $\alpha$:
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top \Lambda^{-1}\left(y - X\beta\right)\\
&-\tfrac12(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))^\top A(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))\\
&+\tfrac12(y-X\beta)^\top\Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1} (y-X\beta). 
\end{align*}
Combine the terms quadratic in $y-X\beta$.  The resulting matrix $\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1}$ is equal to $(Z\psi Z^\top + \Lambda)^{-1}$, as can be seen by the following computation:
\begin{align*}
&(\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1})(Z\psi Z^\top + \Lambda) \\
& = \Lambda^{-1}Z\psi Z^\top + I - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1}Z\psi Z^\top - \Lambda^{-1}ZA^{-1}Z^\top \\
& = I + \Lambda^{-1}Z(\psi - A^{-1}(Z^\top \Lambda^{-1}Z\psi + I))Z^\top \\
& = I+\Lambda^{-1}Z(\psi - \psi(Z^\top \Lambda^{-1}A\psi + I)^{-1}(Z^\top \Lambda^{-1}A\psi + I))Z^\top \\
& = I
\end{align*}
Therefore, the posterior factorizes into
\begin{align*}
\log\Pi_n(\beta) &= \text{const.} - \tfrac12 \left(y - X\beta\right)^\top (Z\psi Z^\top +\Lambda)^{-1}(y - X\beta) \\
&- \tfrac12(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta))^\top A(\alpha - A^{-1}Z^\top\Lambda^{-1}(y-X\beta)) \end{align*}

Integrate over $\alpha$ to obtain the posterior for $\beta$.  Marginally, $\alpha$ is multivariate normal; therefore, we obtain
\[\Pi_n(\beta) = (2\Pi)^{-n/2}|\Lambda|^{-1/2}|\psi|^{-1/2}|A|^{-1/2}\exp\left\{-\tfrac12 \left(y - X\beta\right)^\top (Z\psi Z^\top +\Lambda)^{-1}(y - X\beta)\right\}.\]

It can be verified that $|Z\psi Z^\top +\Lambda| = |\Lambda||\psi||A|$ by using the following identity
\[|D^{-1} - CA^{-1}B| = |D||A^{-1}||A - BD^{-1}C|\]
where $Z\psi Z^\top +\Lambda = D^{-1} - CA^{-1}B = (\Lambda^{-1} - \Lambda^{-1}ZA^{-1}Z^\top \Lambda^{-1})^{-1}$; and, see, e.g., Appendix M equation 31 in Searle et al. (Variance Components).  As a result, the posterior for $\beta$ is multivariate normal with covariance $Z\psi Z^\top +\Lambda$.

Let $V := (Z\psi Z^\top +\Lambda)$.  In the exponent, add and subtract $X\hat\beta$ in the quadratic to obtain
\begin{align*}
\left(y - X\beta\right)^\top V^{-1}(y - X\beta) & = \left(y - X\hat\beta\right)^\top V^{-1}(y - X\hat\beta)\\
& -2 \left(y - X\hat\beta\right)^\top V^{-1}(X\hat\beta - X\beta)\\
& + \left(X\hat\beta - X\beta\right)^\top V^{-1}(X\hat\beta - X\beta).
\end{align*}
It is straightforward to show the middle term is zero.  If we integrate out $\beta$ (w.r.t. a multivariate normal density), then we obtain the following marginal posterior for the variance parameters:
\[\Pi_n(\theta, \Lambda) = \frac{(2\pi)^{-n/2}}{(2\pi)^{-p/2}}\frac{|V|^{-1/2}}{|X^\top V^{-1}X|^{-1/2}}\exp\left(-\tfrac12 \left(y - X\hat\beta\right)^\top V^{-1}(y - X\hat\beta)\right).\]










