[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"poisson-regression.html","id":"poisson-regression","chapter":"3 Poisson Regression","heading":"3 Poisson Regression","text":"","code":""},{"path":"poisson-regression.html","id":"children-ever-born-data","chapter":"3 Poisson Regression","heading":"3.1 Children Ever Born Data","text":"“Children Ever Born” (CEB) dataset consists grouped data number births Fijian women. women described according marriage duration years ordinal levels: (0-4, 5-9, 10-14, 15-19, 20-24, 25-29); place residence (Suva—capital city—Urban, Rural); , level education (none, lower primary, upper primary, secondary greater). count, mean, variance number children ever born, group size, given group women cross-classified factorial level. summaries sufficient model counts children ever born Poisson distribution (individual woman’s count needed).CEB data example observational dataset — characteristics individuals inherent rather set experimenters experimental/controlled trial—, clear context. Several interesting questions may answered using data, : fewer born children associated higher lower education among Fijian women; urban versus rural living location influence number children ever born; , number children ever born steadily increase marrige duration, tend plateau?statistician (student statistician) familiar multiple linear regression /ANOVA factorial experiments may instinctively choose fit Gauss-Markov linear model CEB data, treating responses independent normal random variables. However, since responses counts, Poisson model reasonable. , just one perform Poisson regression? — opposed familiar multiple linear regression described Gauss-Markov model:\n\\[Y = X\\beta+ \\epsilon, \\quad \\epsilon \\sim N_{n}(0_{n\\times 1}, \\sigma^2 I_n).\\]motivation chapter, explore family Generalized Linear Models defining fitting model, performing inference model diagnostics, within context CEB example.","code":"\nceb <- read.table('ceb.dat')\nhead(ceb)##   dur   res  educ mean  var  n     y\n## 1 0-4  Suva  none 0.50 1.14  8  4.00\n## 2 0-4  Suva lower 1.14 0.73 21 23.94\n## 3 0-4  Suva upper 0.90 0.67 42 37.80\n## 4 0-4  Suva  sec+ 0.73 0.48 51 37.23\n## 5 0-4 urban  none 1.17 1.06 12 14.04\n## 6 0-4 urban lower 0.85 1.59 27 22.95"},{"path":"poisson-regression.html","id":"defining-glms","chapter":"3 Poisson Regression","heading":"3.2 Defining GLMs","text":"CEB data naturally want model CEB grouped counts realizations Poisson r.v.’s means \\(n_{j}x^\\top_j\\beta\\) \\(n_j\\) number women \\(j^{\\text{th}}\\) factorial group, \\(x_j\\) vector common covariates, \\(\\beta\\) common regression coefficient vector. , likelihood model \n\\[L(\\beta;\\text{data}) = \\prod_{j=1}^{70}\\frac{(n_{j}x^\\top_j\\beta)^{y_j}e^{-n_{j}x^\\top_j\\beta}}{y_j!}\\]\nloglikelihood given \n\\[\\ell(\\beta;\\text{data}) = \\text{constant} + \\sum_{j=1}^{70}y_j\\log(n_{j}x^\\top_j\\beta) - n_{j}x^\\top_j\\beta.\\]\nPoisson likelihood member Exponential Family, contains distributions PDFs may expressed \n\\[f(y;\\theta,\\phi) = \\exp\\{[y\\theta - b(\\theta)]/(\\phi) + c(y,\\phi)\\}.\\]\nLooking ahead, apply exponential family model independent identically distributed responses, similar data encounter multiple linear regression Gauss-Markov model, allow \\(\\theta\\) well forms \\(\\), \\(b\\), \\(c\\) functions vary observations, fix \\(\\phi\\), loglikelihood sample size \\(n\\) may written follows:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{[y_i\\theta_i - b_i(\\theta_i)]/a_i(\\phi) + c_i(\\phi, y_i)\\}.\\]\nPoisson regression model grouped data fairly simple member family, \\(\\theta = \\log (n_{j}x^\\top_j\\beta)\\), \\(\\phi = (\\phi) = 1\\), \\(b(\\theta) = \\exp(\\theta) =n_{j}x^\\top_j\\beta\\). fact, often case GLMs satisfy \\((\\phi)\\propto \\phi\\) known constant. \ngeneral, GLMs satisfy\n\\[E(Y) = b'(\\theta) \\quad \\text{}\\quad V(Y) = b''(\\theta)(\\phi).\\]\nPoisson regression model, particular, \n\\[\\theta = \\log(n_{j}x^\\top_j\\beta); \\quad b(\\theta) = \\exp(\\theta); \\quad \\text{}\\quad (\\phi) = 1\\]\n\\[E(Y) = b'(\\theta) = \\frac{\\partial}{\\partial \\theta}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta; \\text{ ,}\\]\n\\[V(Y) = b''(\\theta)(\\phi) = \\frac{\\partial^2}{\\partial \\theta^2}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta.\\]","code":""},{"path":"poisson-regression.html","id":"fitting-glms","chapter":"3 Poisson Regression","heading":"3.3 Fitting GLMs","text":"Like model defined likelihood, GLMs may fit maximizing (log)likelihood. , generally case maximizers (MLEs) available closed form. Instead, computed iteratively using Newton’s method similar iterative procedure. Refer exponential family loglikelihood using usual representation \\(a_i(\\phi) = \\phi/w_i\\) \\(w_i\\) known constants:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{w_i[y_i\\theta_i - b_i(\\theta_i)]/\\phi + c_i(\\phi, y_i)\\}.\\]\nLet \\(\\mu_i = E(Y_i)\\). , \\(b'(\\theta_i) = \\mu_i\\), , equivalently, \\(g_c(\\mu_i) = \\theta_i\\) \\(g_c\\) termed canonical link; example, \\(g_c := \\log\\) Poisson distribution. Additionally, let \\(g\\) link mean linear function covariates, .e., \\(g(\\mu_i) = \\eta_i = x_i^\\top\\beta\\); e.g., \\(g\\) identity function Poisson model. Since \\(b_i'(\\theta_i)\\) also equal \\(\\mu_i\\) exponential family, may differentiate loglikelihood respect regression parameter \\(\\beta\\) using chain rule:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{=1}^n \\left\\{\\frac{w_i}{\\phi}\\left[y_i\\frac{\\partial \\theta_i}{\\partial\\beta_j} - \\frac{\\partial b_i(\\theta_i)}{\\partial \\beta_j}\\right] + c_i(\\phi, y_i)\\right\\}\\]\nusing\n\\[\\frac{\\partial \\theta_i}{\\partial \\beta_j} = \\frac{\\partial \\theta_i}{\\partial \\mu_i}\\frac{\\partial \\mu_i}{\\partial \\beta_j}.\\]\nSince \\(\\mu_i = b_i'(\\theta_i)\\) \\(\\partial \\theta_i/\\partial \\mu_i = 1/b_i''(\\theta_i)\\). , light \\(\\mu_i = b'(\\theta_i)\\) may always write \\(b_i''(\\theta_i)\\) function \\(\\mu_i\\), .e., \\(V(\\mu_i) = b_i''(\\theta_i)/w\\) \\(V(Y_i) = V(\\mu_i)\\phi\\). Moreover, since \\(\\mu_i = g^{-1}(x_i^\\top \\beta)\\) \\(\\partial\\mu_i/\\partial \\beta_j = x_{ij}/g'[g^{-1}(x_i^\\top \\beta)]\\). Substituting, can write score function using \\(\\mu_i\\) follows:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{1}{\\phi}\\sum_{=1}^n \\frac{y_i - \\mu_i}{g'(\\mu_i)V(\\mu_i)}x_{ij}.\\]\nsecond (mixed partial) derivative may written\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta_j\\partial\\beta_k} = -\\frac{1}{\\phi}\\sum_{=1}^n \\frac{x_{ij}x_{ik}h(\\mu_i)}{g'(\\mu_i)^2V(\\mu_i)}\\]\n\\(h(\\mu_i) = 1+(y_i-\\mu_i)\\{V'(\\mu_i)/V(\\mu_i) + g''(\\mu_i)/g'(\\mu_i)\\}\\). expectation second derivative (multiplied -1 appears Fisher information matrix) quantity \\(h(\\mu_i)\\) replaced \\(E[h(\\mu_i)]\\), simply equals 1 \\(E(Y_i - \\mu_i) = 0\\).\nHessian loglikelihood clearly quadratic form \\(\\phi^{-1}X^\\top WX\\) \\(X\\) \\(n\\times p\\) design matrix covariates \\(W = [h(\\mu_i)/\\{g'(\\mu_i)^2V(\\mu_i)\\}]\\) \\(n\\times n\\) diagonal matrix “weights”. Less obvious, may define \\(G = \\text{diag}\\{g'(\\mu_i)/h(\\mu_i)\\}\\) gradient loglikelihood equals \\(\\phi^{-1}X^\\top WG(y - \\mu)\\). clever rewriting, Newton’s method updates take form weighted least squares solution:\n\\[\\begin{align*}\n\\beta^{[k+1]} &= \\beta^{[k]} + (X^\\top WX)^{-1}X^\\top WG(y-\\mu)\\\\\n& = (X^\\top WX)^{-1}X^\\top W\\{G(y-\\mu)X+\\beta^{[k]}\\}\\\\\n& = (X^\\top WX)^{-1}X^\\top Wz\n\\end{align*}\\]\n\\(z := G(y-\\mu)+X\\beta^{[k]}\\) sometimes referred “pseudo-data”. Repeating weighted least squares update, iteratively, convergence, termed iteratively re-weighted least squares (IRLS) since, course, weights \\(W\\) updating iteration.Poisson regression based grouped CEB data following likelihood, gradient, Hessian:\n\\[\\begin{align*}\n&\\ell(\\beta;\\text{data}) = \\sum_{j=1}^{70} \\left[y_j x_j^\\top \\beta - n_j e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla_s \\ell = \\sum_{j=1}^{70} \\left[y_j x_{js} - n_j x_{js}e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla^2_{s,t} \\ell = -\\sum_{j=1}^{70}  n_j x_{js}x_{jt}e^{x_j^\\top \\beta}.\n\\end{align*}\\]Rewriting Hessian gradient general exponential family GLM \n\\[W_{k,k} = n_k\\mu_k\\quad\\text{}\\quad G_k = (n_k\\mu_k)^{-1}\\]\nIRLS updates given \n\\[(X^\\top WX)^{-1}X^\\top Wz\\]\n\\(z_k = (n_k\\mu_k)^{-1}(y_k - n_k\\mu_k) + x_k^\\top \\beta\\).","code":""},{"path":"poisson-regression.html","id":"irls-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.3.1 IRLS for the CEB data","text":"compute MLEs Poisson regression grouped CEB data “hand” using IRLS—, also compare glm function R. calculation initialize elements parameter vector \\(\\mu\\) sample means \\(\\mu_j = y_j/n_j\\). set pseudo data equal \\(z_j = -(1/\\mu_j)(y_j / n_j - \\mu_j) + log(\\mu_j)\\) iterate computation least squares estimates \\(\\hat\\beta\\). Note CEB data contains grouped “counts” computed \\(y_j = \\mu_jn_j\\) \\(\\mu_j\\) values rounded. , result, \\(y_j\\) counts integers. affect “hand” calculation \\(\\hat\\beta\\) whatsoever never use full Poisson PMF computations; glm function R, hand, throw many warnings \\(y_j\\) values rounded, apparently uses PMF via dpois “hood”. differences fitted \\(\\hat\\beta\\) glm’s due rounding \\(y_j\\)’s.","code":"\nn <- nrow(ceb)\ngroup.sizes <- ceb$n\nY <- ceb$y\n# IRLS - factor coding\n# initialize with mu = Y/group.sizes\noptions(contrasts = c('contr.treatment', 'contr.treatment'))\nX <- model.matrix(y~dur+res+educ, data = ceb)\nmu <- Y/group.sizes\nXB <- log(mu)\nW <- diag(as.numeric(mu))\nz <- -(1/mu)*(Y/group.sizes-mu) + XB\nbeta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\ntol <- 0.0001\ndifference <- 1\nmaxiter <- 100\niter <- 1\nwhile((difference > tol) & (iter < maxiter)){\n  XB <- X%*%beta\n  mu <- exp(XB)\n  W <- diag(as.numeric(group.sizes*mu))\n  z <- (Y/diag(W) - rep(1,n)) + XB\n  beta.old <- beta\n  beta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\n  difference <- max(abs(beta - beta.old))\n  iter<-iter+1\n}\nbeta##                    [,1]\n## (Intercept)  0.05695417\n## dur10-14     1.37053208\n## dur15-19     1.61423104\n## dur20-24     1.78548879\n## dur25-29     1.97679469\n## dur5-9       0.99765038\n## resSuva     -0.15121728\n## resurban    -0.03895822\n## educnone    -0.02308034\n## educsec+    -0.33266474\n## educupper   -0.12474575\n## the glm function can be used with offset equal to logarithm of the group sizes\nmy.glm <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb, offset = log(n))\nsummary(my.glm)## \n## Call:\n## glm(formula = round(y) ~ dur + res + educ, family = poisson(link = \"log\"), \n##     data = ceb, offset = log(n))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2960  -0.6641   0.0725   0.6336   3.6782  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.05754    0.04803   1.198    0.231    \n## dur10-14     1.36940    0.05107  26.815  < 2e-16 ***\n## dur15-19     1.61376    0.05119  31.522  < 2e-16 ***\n## dur20-24     1.78491    0.05121  34.852  < 2e-16 ***\n## dur25-29     1.97641    0.05003  39.501  < 2e-16 ***\n## dur5-9       0.99693    0.05274  18.902  < 2e-16 ***\n## resSuva     -0.15166    0.02833  -5.353 8.63e-08 ***\n## resurban    -0.03924    0.02463  -1.594    0.111    \n## educnone    -0.02297    0.02266  -1.014    0.311    \n## educsec+    -0.33312    0.05390  -6.180 6.41e-10 ***\n## educupper   -0.12425    0.03000  -4.142 3.44e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 3731.852  on 69  degrees of freedom\n## Residual deviance:   70.665  on 59  degrees of freedom\n## AIC: 522.14\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"poisson-regression.html","id":"inference-on-glms","chapter":"3 Poisson Regression","heading":"3.4 Inference on GLMs","text":"doubt noticed glm function output produces standard errors, “z” values, p-values fitted coefficient, just find accompanying summarized lm output. , justification p-values?Since \\(\\hat\\beta\\) MLE, standard likelihood theory holds \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, ^{-1}(\\beta))\\) “large” \\(n\\) \\(^{-1}(\\beta)\\) Fisher information. discussed , “observed information” (Hessian) equal \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) \\(W\\) weight matrix final iteration IRLS, \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) coincides Fisher Information replace \\(h(\\mu_i)\\) \\(E(h(\\mu_i))=1\\) IRLS (called Fisher scoring) updates. Therefore, \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, \\phi^{-1}(X^\\top W X)^{-1})\\) “large” \\(n\\), useable \\(\\phi\\) known (, equals 1, Poisson Binomial models). Otherwise, replace \\(\\phi\\) MLE use corresponding Student’s \\(t\\) distribution \\(n - p\\) degrees freedom rather standard normal inference \\(\\beta_j\\). upshot may base tests , e.g., \\(H_0:\\beta_j = 0\\), Student’s \\(t\\) \\(n-p\\) df; .e.,\n\\[\\text{Reject }H_0:\\beta_j = 0 \\text{ }\\left|\\frac{\\hat\\beta_j}{\\sqrt{\\hat\\phi^{-1}(X^\\top W X)^{-1}_{j,j}}}\\right| > t_{1-\\alpha/2,n-p}.\\]\nMultivariate Wald simultaneous \\(100(1-\\alpha)\\%\\) confidence regions given eliiptical contours:\n\\[\\phi\\text{ known: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< \\chi^2_{1-\\alpha,p} \\}\\]\n\\[\\phi\\text{ unknown: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\hat\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< F_{1-\\alpha,p, n-p} \\}\\]Moreover, approximate \\(95\\%\\) CI mean response \\(\\mu = g^{-1}(x^\\top \\beta)\\) covariate vector \\(x\\) given Delta method interval:\\[g^{-1}(x^\\top \\hat\\beta)\\pm t_{1-\\alpha/2,n-p}\\sqrt{\\hat\\phi^{-1} \\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]^\\top(X^\\top W X)^{-1}\\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]}.\\]\nmultiple linear regression (Gauss-Markov) models use partial F tests (likelihood ratio tests) test significance sets covariates, .e., \\(H_0: \\beta_j = \\beta_{j+1} = \\cdots = \\beta_{j+\\ell} = 0\\). GLMs, similar tests available. models \\(\\phi\\) known, \n\\[-2\\{\\ell(\\hat\\beta_{h_0}) - \\ell(\\hat\\beta)\\}\\stackrel{H_0}{\\sim} F_{\\ell,n-p}\\]\n\\(\\hat\\beta_{h_0}\\) MLE null hypothesis \\(\\ell\\) coefficients set equal 0.several methods estimate \\(\\phi\\) unknown. Pearson’s method observes \n\\[\\phi^{-1}X^2 \\stackrel{\\cdot}{\\sim}\\chi^2_{n-p}\\quad \\text{}\\quad X^2 :=\\sum_{=1}^n \\frac{(Y_i - \\hat\\mu_i)^2}{\\phi V(\\hat\\mu_i)}\\]\nmodel fits data adequately. Hence, \\(\\hat\\phi_P = X^2/(n-p)\\) good estimate \\(\\phi\\). certain data sets, Poisson data low counts, Pearson estimate may behave badly, modified version (Fletcher’s estimator) preferred:\n\\[\\hat\\phi_F = \\frac{\\hat\\phi_P}{1-\\overline s}, \\quad\\text{}\\]\n\\[\\overline s:=n^{-1}\\sum_{=1}^n V'(\\hat\\mu_i)\\frac{(y_i - \\hat\\mu_i)}{V(\\hat\\mu_i)}.\\]Deviance difference models B \\(\\subset B\\) given \\(D_A - D_B = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\phi\\). scaled deviance difference \n\\[D_A^* - D_B^* = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell}\\]\ndifference number fitted parameters \\(\\ell\\). Despite notation, scaled deviance difference depend \\(\\phi\\), whereas deviance difference . Two alternative tests make use scaled deviance compare nested GLMs. first analogous partial F test:\n\\[F = \\frac{(D_A^* - D_B^*)/\\ell}{D_B^* / (n-p)}\\stackrel{\\cdot}{\\sim} F_{\\ell, n-p}\\]\napproximation F distribution rough. Alternatively, can replace scale parameter scaled deviance Pearson (Fletcher) estimator obtain\n\\[\\hat D_A^* - \\hat D_B^*\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell},\\]\n“hat” scaled deviances indicates dependence \\(\\hat\\phi\\).","code":""},{"path":"poisson-regression.html","id":"inference-and-prediction-for-the-ceb-data-using-poisson-regression","chapter":"3 Poisson Regression","heading":"3.4.1 Inference and prediction for the CEB data using Poisson regression","text":"Next, ’ll demonstrate computations confidence intervals tests significance sets covariates within Poisson regression model CEB data. can either computations “hand” using results IRLS can use built-R functions like glm confint. Wald-type confidence intervals regression coefficients require inverse Hessian (estimate inverse Fisher information, equal Fisher scoring used). compute final iteration IRLS (assuming algorithm converged). GLMs need estimate scale parameter \\(\\phi\\) include Pearson Fletcher estimates —Poisson model \\(\\phi=1\\). Note estimates close 1. p-values included summarized glm output imply intercept significantly different zero, several coefficients different zero, including, e.g., \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\). Wald-type CIs p-values agree glm p-values; example, 95% CI \\(\\beta_0\\) computed “hand” (-0.0372 0.1511), suggesting intercept significantly different zero, 95% CI \\(\\beta_1\\) (1.2704, 1.4706) p-value test \\(\\beta_1 = 0\\) indistinguishable 0. computed deviance difference (minus twice difference loglikelihood) intercept-model full model. deviance difference 3661 ten degrees freedom (difference number fitted coefficients models). intercept model fits, deviance difference comparable Chi-squared r.v. 10 degrees freedom, corresponding p-value basically zero, supporting claim full model fits much better intercept-model. Compare deviance difference calculation output glm: glm includes null deviance residual deviance, difference two gives deviance difference statistic used compare intercept-full models. 3661, agreeing almost exactly “hand” calculation.","code":"\nHessian <- t(X)%*%W%*%X\ninv.Hessian <- solve(Hessian)\np <- length(beta)\n\n# Just for illustration, phi = 1 for Poisson model\nPearson.X2 <- sum(((Y - group.sizes * mu)^2) / (group.sizes * mu))\nPearson.phi <- Pearson.X2 / (n-p)\ns.bar <- mean((Y - group.sizes * mu) / (group.sizes * mu))\nFletcher.phi <- Pearson.phi/(1-s.bar)\nPearson.phi ## [1] 1.211949\nFletcher.phi ## [1] 1.194283\n# CIs for the first 4 regression coefficients\n#   If phi were unknown, it's estimate would appear in the estimated standard error of the \n#   estimated coefficient\n# beta[1] + qt(c(0.025,0.975),n-p)*sqrt((1/Pearson.phi)*inv.Hessian[1,1])\nbeta[1] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[1,1])## [1] -0.03721264  0.15112097\nbeta[2] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[2,2])## [1] 1.270425 1.470639\n2*(1-pnorm(abs(beta[2]/sqrt(inv.Hessian[2,2]))))## [1] 0\nbeta[3] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[3,3])## [1] 1.513870 1.714592\nbeta[4] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[4,4])## [1] 1.685091 1.885886\n# The R function confint can also be used with GLMs to provide confidence intervals for coefficients\n\nconfint(my.glm, 'dur10-14')## Waiting for profiling to be done...##    2.5 %   97.5 % \n## 1.270141 1.470370\n# \"Model F Test\" - testing that all (non-intercept) coefficients equal zero\n# for Poisson model, since phi is known, we have a LRT equivalent to a partial F test\n# based on deviance difference\nYbar <- sum(Y)/sum(group.sizes)\nD <- -2*(sum(Y*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Y*log(group.sizes*mu))+sum(group.sizes*mu))\nYbar## [1] 3.960497\nD## [1] 3660.872\n1-pchisq(D,p-1)## [1] 0\n# with rounded Ys and using glm output\nYr <- round(Y)\nmu.glm <- exp(X%*%matrix(my.glm$coefficients,p,1))\nYbar <- sum(Yr)/sum(group.sizes)\nD <- -2*(sum(Yr*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Yr*log(group.sizes*mu.glm))+sum(group.sizes*mu.glm))\nYbar## [1] 3.960403\nD## [1] 3661.186\n1-pchisq(D,p-1)## [1] 0"},{"path":"poisson-regression.html","id":"model-checkingdiagnostics","chapter":"3 Poisson Regression","heading":"3.5 Model Checking/Diagnostics","text":"essential statistical practice check whether model adequately fits data. model fits poorly, inferences/predictions garnered model suspect. multiple linear regression assess model fit analyzing residuals. multiple linear regression model fits, residuals approximately standard normal. Lack fit manifests residuals skewed heavy-tailed, contain outliers, tend increase variability one covariates /predicted responses. Model-checking GLMs can done essentially manner—key find quantity reasonably fills role residuals multiple linear regression. GLMs, two choices, Pearson residuals deviance residuals. Pearson residuals defined \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{V(\\mu_i)}}\\), , sometimes, \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{\\hat\\phi V(\\mu_i)}}\\). first definition results quantities approximately zero-mean normal random variates variance \\(\\phi\\) whereas second definition provides standard normal quantities. practitioners prefer deviance residuals Pearson residuals latter often observed asymmetric , hence, “normal” expected. deviance equal \n\\[\\text{Deviance} = -2\\phi\\{\\ell(\\hat\\beta) - \\sup \\ell\\}\\]\n\\(\\ell(\\hat\\beta)\\) loglikelihood evaluated MLEs \\(\\sup \\ell\\) loglikelihood \\(\\mu_i = y_i\\), .e., totally saturated model. Multiplying \\(\\phi\\) removes dependence loglikelihood scale parameter. deviance can written sum terms, say, \\(\\text{Deviance} = \\sum_{=1}^n d_i\\), observation’s contribution \\(d_i\\) deviance used define deviance residuals follows:\n\\[e^D_i = \\text{sign}(y_i - \\mu_i)\\sqrt{d_i}.\\]","code":""},{"path":"poisson-regression.html","id":"residual-analysis-for-ceb-data","chapter":"3 Poisson Regression","heading":"3.5.1 Residual analysis for CEB data","text":"Using either deviance residuals Pearson residuals shows important things. First, residuals approximately standard normal, exception one “outlier”, observation 17. Second, sort observations fitted mean response \\(\\hat\\mu_i\\) least greatest, see absolutely trend , pattern , residuals. implies correctly modeled mean-variance relationship, also correctly modeled mean linear function covariates. obtain similar plots simply running “plot(glm object)”, default differences. Plotting glm object provide plots “residuals” versus “fitted values”, fact, labels plots slightly misleading. residuals vs. fitted values plots uses either Pearson deviance residuals (tell sure plot documentation) versus logarithm fitted responses \\(\\log(\\hat y_j) =\\log(n_j\\hat\\mu_j)\\). ","code":"\ndev <- -2*((Y*log(group.sizes*mu))-(group.sizes*mu) - ((Y*log(Y))-(Y)))\ndeviance.resids <- ifelse((Y-group.sizes*mu) < 0,-1,1)*sqrt(dev)\npearson.resids <- (Y - group.sizes*mu)/sqrt(group.sizes*mu)\n\n# residual plots using deviance residuals\nqqnorm(deviance.resids)\nqqline(deviance.resids)\nshapiro.test(deviance.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  deviance.resids\n## W = 0.96065, p-value = 0.02719\nhist(deviance.resids, freq = FALSE)\ndev.norm <- function(x) dnorm(x,mean(deviance.resids), sd(deviance.resids))\ncurve(dev.norm, -5,5, add = TRUE)\nplot(mu, deviance.resids)\n# residual plots using pearson residuals\nqqnorm(pearson.resids)\nqqline(pearson.resids)\nshapiro.test(pearson.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  pearson.resids\n## W = 0.94988, p-value = 0.007147\nhist(pearson.resids, freq = FALSE)\npearson.norm <- function(x) dnorm(x,mean(pearson.resids), sd(pearson.resids))\ncurve(pearson.norm, -5,5, add = TRUE)\nplot(mu, pearson.resids)\n# plots are using what look like Pearson (or maybe deviance) residuals versus predicted values under canonical link, i.e. log(Y hat)\nplot(log(group.sizes*mu), pearson.resids)\nplot(my.glm)"},{"path":"poisson-regression.html","id":"outlier-analysis-using-cooks-distance","chapter":"3 Poisson Regression","heading":"3.6 Outlier analysis using Cook’s distance","text":"Outliers observations corresponding large residuals. may occur due chance, , likely, indicate lack model fit specific observation. lack fit may due something innocuous like mistake made recording data; , may observation question different others sample, follow response-covariate relationship.Outliers problem, however, unless inclusion causes fitted model substantially different excluded. Therefore, necessarily care particular residual large, influential model fit. multiple linear regression may measure influence Cook’s distance data point, related magnitude residual leverage, measured hat (influence) matrix. GLMs can define Cook’s distance manner: Cook’s distance data point \\(k\\) given \n\\[C_k = \\frac{1}{(p+1)}\\sum_{=1}^n \\frac{(\\hat\\mu_i^{[k]} - \\hat\\mu_i)^2}{\\hat\\phi V(\\hat\\mu_i)}; \\text{ }\\]\n\\[C_k = \\frac{(e^P_k)^2}{\\hat\\phi (p+1)}\\frac{h_k}{(1-h_k)^2}\\]\n\\(H = W^{1/2}X(X^\\top W X)^{-1}X^\\top W^{1/2}\\) hat matrix \\(h_k\\) \\(k^{th}\\) diagonal entry. Large Cook’s distance implies model predictions change substantially data point question removed.","code":""},{"path":"poisson-regression.html","id":"outlier-analysis-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.6.1 Outlier analysis for the CEB data","text":"Typically data points Cook’s distance \\(>1\\) considered highly influential case exclusion considerable. case, apparent outlier influential, even influential observation sampled. computed Cook’s distance “hand” using corresponding built-R function; (slight) difference two seems caused fact built-function uses glm object, fitted rounding responses, whereas “hand” calculation uses Pearson residuals based original (unrounded) responses.","code":"\nceb2 <- ceb[-17,]\nmy.glm2 <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb2, offset = log(n))\n\nW2 <- sqrt(W)\nh <- W2%*%X%*%solve(t(X)%*%W%*%X)%*%t(X)%*%W2\n\n((pearson.resids[17]^2)/p)*(h[17,17]/((1-h[17,17])^2))## [1] 0.0912897\ncd <- cooks.distance(my.glm)\n\ncd[17]##         17 \n## 0.09123744\nsort(cd)##           44           12           16           43           58           26 \n## 0.0001578983 0.0002173653 0.0003181272 0.0004391595 0.0004872775 0.0008276467 \n##           31           48           69            5           40           37 \n## 0.0009604834 0.0010416260 0.0011600877 0.0011813567 0.0015364693 0.0017374273 \n##           63           15           59           64            8           18 \n## 0.0017913034 0.0020157684 0.0022231406 0.0023307859 0.0024333875 0.0027257717 \n##            1           29           54           56           34           62 \n## 0.0027795217 0.0028483882 0.0029171612 0.0029318194 0.0030296972 0.0030322783 \n##           55           36           27           33            9           60 \n## 0.0033869538 0.0034063422 0.0048780994 0.0049440311 0.0052402581 0.0053485413 \n##           47            6           24            3           11           23 \n## 0.0055965679 0.0056512722 0.0056712755 0.0057553959 0.0059235574 0.0061112712 \n##           50           41           14           49            4            2 \n## 0.0062842199 0.0063288779 0.0064608723 0.0067079644 0.0067366794 0.0068678049 \n##           66           25           13            7           61           71 \n## 0.0072889100 0.0074308461 0.0082351514 0.0104259702 0.0110105154 0.0114006236 \n##           70           42           52           20           28           32 \n## 0.0115384460 0.0145603375 0.0149361729 0.0164709118 0.0173145140 0.0189913643 \n##           51           38           53           19           39           67 \n## 0.0191422888 0.0203317679 0.0205784895 0.0220776790 0.0225077753 0.0265192825 \n##           35           10           45           22           46           17 \n## 0.0290870153 0.0375227510 0.0462157923 0.0771290592 0.0847041708 0.0912374357 \n##           21           65           30           57 \n## 0.1058278940 0.1102296604 0.1184297114 0.2448544534"},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"4 Linear Mixed Models","heading":"4 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"anova-with-random-factors","chapter":"4 Linear Mixed Models","heading":"4.1 ANOVA with random factors","text":"simplest linear mixed models used analyze linear models one-way ANOVA, randomized complete block designs, two-way ANOVA. Mixed models opposed fixed models (linear models heretofore studied) needed factors levels random. Random levels occur whenever units making levels behave like random samples population. Two examples given . , discuss perform ANOVA-like tests factors random rather fixed. upshot (least balanced experiments/datasets) tests fixed effects identical random effects, interpretation different (importantly !).","code":""},{"path":"linear-mixed-models.html","id":"strength-of-metallic-bonds","chapter":"4 Linear Mixed Models","heading":"4.1.1 Strength of metallic bonds","text":"dataset , called “Bonds”, contains responses 21 samples metals, 7 iron, nickel, copper, quantify strength metallic bonds. One sample metal extracted 7 ingots. expect ingots act like blocks—differences ingots account substantial amount variability responses, precise block effects inferential/scientific inference. include blocks order reduce residual variance accounting block variance. randomized controlled block design describes data collected, , repeated experiment, blocks (ingots) completely different. , blocks fixed random. Rather estimating block effects surely change experiment experiment, focus estimating amount variability explained blocks, remain experiment experiment. suggests different model used analyze RCBD experiments fixed blocks.usual linear model fixed blocks \n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\\]\n\\(y_{ij}\\) response treatment (metal) \\(\\) block (ingot) \\(j\\); \\(\\alpha_i\\)’s metal (treatment) effects; \\(\\beta_j\\)’s ingot (block) effects; , \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\) random residuals.linear model wrong model data block effects (, hence, also interaction effects) meaningless outside given data set; population-level parameters blocks random rather fixed. appropriate model (given normality independence random residuals reasonable) following mixed effects model:\\[\\begin{equation}\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.1}\n\\end{equation}\\]\n\\(\\beta_j\\stackrel{iid}{\\sim}N(0, \\sigma_b^2)\\) , independently, \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\). balanced experiments (number replicates equal across combination factor levels) can test block treatment effects comparing nested/aggregated models. Let \\(\\overline Y_{\\cdot}\\) denote mean response metal \\(\\) averaged ingots. can write following aggregated model (4.1) \n\\[\\begin{equation}\n\\overline y_{\\cdot} = \\mu + \\alpha_i + \\epsilon_{},\n\\tag{4.2}\n\\end{equation}\\]\n\\(\\epsilon_i = \\frac{1}{J}\\sum_{j=1}^j \\epsilon_{ij}\\). , \\(\\epsilon_j\\) variance \\(\\sigma_b^2 + \\sigma^2/J\\). F statistic\n\\[F = \\frac{J\\cdot MSE_{agg}}{MSE_{full}}\\]\n\\(MSE_{agg}\\) \\(MSE_{full}\\) mean squared errors models (4.2) (4.1) can used test hypothesis \\(H_0:\\sigma_b^2 = 0\\).","code":""},{"path":"linear-mixed-models.html","id":"machine-productivity","chapter":"4 Linear Mixed Models","heading":"4.1.2 Machine Productivity","text":"dataset given contains results designed experiment evaluate worker productivity using 3 different industrial machines. goal determine machine productive controlling natural variation worker productivity. observed workers represent random sample population workers (blocks), analogous ingots previous example. difference two examples (besides context) machine treatments replicated wihtin workers, three observations productivity score fore worker type machine. means can fit model interaction terms capable capturing changes machine effects productivity different workers (changes present):\n\\[\\begin{equation}\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\tag{4.3}\n\\end{equation}\\]\n\\(k\\) denotes \\(k^{\\text{th}}\\) replicate within machine \\(\\) worker \\(j\\); \\((\\alpha\\beta)_{ij}\\) denote machine-worker interaction effects. Let \\(\\overline Y_{ij\\cdot}\\) mean response averaging replicates treatment \\(\\) block \\(j\\) combination. ,\n\\[\\begin{align*}\nV(\\overline Y_{ij\\cdot}) &= V\\left(K^{-1}\\sum_{k=1}^K Y_{ijk}\\right) \\\\\n&= \\frac{1}{K^2}V\\left(\\sum_{k=1}^K \\{\\mu+\\alpha_i+\\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\}\\right)\\\\\n& = \\frac{1}{K^2}V\\left(K\\mu + K\\alpha_i + K\\beta_j + K(\\alpha\\beta)_{ij} + \\sum_{k=1}^K \\epsilon_{ijk}\\right)\\\\\n& = \\sigma_b^2 + \\sigma_{ab}^2 + K^{-1}\\sigma^2.\n\\end{align*}\\]\nrewrite model cell mean responses \n\\[\\begin{equation}\n\\overline y_{ij\\cdot} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.4}\n\\end{equation}\\]\naggregated error term follows \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0, \\sigma_{ab}^2 + \\sigma^2/K)\\). residual mean square (called mean squared error) model (4.3) (let’s call \\(MSE_{\\text{full}}\\)) mean \\(\\sigma^2\\) \\(n-p_1\\) degrees freedom \\(n\\) sample size \\(p\\) number coefficients fitted model (\\(p_1\\) equals number crossed factor levels, number blocks times number treatments). residual mean square aggregated model (4.4) (let’s call \\(MSE_{\\text{agg}}\\)) mean \\(\\sigma_{ab}^2 + \\sigma^2/K\\) \\(n/K-p_2\\) degrees freedom \\(p_2\\) number treatments plus number blocks minus 1. unbiased estimate \\(\\sigma_{ab}^2\\) given \\(MSE_{\\text{agg}} - \\frac{1}{K}MSE_{\\text{full}}\\). Consider testing null hypothesis \\(H_0:\\sigma_{ab}^2 = 0\\). statistic\n\\[F := \\frac{K\\cdot MSE_{agg}}{MSE_{full}}\\stackrel{H_0}{\\sim}F_{n/K-p_2, n-p_1},\\]\n, null hypothesis. test rejects \\(H_0\\) \\(F > F_{1-\\alpha,n/K-p_2, n-p_1}\\) exactly equivalent partial F test full model full model without interaction terms (additive model). use R compute ANOVA tables full model, full model without interaction, aggregated model. F test statistic aggregated model 46.13 10 36 degrees freedom, exactly matches partial F test full additive models.","code":"\nlibrary(nlme)\n\n# aggregated model\nMach.agg <- aggregate(score~Machine*Worker, data = Machines, FUN=mean)\n\nm2 <- lm(score~Machine+Worker, data = Mach.agg)\n\nanova(m2)## Analysis of Variance Table\n## \n## Response: score\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 585.09 292.544 20.5761 0.0002855 ***\n## Worker     5 413.97  82.793  5.8232 0.0089495 ** \n## Residuals 10 142.18  14.218                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# full model with interaction\nm0 <- lm(score~Machine*Worker, data = Machines)\n\nanova(m0)## Analysis of Variance Table\n## \n## Response: score\n##                Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine         2 1755.26  877.63  949.17 < 2.2e-16 ***\n## Worker          5 1241.89  248.38  268.63 < 2.2e-16 ***\n## Machine:Worker 10  426.53   42.65   46.13 < 2.2e-16 ***\n## Residuals      36   33.29    0.92                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(142.18*3/10)/(33.29/36)## [1] 46.12628\n1-pf((142.18*3/10)/(33.29/36), 10, 36)## [1] 0\n# additive model (no interaction)\nm1 <- lm(score~Machine+Worker, data = Machines)\n\nanova(m1)## Analysis of Variance Table\n## \n## Response: score\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 1755.26  877.63  87.798 < 2.2e-16 ***\n## Worker     5 1241.89  248.38  24.848 4.867e-12 ***\n## Residuals 46  459.82   10.00                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(m1,m0)## Analysis of Variance Table\n## \n## Model 1: score ~ Machine + Worker\n## Model 2: score ~ Machine * Worker\n##   Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n## 1     46 459.82                                 \n## 2     36  33.29 10    426.53 46.13 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"general-mixed-model-parameter-estimation","chapter":"4 Linear Mixed Models","heading":"4.2 General Mixed Model Parameter Estimation","text":"experiments comparing responses factors ANOVA-type analyses sufficient. , general models random effects, e.g., including continuous covariates, general-purpose methods needed. general mixed effects model may written\n\\[Y = X\\beta+ Z\\alpha + \\epsilon\\]\nY \\(n\\times 1\\) response, \\(X\\) \\(n \\times p\\) design matrix fixed (non-random) effects; \\(Z\\) \\(n\\times \\) matrix random effects; \\(\\beta\\) \\(p\\times 1\\) non-random coefficient vector; \\(\\alpha\\sim N_a(0, \\psi_\\theta)\\) \\(\\times 1\\) multivariate normal random coefficient vector mean 0 covariance matrix \\(\\psi_\\theta\\) indexed parameter \\(\\theta\\); \\(\\epsilon\\sim N_n(0, \\Lambda_\\theta)\\) multivariate normal random residual vector covariance matrix \\(\\Lambda_\\theta\\). alternative way writing model (quite succintly) \n\\[Y\\sim N_n(X\\beta, Z \\psi_\\theta Z^\\top + \\Lambda_\\theta).\\]","code":""},{"path":"linear-mixed-models.html","id":"maximum-likelihood","chapter":"4 Linear Mixed Models","heading":"4.2.1 Maximum Likelihood","text":"familiar least squares (weighted least squares) may recognize MLE \\(\\beta\\) given weighted least squares solution\n\\[\\hat\\beta_{WLS} = (X^\\top W^{-1}X)^{-1}X^\\top W^{-1}Y\\]\n\\(W^{-1} = (Z \\psi_\\theta Z^\\top + \\Lambda_\\theta)^{-1}\\). WLS estimator fine theory, hard compute practice requires inverting \\(n\\times n\\) matrix. large \\(n\\) computationally challenging—\\(O(n^3)\\) computations—unless matrix special “sparse” structure. ’s just number computations (time) needed compute \\(n\\times n\\) matrix inverse problematic. large matrices, algorithms computing inverse may maintain adequate numerical precision, , therefore, resulting matrix may truly equal inverse desired. errors floating-point arithmetic compound large matrix inversion problems lead estimate \\(\\hat\\beta_{WLS}\\) actually equal true weighted least squares estimate, due computational errors.Typically, direct WLS approach maximum likelihood estimation replaced iterative procedure order avoid computation inverse large matrix. WLS solution based joint density (likelihood) observations \\(Y\\). Instead, consider joint density \\((Y,\\alpha)\\) parameterized \\((\\beta, \\theta)\\). joint density conveniently expressed product conditional marginal densities \\(f(y|\\alpha;\\beta,\\theta)\\cdot f(\\alpha;\\beta,\\theta)\\) given \n\\[f(y|\\alpha;\\beta,\\theta) = (2\\pi)^{-2/n}|\\Lambda_\\theta|^{-1/2}\\exp\\{-\\tfrac12\\|y - X\\beta - Z\\alpha\\|^2_{\\lambda_\\theta^{-1}}\\},\\]\n\\(\\|\\|_B := ^\\top Ba\\), \n\\[f(\\alpha;\\beta,\\theta) = (2\\pi)^{-p/2}|\\psi_\\theta|^{-1/2}\\exp\\{-\\tfrac12\\alpha^\\top \\psi_\\theta^{-1}\\alpha\\}.\\]Now, joint density \\(f(y,\\alpha;\\beta,\\theta)\\) used directly likelihood, .e., \\(L(\\beta, \\theta| Y,\\alpha)\\) \\(\\alpha\\) unobserved random variable. Instead, average \\(\\alpha\\) values (integrate \\(\\alpha\\)) obtain marginal density \\(f(y;\\beta,\\theta)\\) use density \\(Y=y\\) fixed observed values likelihood \\((\\beta, \\theta)\\). compute marginal density begin tranformation Taylor expansion:\n\\[\\begin{align*}\nf(y;\\beta, \\theta) &= \\int f(y,\\alpha;\\beta, \\theta)d\\alpha = \\int \\exp\\{\\log f(y\\alpha;\\beta,\\theta)\\}d\\alpha \\\\\n& = \\int \\exp\\left\\{\\log f(y,\\hat\\alpha;\\beta,\\theta) + \\tfrac12 (\\alpha -\\hat\\alpha)^\\top \\frac{\\partial^2 \\log f(y,\\alpha;\\beta,\\theta)}{\\partial \\alpha \\partial\\alpha^\\top}(\\alpha - \\hat\\alpha)\\right\\}d\\alpha,\n\\end{align*}\\]\n\\(\\hat\\alpha\\) optimal prediction \\(\\alpha\\) maximizing \\(f(y,\\alpha;\\beta,\\theta)\\) fixed \\((\\beta,\\theta)\\). maximizer explicit form: \\(\\hat\\alpha = (Z^\\top \\Lambda_\\theta^{-1}Z + \\psi_\\theta^{-1})^{-1}Z^\\top \\Lambda_\\theta^{-1}(Y - X\\beta)\\). second line contains Taylor expansion \\(\\log f(y,\\alpha;\\beta,\\theta)\\) around \\(\\hat\\alpha\\) exact, .e., higher-order terms zero due Gaussian form densities. Pulling constant terms integration \n\\[f(y;\\beta, \\theta) = f(y, \\hat\\alpha;\\beta, \\theta)\\int \\exp\\left\\{-\\tfrac12 (\\alpha -\\hat\\alpha)^\\top (Z^\\top \\lambda_\\theta^{-1}Z + \\psi_\\theta^{-1})(\\alpha - \\hat\\alpha)\\right\\}d\\alpha.\\]\nUsing fact integrand kernel multivariate Gaussian density, see integral evaluates \\((2\\pi)^{p/2}|Z^\\top \\Lambda_\\theta^{-1}Z+\\psi_\\theta^{-1}|^{-1/2}\\), , hence\n\\[f(y;\\beta, \\theta) = f(y, \\hat\\alpha;\\beta, \\theta)(2\\pi)^{p/2}|Z^\\top \\Lambda_\\theta^{-1}Z+\\psi_\\theta^{-1}|^{-1/2}.\\]\nNow, using marginal density \\(Y\\) define loglikelihood \n\\[2\\ell(\\beta,\\theta;y) = -\\|y - X\\beta - Z\\hat\\alpha\\|_{\\Lambda_\\theta^{-1}}-\\hat\\alpha^\\top \\psi_\\theta^{-1}\\hat\\alpha - \\log|\\Lambda_\\theta|-\\log|\\psi_\\theta|-\\log|Z^\\top\\Lambda_\\theta^{-1}Z+\\psi_\\theta^{-1}|-n\\log 2\\pi.\\]\nGiven fixed value \\(\\theta\\), MLE \\(\\hat\\beta\\) may found maximizing\n\\[-\\|y - X\\beta - Z\\hat\\alpha\\|_{\\Lambda_\\theta^{-1}}-\\hat\\alpha^\\top \\psi_\\theta^{-1}\\hat\\alpha\\]\n, recall, \\(\\hat\\alpha = (Z^\\top \\Lambda_\\theta^{-1}Z)^{-1}Z^\\top \\Lambda_\\theta^{-1}(Y - X\\beta)\\). Plug \\(\\hat\\alpha\\) see becomes WLS problem; hence, \\(\\hat\\beta\\) WLS solution, weight matrix given \\(V = V_1 + V_2\\), (suppressing \\(\\theta\\) subscripts)\n\\[\\begin{align*}\nV_1 &= (- Z(Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top \\Lambda^{-1})^\\top \\Lambda^{-1} (- Z(Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top \\Lambda^{-1})\\\\\n& = \\Lambda^{-1} - \\Lambda^{-1}Z(Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top\\Lambda^{-1}\\\\\nV_2 &= ((Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top \\Lambda^{-1})^\\top \\psi^{-1} ((Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top \\Lambda^{-1})\\\\\n& = \\Lambda^{-1}Z(Z^\\top \\Lambda^{-1}Z)^{-1}\\psi^{-1}(Z^\\top \\Lambda^{-1}Z)^{-1}Z^\\top \\Lambda^{-1}\n\\end{align*}\\]Computing covariance \\(\\hat\\beta\\) still computationally intensive. Computing covariance \n\\[(X^\\top VX)^{-1}X^\\top V Y\\]\nstill requires inversion covariance \\(Y\\), problematic component. Curiously, Bayesian point view provides shortcut computation MLE covariance. Suppose model (Bayesian sense) parameter \\(\\beta\\) improper constant prior, random component \\(\\alpha\\) multivariate normal prior covariance \\(\\psi\\). , combining priors multivariate normal likelihood following posterior:\n\\[\\log\\Pi_n(\\beta) = \\text{const.} - \\tfrac12 \\left(y - X\\beta - Z\\alpha\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta - Z\\alpha\\right) - \\tfrac12\\alpha^\\top \\psi^{-1}\\alpha . \\]\nnice (otherwise hard find) factorization posterior due Searle et al. (Variance Components, Section 9.2). Expand exponent:\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12 \\alpha^\\top (\\psi^{-1} + Z^\\top \\Lambda^{-1}Z)\\alpha + \\alpha^\\top Z^\\top \\Lambda^{-1}(y - X\\beta)\n\\end{align*}\\]\nLet \\(:=\\psi^{-1} + Z^\\top \\Lambda^{-1}Z\\) complete square \\(\\alpha\\):\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))\\\\\n&+\\tfrac12(y-X\\beta)^\\top\\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1} (y-X\\beta).\n\\end{align*}\\]\nCombine terms quadratic \\(y-X\\beta\\). resulting matrix \\(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}\\) equal \\((Z\\psi Z^\\top + \\Lambda)^{-1}\\), can seen following computation:\n\\[\\begin{align*}\n&(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})(Z\\psi Z^\\top + \\Lambda) \\\\\n& = \\Lambda^{-1}Z\\psi Z^\\top + - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}Z\\psi Z^\\top - \\Lambda^{-1}ZA^{-1}Z^\\top \\\\\n& = + \\Lambda^{-1}Z(\\psi - ^{-1}(Z^\\top \\Lambda^{-1}Z\\psi + ))Z^\\top \\\\\n& = +\\Lambda^{-1}Z(\\psi - \\psi(Z^\\top \\Lambda^{-1}\\psi + )^{-1}(Z^\\top \\Lambda^{-1}\\psi + ))Z^\\top \\\\\n& = \n\\end{align*}\\]\nTherefore, posterior factorizes \n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta) \\\\\n&- \\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta)) \\end{align*}\\]Integrate \\(\\alpha\\) obtain posterior \\(\\beta\\). Marginally, \\(\\alpha\\) multivariate normal; therefore, obtain\n\\[\\Pi_n(\\beta) = (2\\Pi)^{-n/2}|\\Lambda|^{-1/2}|\\psi|^{-1/2}||^{-1/2}\\exp\\left\\{-\\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta)\\right\\}.\\]can verified \\(|Z\\psi Z^\\top +\\Lambda| = |\\Lambda||\\psi|||\\) using following identity\n\\[|D^{-1} - CA^{-1}B| = |D||^{-1}||- BD^{-1}C|\\]\n\\(Z\\psi Z^\\top +\\Lambda = D^{-1} - CA^{-1}B = (\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})^{-1}\\); , see, e.g., Appendix M equation 31 Searle et al. (Variance Components). result, posterior \\(\\beta\\) multivariate normal covariance \\(Z\\psi Z^\\top +\\Lambda\\).Let \\(V := (Z\\psi Z^\\top +\\Lambda)\\). exponent, add subtract \\(X\\hat\\beta\\) quadratic obtain\n\\[\\begin{align*}\n\\left(y - X\\beta\\right)^\\top V^{-1}(y - X\\beta) & = \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(y - X\\hat\\beta)\\\\\n& -2 \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(X\\hat\\beta - X\\beta)\\\\\n& + \\left(X\\hat\\beta - X\\beta\\right)^\\top V^{-1}(X\\hat\\beta - X\\beta).\n\\end{align*}\\]\nstraightforward show middle term zero. integrate \\(\\beta\\) (w.r.t. multivariate normal density), obtain following marginal posterior variance parameters:\n\\[\\Pi_n(\\theta, \\Lambda) = \\frac{(2\\pi)^{-n/2}}{(2\\pi)^{-p/2}}\\frac{|V|^{-1/2}}{|X^\\top V^{-1}X|^{-1/2}}\\exp\\left(-\\tfrac12 \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(y - X\\hat\\beta)\\right).\\]","code":""}]
