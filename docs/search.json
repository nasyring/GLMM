[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"poisson-regression.html","id":"poisson-regression","chapter":"3 Poisson Regression","heading":"3 Poisson Regression","text":"","code":""},{"path":"poisson-regression.html","id":"children-ever-born-data","chapter":"3 Poisson Regression","heading":"3.1 Children Ever Born Data","text":"“Children Ever Born” (CEB) dataset consists grouped data number births Fijian women. women described according marriage duration years ordinal levels: (0-4, 5-9, 10-14, 15-19, 20-24, 25-29); place residence (Suva—capital city—Urban, Rural); , level education (none, lower primary, upper primary, secondary greater). count, mean, variance number children ever born, group size, given group women cross-classified factorial level. summaries sufficient model counts children ever born Poisson distribution (individual woman’s count needed).CEB data example observational dataset — characteristics individuals inherent rather set experimenters experimental/controlled trial—, clear context. Several interesting questions may answered using data, : fewer born children associated higher lower education among Fijian women; urban versus rural living location influence number children ever born; , number children ever born steadily increase marrige duration, tend plateau?statistician (student statistician) familiar multiple linear regression /ANOVA factorial experiments may instinctively choose fit Gauss-Markov linear model CEB data, treating responses independent normal random variables. However, since responses counts, Poisson model reasonable. , just one perform Poisson regression? — opposed familiar multiple linear regression described Gauss-Markov model:\n\\[Y = X\\beta+ \\epsilon, \\quad \\epsilon \\sim N_{n}(0_{n\\times 1}, \\sigma^2 I_n).\\]motivation chapter, explore family Generalized Linear Models defining fitting model, performing inference model diagnostics, within context CEB example.","code":"\nceb <- read.table('ceb.dat')\nhead(ceb)##   dur   res  educ mean  var  n     y\n## 1 0-4  Suva  none 0.50 1.14  8  4.00\n## 2 0-4  Suva lower 1.14 0.73 21 23.94\n## 3 0-4  Suva upper 0.90 0.67 42 37.80\n## 4 0-4  Suva  sec+ 0.73 0.48 51 37.23\n## 5 0-4 urban  none 1.17 1.06 12 14.04\n## 6 0-4 urban lower 0.85 1.59 27 22.95"},{"path":"poisson-regression.html","id":"defining-glms","chapter":"3 Poisson Regression","heading":"3.2 Defining GLMs","text":"CEB data naturally want model CEB grouped counts realizations Poisson r.v.’s means \\(n_{j}x^\\top_j\\beta\\) \\(n_j\\) number women \\(j^{\\text{th}}\\) factorial group, \\(x_j\\) vector common covariates, \\(\\beta\\) common regression coefficient vector. , likelihood model \n\\[L(\\beta;\\text{data}) = \\prod_{j=1}^{70}\\frac{(n_{j}x^\\top_j\\beta)^{y_j}e^{-n_{j}x^\\top_j\\beta}}{y_j!}\\]\nloglikelihood given \n\\[\\ell(\\beta;\\text{data}) = \\text{constant} + \\sum_{j=1}^{70}y_j\\log(n_{j}x^\\top_j\\beta) - n_{j}x^\\top_j\\beta.\\]\nPoisson likelihood member Exponential Family, contains distributions PDFs may expressed \n\\[f(y;\\theta,\\phi) = \\exp\\{[y\\theta - b(\\theta)]/(\\phi) + c(y,\\phi)\\}.\\]\nLooking ahead, apply exponential family model independent identically distributed responses, similar data encounter multiple linear regression Gauss-Markov model, allow \\(\\theta\\) well forms \\(\\), \\(b\\), \\(c\\) functions vary observations, fix \\(\\phi\\), loglikelihood sample size \\(n\\) may written follows:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{[y_i\\theta_i - b_i(\\theta_i)]/a_i(\\phi) + c_i(\\phi, y_i)\\}.\\]\nPoisson regression model grouped data fairly simple member family, \\(\\theta = \\log (n_{j}x^\\top_j\\beta)\\), \\(\\phi = (\\phi) = 1\\), \\(b(\\theta) = \\exp(\\theta) =n_{j}x^\\top_j\\beta\\). fact, often case GLMs satisfy \\((\\phi)\\propto \\phi\\) known constant. \ngeneral, GLMs satisfy\n\\[E(Y) = b'(\\theta) \\quad \\text{}\\quad V(Y) = b''(\\theta)(\\phi).\\]\nPoisson regression model, particular, \n\\[\\theta = \\log(n_{j}x^\\top_j\\beta); \\quad b(\\theta) = \\exp(\\theta); \\quad \\text{}\\quad (\\phi) = 1\\]\n\\[E(Y) = b'(\\theta) = \\frac{\\partial}{\\partial \\theta}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta; \\text{ ,}\\]\n\\[V(Y) = b''(\\theta)(\\phi) = \\frac{\\partial^2}{\\partial \\theta^2}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta.\\]","code":""},{"path":"poisson-regression.html","id":"fitting-glms","chapter":"3 Poisson Regression","heading":"3.3 Fitting GLMs","text":"Like model defined likelihood, GLMs may fit maximizing (log)likelihood. , generally case maximizers (MLEs) available closed form. Instead, computed iteratively using Newton’s method similar iterative procedure. Refer exponential family loglikelihood using usual representation \\(a_i(\\phi) = \\phi/w_i\\) \\(w_i\\) known constants:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{w_i[y_i\\theta_i - b_i(\\theta_i)]/\\phi + c_i(\\phi, y_i)\\}.\\]\nLet \\(\\mu_i = E(Y_i)\\). , \\(b'(\\theta_i) = \\mu_i\\), , equivalently, \\(g_c(\\mu_i) = \\theta_i\\) \\(g_c\\) termed canonical link; example, \\(g_c := \\log\\) Poisson distribution. Additionally, let \\(g\\) link mean linear function covariates, .e., \\(g(\\mu_i) = \\eta_i = x_i^\\top\\beta\\); e.g., \\(g\\) identity function Poisson model. Since \\(b_i'(\\theta_i)\\) also equal \\(\\mu_i\\) exponential family, may differentiate loglikelihood respect regression parameter \\(\\beta\\) using chain rule:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{=1}^n \\left\\{\\frac{w_i}{\\phi}\\left[y_i\\frac{\\partial \\theta_i}{\\partial\\beta_j} - \\frac{\\partial b_i(\\theta_i)}{\\partial \\beta_j}\\right] + c_i(\\phi, y_i)\\right\\}\\]\nusing\n\\[\\frac{\\partial \\theta_i}{\\partial \\beta_j} = \\frac{\\partial \\theta_i}{\\partial \\mu_i}\\frac{\\partial \\mu_i}{\\partial \\beta_j}.\\]\nSince \\(\\mu_i = b_i'(\\theta_i)\\) \\(\\partial \\theta_i/\\partial \\mu_i = 1/b_i''(\\theta_i)\\). , light \\(\\mu_i = b'(\\theta_i)\\) may always write \\(b_i''(\\theta_i)\\) function \\(\\mu_i\\), .e., \\(V(\\mu_i) = b_i''(\\theta_i)/w\\) \\(V(Y_i) = V(\\mu_i)\\phi\\). Moreover, since \\(\\mu_i = g^{-1}(x_i^\\top \\beta)\\) \\(\\partial\\mu_i/\\partial \\beta_j = x_{ij}/g'[g^{-1}(x_i^\\top \\beta)]\\). Substituting, can write score function using \\(\\mu_i\\) follows:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{1}{\\phi}\\sum_{=1}^n \\frac{y_i - \\mu_i}{g'(\\mu_i)V(\\mu_i)}x_{ij}.\\]\nsecond (mixed partial) derivative may written\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta_j\\partial\\beta_k} = -\\frac{1}{\\phi}\\sum_{=1}^n \\frac{x_{ij}x_{ik}h(\\mu_i)}{g'(\\mu_i)^2V(\\mu_i)}\\]\n\\(h(\\mu_i) = 1+(y_i-\\mu_i)\\{V'(\\mu_i)/V(\\mu_i) + g''(\\mu_i)/g'(\\mu_i)\\}\\). expectation second derivative (multiplied -1 appears Fisher information matrix) quantity \\(h(\\mu_i)\\) replaced \\(E[h(\\mu_i)]\\), simply equals 1 \\(E(Y_i - \\mu_i) = 0\\).\nHessian loglikelihood clearly quadratic form \\(\\phi^{-1}X^\\top WX\\) \\(X\\) \\(n\\times p\\) design matrix covariates \\(W = [h(\\mu_i)/\\{g'(\\mu_i)^2V(\\mu_i)\\}]\\) \\(n\\times n\\) diagonal matrix “weights”. Less obvious, may define \\(G = \\text{diag}\\{g'(\\mu_i)/h(\\mu_i)\\}\\) gradient loglikelihood equals \\(\\phi^{-1}X^\\top WG(y - \\mu)\\). clever rewriting, Newton’s method updates take form weighted least squares solution:\n\\[\\begin{align*}\n\\beta^{[k+1]} &= \\beta^{[k]} + (X^\\top WX)^{-1}X^\\top WG(y-\\mu)\\\\\n& = (X^\\top WX)^{-1}X^\\top W\\{G(y-\\mu)X+\\beta^{[k]}\\}\\\\\n& = (X^\\top WX)^{-1}X^\\top Wz\n\\end{align*}\\]\n\\(z := G(y-\\mu)+X\\beta^{[k]}\\) sometimes referred “pseudo-data”. Repeating weighted least squares update, iteratively, convergence, termed iteratively re-weighted least squares (IRLS) since, course, weights \\(W\\) updating iteration.Poisson regression based grouped CEB data following likelihood, gradient, Hessian:\n\\[\\begin{align*}\n&\\ell(\\beta;\\text{data}) = \\sum_{j=1}^{70} \\left[y_j x_j^\\top \\beta - n_j e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla_s \\ell = \\sum_{j=1}^{70} \\left[y_j x_{js} - n_j x_{js}e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla^2_{s,t} \\ell = -\\sum_{j=1}^{70}  n_j x_{js}x_{jt}e^{x_j^\\top \\beta}.\n\\end{align*}\\]Rewriting Hessian gradient general exponential family GLM \n\\[W_{k,k} = n_k\\mu_k\\quad\\text{}\\quad G_k = (n_k\\mu_k)^{-1}\\]\nIRLS updates given \n\\[(X^\\top WX)^{-1}X^\\top Wz\\]\n\\(z_k = (n_k\\mu_k)^{-1}(y_k - n_k\\mu_k) + x_k^\\top \\beta\\).","code":""},{"path":"poisson-regression.html","id":"irls-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.3.1 IRLS for the CEB data","text":"compute MLEs Poisson regression grouped CEB data “hand” using IRLS—, also compare glm function R. calculation initialize elements parameter vector \\(\\mu\\) sample means \\(\\mu_j = y_j/n_j\\). set pseudo data equal \\(z_j = -(1/\\mu_j)(y_j / n_j - \\mu_j) + log(\\mu_j)\\) iterate computation least squares estimates \\(\\hat\\beta\\). Note CEB data contains grouped “counts” computed \\(y_j = \\mu_jn_j\\) \\(\\mu_j\\) values rounded. , result, \\(y_j\\) counts integers. affect “hand” calculation \\(\\hat\\beta\\) whatsoever never use full Poisson PMF computations; glm function R, hand, throw many warnings \\(y_j\\) values rounded, apparently uses PMF via dpois “hood”. differences fitted \\(\\hat\\beta\\) glm’s due rounding \\(y_j\\)’s.","code":"\nn <- nrow(ceb)\ngroup.sizes <- ceb$n\nY <- ceb$y\n# IRLS - factor coding\n# initialize with mu = Y/group.sizes\noptions(contrasts = c('contr.treatment', 'contr.treatment'))\nX <- model.matrix(y~dur+res+educ, data = ceb)\nmu <- Y/group.sizes\nXB <- log(mu)\nW <- diag(as.numeric(mu))\nz <- -(1/mu)*(Y/group.sizes-mu) + XB\nbeta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\ntol <- 0.0001\ndifference <- 1\nmaxiter <- 100\niter <- 1\nwhile((difference > tol) & (iter < maxiter)){\n  XB <- X%*%beta\n  mu <- exp(XB)\n  W <- diag(as.numeric(group.sizes*mu))\n  z <- (Y/diag(W) - rep(1,n)) + XB\n  beta.old <- beta\n  beta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\n  difference <- max(abs(beta - beta.old))\n  iter<-iter+1\n}\nbeta##                    [,1]\n## (Intercept)  0.05695417\n## dur10-14     1.37053208\n## dur15-19     1.61423104\n## dur20-24     1.78548879\n## dur25-29     1.97679469\n## dur5-9       0.99765038\n## resSuva     -0.15121728\n## resurban    -0.03895822\n## educnone    -0.02308034\n## educsec+    -0.33266474\n## educupper   -0.12474575\n## the glm function can be used with offset equal to logarithm of the group sizes\nmy.glm <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb, offset = log(n))\nsummary(my.glm)## \n## Call:\n## glm(formula = round(y) ~ dur + res + educ, family = poisson(link = \"log\"), \n##     data = ceb, offset = log(n))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2960  -0.6641   0.0725   0.6336   3.6782  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.05754    0.04803   1.198    0.231    \n## dur10-14     1.36940    0.05107  26.815  < 2e-16 ***\n## dur15-19     1.61376    0.05119  31.522  < 2e-16 ***\n## dur20-24     1.78491    0.05121  34.852  < 2e-16 ***\n## dur25-29     1.97641    0.05003  39.501  < 2e-16 ***\n## dur5-9       0.99693    0.05274  18.902  < 2e-16 ***\n## resSuva     -0.15166    0.02833  -5.353 8.63e-08 ***\n## resurban    -0.03924    0.02463  -1.594    0.111    \n## educnone    -0.02297    0.02266  -1.014    0.311    \n## educsec+    -0.33312    0.05390  -6.180 6.41e-10 ***\n## educupper   -0.12425    0.03000  -4.142 3.44e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 3731.852  on 69  degrees of freedom\n## Residual deviance:   70.665  on 59  degrees of freedom\n## AIC: 522.14\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"poisson-regression.html","id":"inference-on-glms","chapter":"3 Poisson Regression","heading":"3.4 Inference on GLMs","text":"doubt noticed glm function output produces standard errors, “z” values, p-values fitted coefficient, just find accompanying summarized lm output. , justification p-values?Since \\(\\hat\\beta\\) MLE, standard likelihood theory holds \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, ^{-1}(\\beta))\\) “large” \\(n\\) \\(^{-1}(\\beta)\\) Fisher information. discussed , “observed information” (Hessian) equal \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) \\(W\\) weight matrix final iteration IRLS, \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) coincides Fisher Information replace \\(h(\\mu_i)\\) \\(E(h(\\mu_i))=1\\) IRLS (called Fisher scoring) updates. Therefore, \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, \\phi^{-1}(X^\\top W X)^{-1})\\) “large” \\(n\\), useable \\(\\phi\\) known (, equals 1, Poisson Binomial models). Otherwise, replace \\(\\phi\\) MLE use corresponding Student’s \\(t\\) distribution \\(n - p\\) degrees freedom rather standard normal inference \\(\\beta_j\\). upshot may base tests , e.g., \\(H_0:\\beta_j = 0\\), Student’s \\(t\\) \\(n-p\\) df; .e.,\n\\[\\text{Reject }H_0:\\beta_j = 0 \\text{ }\\left|\\frac{\\hat\\beta_j}{\\sqrt{\\hat\\phi^{-1}(X^\\top W X)^{-1}_{j,j}}}\\right| > t_{1-\\alpha/2,n-p}.\\]\nMultivariate Wald simultaneous \\(100(1-\\alpha)\\%\\) confidence regions given eliiptical contours:\n\\[\\phi\\text{ known: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< \\chi^2_{1-\\alpha,p} \\}\\]\n\\[\\phi\\text{ unknown: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\hat\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< F_{1-\\alpha,p, n-p} \\}\\]Moreover, approximate \\(95\\%\\) CI mean response \\(\\mu = g^{-1}(x^\\top \\beta)\\) covariate vector \\(x\\) given Delta method interval:\\[g^{-1}(x^\\top \\hat\\beta)\\pm t_{1-\\alpha/2,n-p}\\sqrt{\\hat\\phi^{-1} \\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]^\\top(X^\\top W X)^{-1}\\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]}.\\]\nmultiple linear regression (Gauss-Markov) models use partial F tests (likelihood ratio tests) test significance sets covariates, .e., \\(H_0: \\beta_j = \\beta_{j+1} = \\cdots = \\beta_{j+\\ell} = 0\\). GLMs, similar tests available. models \\(\\phi\\) known, \n\\[-2\\{\\ell(\\hat\\beta_{h_0}) - \\ell(\\hat\\beta)\\}\\stackrel{H_0}{\\sim} F_{\\ell,n-p}\\]\n\\(\\hat\\beta_{h_0}\\) MLE null hypothesis \\(\\ell\\) coefficients set equal 0.several methods estimate \\(\\phi\\) unknown. Pearson’s method observes \n\\[\\phi^{-1}X^2 \\stackrel{\\cdot}{\\sim}\\chi^2_{n-p}\\quad \\text{}\\quad X^2 :=\\sum_{=1}^n \\frac{(Y_i - \\hat\\mu_i)^2}{\\phi V(\\hat\\mu_i)}\\]\nmodel fits data adequately. Hence, \\(\\hat\\phi_P = X^2/(n-p)\\) good estimate \\(\\phi\\). certain data sets, Poisson data low counts, Pearson estimate may behave badly, modified version (Fletcher’s estimator) preferred:\n\\[\\hat\\phi_F = \\frac{\\hat\\phi_P}{1-\\overline s}, \\quad\\text{}\\]\n\\[\\overline s:=n^{-1}\\sum_{=1}^n V'(\\hat\\mu_i)\\frac{(y_i - \\hat\\mu_i)}{V(\\hat\\mu_i)}.\\]Deviance difference models B \\(\\subset B\\) given \\(D_A - D_B = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\phi\\). scaled deviance difference \n\\[D_A^* - D_B^* = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell}\\]\ndifference number fitted parameters \\(\\ell\\). Despite notation, scaled deviance difference depend \\(\\phi\\), whereas deviance difference . Two alternative tests make use scaled deviance compare nested GLMs. first analogous partial F test:\n\\[F = \\frac{(D_A^* - D_B^*)/\\ell}{D_B^* / (n-p)}\\stackrel{\\cdot}{\\sim} F_{\\ell, n-p}\\]\napproximation F distribution rough. Alternatively, can replace scale parameter scaled deviance Pearson (Fletcher) estimator obtain\n\\[\\hat D_A^* - \\hat D_B^*\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell},\\]\n“hat” scaled deviances indicates dependence \\(\\hat\\phi\\).","code":""},{"path":"poisson-regression.html","id":"inference-and-prediction-for-the-ceb-data-using-poisson-regression","chapter":"3 Poisson Regression","heading":"3.4.1 Inference and prediction for the CEB data using Poisson regression","text":"Next, ’ll demonstrate computations confidence intervals tests significance sets covariates within Poisson regression model CEB data. can either computations “hand” using results IRLS can use built-R functions like glm confint. Wald-type confidence intervals regression coefficients require inverse Hessian (estimate inverse Fisher information, equal Fisher scoring used). compute final iteration IRLS (assuming algorithm converged). GLMs need estimate scale parameter \\(\\phi\\) include Pearson Fletcher estimates —Poisson model \\(\\phi=1\\). Note estimates close 1. p-values included summarized glm output imply intercept significantly different zero, several coefficients different zero, including, e.g., \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\). Wald-type CIs p-values agree glm p-values; example, 95% CI \\(\\beta_0\\) computed “hand” (-0.0372 0.1511), suggesting intercept significantly different zero, 95% CI \\(\\beta_1\\) (1.2704, 1.4706) p-value test \\(\\beta_1 = 0\\) indistinguishable 0. computed deviance difference (minus twice difference loglikelihood) intercept-model full model. deviance difference 3661 ten degrees freedom (difference number fitted coefficients models). intercept model fits, deviance difference comparable Chi-squared r.v. 10 degrees freedom, corresponding p-value basically zero, supporting claim full model fits much better intercept-model. Compare deviance difference calculation output glm: glm includes null deviance residual deviance, difference two gives deviance difference statistic used compare intercept-full models. 3661, agreeing almost exactly “hand” calculation.","code":"\nHessian <- t(X)%*%W%*%X\ninv.Hessian <- solve(Hessian)\np <- length(beta)\n\n# Just for illustration, phi = 1 for Poisson model\nPearson.X2 <- sum(((Y - group.sizes * mu)^2) / (group.sizes * mu))\nPearson.phi <- Pearson.X2 / (n-p)\ns.bar <- mean((Y - group.sizes * mu) / (group.sizes * mu))\nFletcher.phi <- Pearson.phi/(1-s.bar)\nPearson.phi ## [1] 1.211949\nFletcher.phi ## [1] 1.194283\n# CIs for the first 4 regression coefficients\n#   If phi were unknown, it's estimate would appear in the estimated standard error of the \n#   estimated coefficient\n# beta[1] + qt(c(0.025,0.975),n-p)*sqrt((1/Pearson.phi)*inv.Hessian[1,1])\nbeta[1] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[1,1])## [1] -0.03721264  0.15112097\nbeta[2] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[2,2])## [1] 1.270425 1.470639\n2*(1-pnorm(abs(beta[2]/sqrt(inv.Hessian[2,2]))))## [1] 0\nbeta[3] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[3,3])## [1] 1.513870 1.714592\nbeta[4] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[4,4])## [1] 1.685091 1.885886\n# The R function confint can also be used with GLMs to provide confidence intervals for coefficients\n\nconfint(my.glm, 'dur10-14')## Waiting for profiling to be done...##    2.5 %   97.5 % \n## 1.270141 1.470370\n# \"Model F Test\" - testing that all (non-intercept) coefficients equal zero\n# for Poisson model, since phi is known, we have a LRT equivalent to a partial F test\n# based on deviance difference\nYbar <- sum(Y)/sum(group.sizes)\nD <- -2*(sum(Y*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Y*log(group.sizes*mu))+sum(group.sizes*mu))\nYbar## [1] 3.960497\nD## [1] 3660.872\n1-pchisq(D,p-1)## [1] 0\n# with rounded Ys and using glm output\nYr <- round(Y)\nmu.glm <- exp(X%*%matrix(my.glm$coefficients,p,1))\nYbar <- sum(Yr)/sum(group.sizes)\nD <- -2*(sum(Yr*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Yr*log(group.sizes*mu.glm))+sum(group.sizes*mu.glm))\nYbar## [1] 3.960403\nD## [1] 3661.186\n1-pchisq(D,p-1)## [1] 0"},{"path":"poisson-regression.html","id":"model-checkingdiagnostics","chapter":"3 Poisson Regression","heading":"3.5 Model Checking/Diagnostics","text":"essential statistical practice check whether model adequately fits data. model fits poorly, inferences/predictions garnered model suspect. multiple linear regression assess model fit analyzing residuals. multiple linear regression model fits, residuals approximately standard normal. Lack fit manifests residuals skewed heavy-tailed, contain outliers, tend increase variability one covariates /predicted responses. Model-checking GLMs can done essentially manner—key find quantity reasonably fills role residuals multiple linear regression. GLMs, two choices, Pearson residuals deviance residuals. Pearson residuals defined \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{V(\\mu_i)}}\\), , sometimes, \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{\\hat\\phi V(\\mu_i)}}\\). first definition results quantities approximately zero-mean normal random variates variance \\(\\phi\\) whereas second definition provides standard normal quantities. practitioners prefer deviance residuals Pearson residuals latter often observed asymmetric , hence, “normal” expected. deviance equal \n\\[\\text{Deviance} = -2\\phi\\{\\ell(\\hat\\beta) - \\sup \\ell\\}\\]\n\\(\\ell(\\hat\\beta)\\) loglikelihood evaluated MLEs \\(\\sup \\ell\\) loglikelihood \\(\\mu_i = y_i\\), .e., totally saturated model. Multiplying \\(\\phi\\) removes dependence loglikelihood scale parameter. deviance can written sum terms, say, \\(\\text{Deviance} = \\sum_{=1}^n d_i\\), observation’s contribution \\(d_i\\) deviance used define deviance residuals follows:\n\\[e^D_i = \\text{sign}(y_i - \\mu_i)\\sqrt{d_i}.\\]","code":""},{"path":"poisson-regression.html","id":"residual-analysis-for-ceb-data","chapter":"3 Poisson Regression","heading":"3.5.1 Residual analysis for CEB data","text":"Using either deviance residuals Pearson residuals shows important things. First, residuals approximately standard normal, exception one “outlier”, observation 17. Second, sort observations fitted mean response \\(\\hat\\mu_i\\) least greatest, see absolutely trend , pattern , residuals. implies correctly modeled mean-variance relationship, also correctly modeled mean linear function covariates. obtain similar plots simply running “plot(glm object)”, default differences. Plotting glm object provide plots “residuals” versus “fitted values”, fact, labels plots slightly misleading. residuals vs. fitted values plots uses either Pearson deviance residuals (tell sure plot documentation) versus logarithm fitted responses \\(\\log(\\hat y_j) =\\log(n_j\\hat\\mu_j)\\). ","code":"\ndev <- -2*((Y*log(group.sizes*mu))-(group.sizes*mu) - ((Y*log(Y))-(Y)))\ndeviance.resids <- ifelse((Y-group.sizes*mu) < 0,-1,1)*sqrt(dev)\npearson.resids <- (Y - group.sizes*mu)/sqrt(group.sizes*mu)\n\n# residual plots using deviance residuals\nqqnorm(deviance.resids)\nqqline(deviance.resids)\nshapiro.test(deviance.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  deviance.resids\n## W = 0.96065, p-value = 0.02719\nhist(deviance.resids, freq = FALSE)\ndev.norm <- function(x) dnorm(x,mean(deviance.resids), sd(deviance.resids))\ncurve(dev.norm, -5,5, add = TRUE)\nplot(mu, deviance.resids)\n# residual plots using pearson residuals\nqqnorm(pearson.resids)\nqqline(pearson.resids)\nshapiro.test(pearson.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  pearson.resids\n## W = 0.94988, p-value = 0.007147\nhist(pearson.resids, freq = FALSE)\npearson.norm <- function(x) dnorm(x,mean(pearson.resids), sd(pearson.resids))\ncurve(pearson.norm, -5,5, add = TRUE)\nplot(mu, pearson.resids)\n# plots are using what look like Pearson (or maybe deviance) residuals versus predicted values under canonical link, i.e. log(Y hat)\nplot(log(group.sizes*mu), pearson.resids)\nplot(my.glm)"},{"path":"poisson-regression.html","id":"outlier-analysis-using-cooks-distance","chapter":"3 Poisson Regression","heading":"3.6 Outlier analysis using Cook’s distance","text":"Outliers observations corresponding large residuals. may occur due chance, , likely, indicate lack model fit specific observation. lack fit may due something innocuous like mistake made recording data; , may observation question different others sample, follow response-covariate relationship.Outliers problem, however, unless inclusion causes fitted model substantially different excluded. Therefore, necessarily care particular residual large, influential model fit. multiple linear regression may measure influence Cook’s distance data point, related magnitude residual leverage, measured hat (influence) matrix. GLMs can define Cook’s distance manner: Cook’s distance data point \\(k\\) given \n\\[C_k = \\frac{1}{(p+1)}\\sum_{=1}^n \\frac{(\\hat\\mu_i^{[k]} - \\hat\\mu_i)^2}{\\hat\\phi V(\\hat\\mu_i)}; \\text{ }\\]\n\\[C_k = \\frac{(e^P_k)^2}{\\hat\\phi (p+1)}\\frac{h_k}{(1-h_k)^2}\\]\n\\(H = W^{1/2}X(X^\\top W X)^{-1}X^\\top W^{1/2}\\) hat matrix \\(h_k\\) \\(k^{th}\\) diagonal entry. Large Cook’s distance implies model predictions change substantially data point question removed.","code":""},{"path":"poisson-regression.html","id":"outlier-analysis-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.6.1 Outlier analysis for the CEB data","text":"Typically data points Cook’s distance \\(>1\\) considered highly influential case exclusion considerable. case, apparent outlier influential, even influential observation sampled. computed Cook’s distance “hand” using corresponding built-R function; (slight) difference two seems caused fact built-function uses glm object, fitted rounding responses, whereas “hand” calculation uses Pearson residuals based original (unrounded) responses.","code":"\nceb2 <- ceb[-17,]\nmy.glm2 <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb2, offset = log(n))\n\nW2 <- sqrt(W)\nh <- W2%*%X%*%solve(t(X)%*%W%*%X)%*%t(X)%*%W2\n\n((pearson.resids[17]^2)/p)*(h[17,17]/((1-h[17,17])^2))## [1] 0.0912897\ncd <- cooks.distance(my.glm)\n\ncd[17]##         17 \n## 0.09123744\nsort(cd)##           44           12           16           43           58           26 \n## 0.0001578983 0.0002173653 0.0003181272 0.0004391595 0.0004872775 0.0008276467 \n##           31           48           69            5           40           37 \n## 0.0009604834 0.0010416260 0.0011600877 0.0011813567 0.0015364693 0.0017374273 \n##           63           15           59           64            8           18 \n## 0.0017913034 0.0020157684 0.0022231406 0.0023307859 0.0024333875 0.0027257717 \n##            1           29           54           56           34           62 \n## 0.0027795217 0.0028483882 0.0029171612 0.0029318194 0.0030296972 0.0030322783 \n##           55           36           27           33            9           60 \n## 0.0033869538 0.0034063422 0.0048780994 0.0049440311 0.0052402581 0.0053485413 \n##           47            6           24            3           11           23 \n## 0.0055965679 0.0056512722 0.0056712755 0.0057553959 0.0059235574 0.0061112712 \n##           50           41           14           49            4            2 \n## 0.0062842199 0.0063288779 0.0064608723 0.0067079644 0.0067366794 0.0068678049 \n##           66           25           13            7           61           71 \n## 0.0072889100 0.0074308461 0.0082351514 0.0104259702 0.0110105154 0.0114006236 \n##           70           42           52           20           28           32 \n## 0.0115384460 0.0145603375 0.0149361729 0.0164709118 0.0173145140 0.0189913643 \n##           51           38           53           19           39           67 \n## 0.0191422888 0.0203317679 0.0205784895 0.0220776790 0.0225077753 0.0265192825 \n##           35           10           45           22           46           17 \n## 0.0290870153 0.0375227510 0.0462157923 0.0771290592 0.0847041708 0.0912374357 \n##           21           65           30           57 \n## 0.1058278940 0.1102296604 0.1184297114 0.2448544534"},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"4 Linear Mixed Models","heading":"4 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"anova-with-random-factors","chapter":"4 Linear Mixed Models","heading":"4.1 ANOVA with random factors","text":"simplest linear mixed models used analyze linear models one-way ANOVA, randomized complete block designs, two-way ANOVA. Mixed models opposed fixed models (linear models heretofore studied) needed factors levels random. Random levels occur whenever units making levels behave like random samples population. Two examples given . , discuss perform ANOVA-like tests factors random rather fixed. upshot (least balanced experiments/datasets) tests fixed effects identical random effects, interpretation different (importantly !).","code":""},{"path":"linear-mixed-models.html","id":"strength-of-metallic-bonds","chapter":"4 Linear Mixed Models","heading":"4.1.1 Strength of metallic bonds","text":"dataset , called “Bonds”, contains responses 21 samples metals, 7 iron, nickel, copper, quantify strength metallic bonds. One sample metal extracted 7 ingots. expect ingots act like blocks—differences ingots account substantial amount variability responses, precise block effects inferential/scientific inference. include blocks order reduce residual variance accounting block variance. randomized controlled block design describes data collected, , repeated experiment, blocks (ingots) completely different. , blocks fixed random. Rather estimating block effects surely change experiment experiment, focus estimating amount variability explained blocks, remain experiment experiment. suggests different model used analyze RCBD experiments fixed blocks.usual linear model fixed blocks \n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\\]\n\\(y_{ij}\\) response treatment (metal) \\(\\) block (ingot) \\(j\\); \\(\\alpha_i\\)’s metal (treatment) effects; \\(\\beta_j\\)’s ingot (block) effects; , \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\) random residuals.linear model wrong model data block effects (, hence, also interaction effects) meaningless outside given data set; population-level parameters blocks random rather fixed. appropriate model (given normality independence random residuals reasonable) following mixed effects model:\\[\\begin{equation}\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.1}\n\\end{equation}\\]\n\\(\\beta_j\\stackrel{iid}{\\sim}N(0, \\sigma_b^2)\\) , independently, \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\). balanced experiments (number replicates equal across combination factor levels) can test block treatment effects comparing nested/aggregated models. Let \\(\\overline Y_{\\cdot}\\) denote mean response metal \\(\\) averaged ingots. can write following aggregated model (4.1) \n\\[\\begin{equation}\n\\overline y_{\\cdot} = \\mu + \\alpha_i + \\epsilon_{},\n\\tag{4.2}\n\\end{equation}\\]\n\\(\\epsilon_i = \\frac{1}{J}\\sum_{j=1}^j \\epsilon_{ij}\\). , \\(\\epsilon_j\\) variance \\(\\sigma_b^2 + \\sigma^2/J\\). F statistic\n\\[F = \\frac{J\\cdot MSE_{agg}}{MSE_{full}}\\]\n\\(MSE_{agg}\\) \\(MSE_{full}\\) mean squared errors models (4.2) (4.1) can used test hypothesis \\(H_0:\\sigma_b^2 = 0\\).","code":""},{"path":"linear-mixed-models.html","id":"machine-productivity","chapter":"4 Linear Mixed Models","heading":"4.1.2 Machine productivity","text":"dataset given contains results designed experiment evaluate worker productivity using 3 different industrial machines. goal determine machine productive controlling natural variation worker productivity. observed workers represent random sample population workers (blocks), analogous ingots previous example. difference two examples (besides context) machine treatments replicated wihtin workers, three observations productivity score fore worker type machine. means can fit model interaction terms capable capturing changes machine effects productivity different workers (changes present):\n\\[\\begin{equation}\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\tag{4.3}\n\\end{equation}\\]\n\\(k\\) denotes \\(k^{\\text{th}}\\) replicate within machine \\(\\) worker \\(j\\); \\((\\alpha\\beta)_{ij}\\) denote machine-worker interaction effects. Let \\(\\overline Y_{ij\\cdot}\\) mean response averaging replicates treatment \\(\\) block \\(j\\) combination. ,\n\\[\\begin{align*}\nV(\\overline Y_{ij\\cdot}) &= V\\left(K^{-1}\\sum_{k=1}^K Y_{ijk}\\right) \\\\\n&= \\frac{1}{K^2}V\\left(\\sum_{k=1}^K \\{\\mu+\\alpha_i+\\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\}\\right)\\\\\n& = \\frac{1}{K^2}V\\left(K\\mu + K\\alpha_i + K\\beta_j + K(\\alpha\\beta)_{ij} + \\sum_{k=1}^K \\epsilon_{ijk}\\right)\\\\\n& = \\sigma_b^2 + \\sigma_{ab}^2 + K^{-1}\\sigma^2.\n\\end{align*}\\]\nrewrite model cell mean responses \n\\[\\begin{equation}\n\\overline y_{ij\\cdot} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.4}\n\\end{equation}\\]\naggregated error term follows \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0, \\sigma_{ab}^2 + \\sigma^2/K)\\). residual mean square (called mean squared error) model (4.3) (let’s call \\(MSE_{\\text{full}}\\)) mean \\(\\sigma^2\\) \\(n-p_1\\) degrees freedom \\(n\\) sample size \\(p\\) number coefficients fitted model (\\(p_1\\) equals number crossed factor levels, number blocks times number treatments). residual mean square aggregated model (4.4) (let’s call \\(MSE_{\\text{agg}}\\)) mean \\(\\sigma_{ab}^2 + \\sigma^2/K\\) \\(n/K-p_2\\) degrees freedom \\(p_2\\) number treatments plus number blocks minus 1. unbiased estimate \\(\\sigma_{ab}^2\\) given \\(MSE_{\\text{agg}} - \\frac{1}{K}MSE_{\\text{full}}\\). Consider testing null hypothesis \\(H_0:\\sigma_{ab}^2 = 0\\). statistic\n\\[F := \\frac{K\\cdot MSE_{agg}}{MSE_{full}}\\stackrel{H_0}{\\sim}F_{n/K-p_2, n-p_1},\\]\n, null hypothesis. test rejects \\(H_0\\) \\(F > F_{1-\\alpha,n/K-p_2, n-p_1}\\) exactly equivalent partial F test full model full model without interaction terms (additive model). use R compute ANOVA tables full model, full model without interaction, aggregated model. F test statistic aggregated model 46.13 10 36 degrees freedom, exactly matches partial F test full additive models.","code":"\nlibrary(nlme)\n\n# aggregated model\nMach.agg <- aggregate(score~Machine*Worker, data = Machines, FUN=mean)\n\nm2 <- lm(score~Machine+Worker, data = Mach.agg)\n\nanova(m2)## Analysis of Variance Table\n## \n## Response: score\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 585.09 292.544 20.5761 0.0002855 ***\n## Worker     5 413.97  82.793  5.8232 0.0089495 ** \n## Residuals 10 142.18  14.218                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# full model with interaction\nm0 <- lm(score~Machine*Worker, data = Machines)\n\nanova(m0)## Analysis of Variance Table\n## \n## Response: score\n##                Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine         2 1755.26  877.63  949.17 < 2.2e-16 ***\n## Worker          5 1241.89  248.38  268.63 < 2.2e-16 ***\n## Machine:Worker 10  426.53   42.65   46.13 < 2.2e-16 ***\n## Residuals      36   33.29    0.92                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(142.18*3/10)/(33.29/36)## [1] 46.12628\n1-pf((142.18*3/10)/(33.29/36), 10, 36)## [1] 0\n# additive model (no interaction)\nm1 <- lm(score~Machine+Worker, data = Machines)\n\nanova(m1)## Analysis of Variance Table\n## \n## Response: score\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 1755.26  877.63  87.798 < 2.2e-16 ***\n## Worker     5 1241.89  248.38  24.848 4.867e-12 ***\n## Residuals 46  459.82   10.00                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(m1,m0)## Analysis of Variance Table\n## \n## Model 1: score ~ Machine + Worker\n## Model 2: score ~ Machine * Worker\n##   Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n## 1     46 459.82                                 \n## 2     36  33.29 10    426.53 46.13 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"a-general-linear-mixed-model","chapter":"4 Linear Mixed Models","heading":"4.2 A general linear mixed model","text":"experiments comparing responses factors ANOVA-type analyses sufficient. , general models random effects, e.g., including continuous covariates, general-purpose methods needed. general mixed effects model may written\n\\[Y = X\\beta+ Z\\alpha + \\epsilon\\]\nY \\(n\\times 1\\) response, \\(X\\) \\(n \\times p\\) design matrix fixed (non-random) effects; \\(Z\\) \\(n\\times \\) matrix random effects; \\(\\beta\\) \\(p\\times 1\\) non-random coefficient vector; \\(\\alpha\\sim N_a(0, \\psi_\\theta)\\) \\(\\times 1\\) multivariate normal random coefficient vector mean 0 covariance matrix \\(\\psi_\\theta\\) indexed parameter \\(\\theta\\); \\(\\epsilon\\sim N_n(0, \\Lambda_\\theta)\\) multivariate normal random residual vector covariance matrix \\(\\Lambda_\\theta\\). alternative way writing model (quite succintly) \n\\[\\begin{equation}\n\\tag{4.5}\nY\\sim N_n(X\\beta, Z \\psi_\\theta Z^\\top + \\Lambda_\\theta).\n\\end{equation}\\]","code":""},{"path":"linear-mixed-models.html","id":"parameter-estimation-using-pml","chapter":"4 Linear Mixed Models","heading":"4.2.1 Parameter estimation using PML","text":"linear mixed model (4.5) may fit using maximum likelihood estimation (MLE), (potentially) complicated covariance structure poses challenges numerical maximization likelihood function. Rather straightforward MLE, linear mixed models usually fit using either restricted maximum likelihood estimation (REML, also called residual MLE) profile maximum likelihood estimation (PML). approaches aim simplify computation maximum retaining good asymptotic properties maximum likelihood estimators. MLE frequentist concept, turns REML can derived intuitively using Bayesian approach.previously covered general linear models, .e., \\(Y = X\\beta + \\epsilon\\) \\(Cov(\\epsilon) = \\Sigma\\) known \\(\\Sigma\\) matrix \\(\\Sigma = \\sigma^2 V\\) unknown scalar \\(\\sigma^2\\) known matrix \\(V\\), familiar weighted least squares (WLS) estimation. linear mixed model covariance parameter \\(\\theta\\) known, model fit using WLS:\n\\[\\hat\\beta(\\theta) = (X^\\top W^{-1}X)^{-1}X^\\top W^{-1}Y, \\quad \\text{} \\quad W = Z \\psi_\\theta Z^\\top + \\Lambda_\\theta.\\]WLS solution inspires PML technique: maximize likelihood respect \\(\\theta\\) plugging \\(\\beta = \\hat\\beta(\\theta)\\). PML reproduces MLEs exactly; advantage simplified formulation likelihood maximization problem.Nevertheless two problems PML strategy suggested : one computational statistical.first problem PML/WLS requires inverting \\(n\\times n\\) matrix \\(W = Z \\psi_\\theta Z^\\top + \\Lambda_\\theta\\). even moderate sample sizes matrix inversion can computationally demanding computationally unstable. Fortunately, clever linear algebra resolves problem. Define matrix \\(:=\\psi^{-1} + Z^\\top \\Lambda^{-1}Z\\). Observe matrix \\(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}\\) equals inverse \\(Z\\psi Z^\\top + \\Lambda\\):\n\\[\\begin{align*}\n&(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})(Z\\psi Z^\\top + \\Lambda) \\\\\n& = \\Lambda^{-1}Z\\psi Z^\\top + - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}Z\\psi Z^\\top - \\Lambda^{-1}ZA^{-1}Z^\\top \\\\\n& = + \\Lambda^{-1}Z(\\psi - ^{-1}(Z^\\top \\Lambda^{-1}Z\\psi + ))Z^\\top \\\\\n& = +\\Lambda^{-1}Z(\\psi - \\psi(Z^\\top \\Lambda^{-1}\\psi + )^{-1}(Z^\\top \\Lambda^{-1}\\psi + ))Z^\\top \\\\\n& = \n\\end{align*}\\]\nkey property inverse requires computing inverse \\(p\\times p\\) \\(\\times \\) matrices, require computation \\(n\\times n\\) matrix inverse. many practical applications \\(p\\) \\(\\) much smaller \\(n\\), matrices \\(\\Lambda\\) \\(\\psi\\) often simple structures, matrix inversions may performed quickly reliably.second issue PML easily overcome. Think back Gauss-Markov model \\(Y = X\\beta+\\epsilon\\) \\(Cov(\\epsilon) = \\sigma^2 I_n\\). Recall MLE \\(\\hat\\sigma^2\\) biased, .e., \\(E(\\hat\\sigma^2) = \\frac{n-p}{n}\\sigma^2\\). practice may substantial number covariates relative sample size bias can quite significant; , importantly, bias results underestimate causes -optimism respect inferences concerning \\(\\beta\\). phenomenon occurs ML/PML estimation: PML estimate \\(\\theta\\) biased. desire estimation strategy constructively produces unbiased estimates motivation REML, covered next.","code":""},{"path":"linear-mixed-models.html","id":"reml---frequentist-approach","chapter":"4 Linear Mixed Models","heading":"4.2.2 REML - frequentist approach","text":"two formulations leading residual (also called restircted/reduced) maximum likelihood approach (REML), one frequentist (non-Bayesian) Bayesian. Let’s explore approaches.motivation REML produce unbiased (least less biased MLE) estimates covariance parameters. REML approach may interpreted likelihood approach based residuals, marginal likelihood approach covariance parameter estimation.Begin linear mixed model (4.5). Assume \\(X\\) full rank \\(p\\) let \\(L = [L_1 \\,\\,L_2]\\) denote block matrix \\(n\\times p\\) \\(n\\times (n-p)\\) blocks properties\n\\[L_1^\\top X = I_p \\quad\\text{}\\quad L_2^\\top X = 0_{n-p},\\]\n\\(L\\) full rank \\(n\\).setup bit abstract, helps find least one concrete example \\(L\\). Start projection matrix \\(P_X := X(X^\\top X)^{-1}X^\\top\\). Since \\(P_X\\) symmetric, idempotent matrix rank \\(p\\) exactly \\(p\\) eigenvalues value 1 \\(n-p\\) 0 eigenvalues, eigenvectors orthonormal. Hence,\n\\[P_X = \\begin{bmatrix}V_1 & V_2\\\\\nV_3 &V_4\\end{bmatrix}\\begin{bmatrix}1_{p\\times p} & 0_{p\\times n-p}\\\\\n0_{n-p \\times p} & 0_{n-p \\times n-p}\\end{bmatrix}\\begin{bmatrix}V_1 & V_2\\\\\nV_3 &V_4\\end{bmatrix}^\\top.\\]\nfollows representation \n\\[P_X = \\begin{bmatrix}V_1 \\\\\nV_3\\end{bmatrix}\\begin{bmatrix}V_1 \\\\\nV_3 \\end{bmatrix}^\\top=:L_1L_1^\\top\\]\nFurthermore, \\(v\\) eigenvector \\(P_X\\) eigenvalue 1, \\(v\\) eigenvector \\(-P_X\\) eigenvalue 0 just definition eigenvalue (\\(Av=\\lambda v\\)). follows \n\\[-P_X = \\begin{bmatrix}V_2 \\\\\nV_4\\end{bmatrix}\\begin{bmatrix}V_2 \\\\\nV_4 \\end{bmatrix}^\\top=:L_2 L_2^\\top.\\]\nconstruction, \\(L_1L_2^\\top = 0\\), \\(L_1^\\top L_1 = I_p\\) \\(L_2^\\top L_2 = I_{n-p}\\). columns \\(L\\) orthogonal, \\(L\\) full rank.follows, note REML estimates invariant choice \\(L\\), long three properties.Now, consider full rank linear transformation \\(Y \\mapsto LY = [Y_1 \\quad Y_2]^\\top\\). properties \\(L\\), \n\\[LY = \\begin{bmatrix} Y_1\\\\Y_2 \\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\beta \\\\ 0\\end{bmatrix}, \\begin{bmatrix} L_1^\\top \\Sigma L_1 &L_1^\\top \\Sigma L_2\\\\ L_2^\\top \\Sigma L_1 & L_2^\\top \\Sigma L_2 \\end{bmatrix}\\right)\\]\n\\(\\Sigma = Cov(Y) = Z \\psi_\\theta Z^\\top + \\Lambda_\\theta\\).next step consider equivalent characterization distribution \\((Y_1, Y_2)\\) product conditional distribution \\(Y_1|Y_2 = y_2\\) marginal distribution \\(Y_2\\). Using general formulas conditional distribution multivariate normal random vector, get\n\\[Y_1|y_2 \\sim N(\\beta - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2, \\, L_1^\\top \\Sigma L_1-L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1} L_2^\\top \\Sigma L_1).\\]\ntedious linear algebra (omitted ) can used show covariance matrix actually equal \\((X^\\top \\Sigma^{-1}X)^{-1}\\), regardless specific choice \\(L\\).Therefore, conditional likelihood given \n\\[\\ell_c := \\text{const.} - \\tfrac12\\log |(X^\\top \\Sigma^{-1}X)^{-1}| -\\tfrac{1}{2}(y_1 - \\beta - y_2^\\star)X^\\top \\Sigma^{-1}X(y_1 - \\beta - y_2^\\star)^\\top, \\]\n\\(y_2^\\star = L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2\\)., tedious linear algebra computations can used show \n\\[L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top = \\Sigma^{-1} - \\Sigma^{-1}X(X^\\top\\Sigma^{-1} X)^{-1} X^\\top\\Sigma^{-1},\\]\nimprovement computationally requires inversion \\(p\\times p\\) matrix \\(\\Sigma^{-1}\\), efficient formula; see . Furthermore, block matrix determinant identity given \n\\[det\\begin{bmatrix}& B\\\\\nB^\\top & C\\end{bmatrix} = |C||- B^\\top C^{-1}B|,\\]\napplied \\(L^\\top\\Sigma L\\) can used show \n\\[\\log |L_2^\\top \\Sigma L_2| = \\log|L^\\top\\Sigma L| + \\log |X^\\top \\Sigma^{-1}X|.\\]\n, rules determinants imply\n\\[\\log|L^\\top\\Sigma L| = \\log|L^\\top L\\Sigma| = \\log|L^\\top L|+\\log|\\Sigma|.\\]\nUsing simplified expressions, noting \\(\\log|L^\\top L|\\) constant parameters, may write tge marginal (residual) likelihood \n\\[\\ell_r:= \\text{const}.-\\tfrac12 \\log |\\Sigma| +\\tfrac12 \\log|X^\\top \\Sigma^{-1} X| -\\tfrac12 y^\\top(\\Sigma^{-1} - \\Sigma^{-1}X(X^\\top\\Sigma^{-1} X)^{-1} X^\\top\\Sigma^{-1})y.\\]reason referring \\(\\ell_r\\) residual loglikelihood? Recall \\(\\ell_r\\) marginal loglikelihood \\(L_2^\\top Y\\) \\(L_2^\\top X = 0\\). Therefore,\n\\[E(L_2^\\top Y) = L_2^\\top X\\beta = (L_2^\\top X)\\beta = 0\\beta = 0.\\]\nsay \\(L_2^\\top Y\\) behaves residual \\(L_2^\\top X = 0\\).REML estimate \\(\\theta\\) given maximizer \\(\\ell_r\\) respect \\(\\theta\\). Note also simple MLE \\(\\theta\\) respect marginal likelihood \\(L_2^\\top Y\\), .e., likelihood ``part data”.Next, consider maximizing conditional likelihood \\(\\ell_c\\) respect \\(\\beta\\) \\(\\theta=\\hat\\theta\\) fixed REML estimate, .e., \\(\\Sigma = \\hat\\Sigma\\). Take first derivative respect \\(\\beta\\), equate zero, observe\n\\[\\begin{align*}\n\\hat\\beta &= y_1 - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2\\\\\n&= L_1^\\top y - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top y\\\\\n&= L_1^\\top \\Sigma \\Sigma^{-1}y - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma\\Sigma^{-1}y\\\\\n& = (L_1^\\top \\Sigma - (L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma)\\Sigma^{-1}y\\\\\n& = (L_1^\\top \\Sigma L_1^\\top X - (L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma L_1^\\top X)\\Sigma^{-1}y\\\\\n& = [(L_1^\\top \\Sigma L_1)-(L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}(L_2^\\top \\Sigma L_1)]X\\Sigma^{-1}y\\\\\n& = (X^\\top \\Sigma^{-1}X)^{-1}X^\\top \\Sigma^{-1}y,\n\\end{align*}\\]\nusing \\(L_1^\\top X = \\) equivalence conditional covariance \\(Y_1|y_2\\) \\((X^\\top \\Sigma^{-1}X)^{-1}\\) shown .conclude REML estimator \\(\\beta\\) weighted least squares estimator weight matrix given REML-based plug-estimator \\(\\hat\\Sigma^{-1}\\).","code":""},{"path":"linear-mixed-models.html","id":"reml---bayesian-approach","chapter":"4 Linear Mixed Models","heading":"4.2.3 REML - Bayesian approach","text":"Suppose model (Bayesian sense) parameter \\(\\beta\\) improper constant prior, random component \\(\\alpha\\) multivariate normal prior covariance \\(\\psi\\). whatever parameters define \\(\\theta\\), endow constant/improper priors well. , combining priors multivariate normal likelihood following posterior:\n\\[\\log\\Pi_n(\\beta) = \\text{const.} - \\tfrac12 \\left(y - X\\beta - Z\\alpha\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta - Z\\alpha\\right) - \\tfrac12\\alpha^\\top \\psi^{-1}\\alpha . \\]\nnice (somewhat hard find) factorization posterior due Searle et al. (Variance Components, Section 9.2). Expand exponent:\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12 \\alpha^\\top (\\psi^{-1} + Z^\\top \\Lambda^{-1}Z)\\alpha + \\alpha^\\top Z^\\top \\Lambda^{-1}(y - X\\beta)\n\\end{align*}\\]\nLet \\(:=\\psi^{-1} + Z^\\top \\Lambda^{-1}Z\\) complete square \\(\\alpha\\):\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))\\\\\n&+\\tfrac12(y-X\\beta)^\\top\\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1} (y-X\\beta).\n\\end{align*}\\]\nCombine terms quadratic \\(y-X\\beta\\). resulting matrix \\(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}\\) equal \\((Z\\psi Z^\\top + \\Lambda)^{-1}\\), fact used frequentist approach REML. Therefore, posterior factorizes \n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta) \\\\\n&- \\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta)) \\end{align*}\\]Integrate \\(\\alpha\\) obtain posterior \\(\\beta\\). Marginally, \\(\\alpha\\) multivariate normal; therefore, obtain\n\\[\\Pi_n(\\beta) = (2\\Pi)^{-n/2}|\\Lambda|^{-1/2}|\\psi|^{-1/2}||^{-1/2}\\exp\\left\\{-\\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta)\\right\\}.\\]can verified \\(|Z\\psi Z^\\top +\\Lambda| = |\\Lambda||\\psi|||\\) using following identity\n\\[|D^{-1} - CA^{-1}B| = |D||^{-1}||- BD^{-1}C|\\]\n\\(Z\\psi Z^\\top +\\Lambda = D^{-1} - CA^{-1}B = (\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})^{-1}\\); , see, e.g., Appendix M equation 31 Searle et al. (Variance Components). result, posterior \\(\\beta\\) multivariate normal covariance \\(Z\\psi Z^\\top +\\Lambda\\).Let \\(V := (Z\\psi Z^\\top +\\Lambda)\\). exponent, add subtract \\(X\\hat\\beta = X(X^\\top V^{-1} X)^{-1}X^\\top V^{-1}y\\) (depends \\(\\theta\\)) quadratic obtain\n\\[\\begin{align*}\n\\left(y - X\\beta\\right)^\\top V^{-1}(y - X\\beta) & = \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(y - X\\hat\\beta)\\\\\n& -2 \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(X\\hat\\beta - X\\beta)\\\\\n& + \\left(X\\hat\\beta - X\\beta\\right)^\\top V^{-1}(X\\hat\\beta - X\\beta).\n\\end{align*}\\]\nstraightforward show middle term zero. Thereforen, joint posterior \\((\\beta, \\theta)\\) may written\n\\[\\log \\Pi_n(\\beta, \\theta) = \\text{const}. -\\tfrac12\\log|V|-\\tfrac12(y-X\\hat\\beta)^\\top V^{-1}(y-X\\hat\\beta) -\\tfrac12(X\\hat\\beta - X\\beta)V^{-1}(X\\hat\\beta - X\\beta)\\]integrate \\(\\beta\\) (w.r.t. multivariate normal density), obtain following marginal posterior variance parameters:\n\\[\\log\\Pi_n(\\theta) = \\text{const.} - \\tfrac{1}{2}\\log|V| + \\tfrac{1}{2}\\log|X^\\top V^{-1}X|-\\tfrac12 \\left(y - X\\hat\\beta\\right)^\\top V^{-1}(y - X\\hat\\beta).\\]","code":""},{"path":"linear-mixed-models.html","id":"modeling-and-estimation-for-loblolly-pines-data","chapter":"4 Linear Mixed Models","heading":"4.3 Modeling and estimation for Loblolly pines data","text":"data set “Loblolly” included nlme package contains longitudinal measurements tree height 14 Loblolly pine trees 6 age points.plot age height trees suggests common concave polynomial behavior—quite linear.model mean height behavior might choose , say, third degree polynomial function age. ways : specify polynomial age, polynomial {age - mean(age)}, basis orthogonal polynomials. second third strategies ","code":"\nlibrary(nlme)\nlibrary(ggplot2)## Warning: package 'ggplot2' was built under R version 4.1.3\nhead(Loblolly)## Grouped Data: height ~ age | Seed\n##    height age Seed\n## 1    4.51   3  301\n## 15  10.89   5  301\n## 29  28.72  10  301\n## 43  41.74  15  301\n## 57  52.70  20  301\n## 71  60.92  25  301\nggplot(data = Loblolly) + \n  geom_line(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = height)) + \n  facet_wrap(~ Seed, nrow = 2)"},{"path":"linear-mixed-models.html","id":"inference-for-fixed-effects","chapter":"4 Linear Mixed Models","heading":"4.4 Inference for fixed effects","text":"common inferential questions concern regression coefficient vector \\(\\beta\\). Wald confidence regions \\(\\beta\\) defined sets\n\\[C_\\alpha := \\{b: (b - \\hat\\beta)(X^\\top V^{-1}X)^{-1}(b - \\hat\\beta)<\\chi^2_{1-\\alpha, p}\\}\\]\n\\(\\chi^2_{1-\\alpha, p}\\) upper \\(\\alpha\\) quantile Chi-squared distribution \\(p\\) degrees freedom. Unfortunately, regions exact \\(\\theta\\) known can severely undercover small sample sizes. Software packages typically perform adjustment, Kenward-Rogers scaling, generalization Satterthwaite’s approximation.point null hypothesis \\(H_0: \\beta = \\beta_{0}\\) Wald test statistic \n\\[W = (\\beta_0 - \\hat\\beta)(X^\\top V^{-1}X)^{-1}(\\beta_0 - \\hat\\beta)\\]\n\\(H_0\\) rejected \\(W > \\chi^2_{1-\\alpha, p}\\). generally, let \\(D\\) denote \\(r\\times p\\) matrix \\(r<p\\) full rank. Reject null hypothesis \\(H_0:D\\beta = b_0\\) \\(W > \\chi^2_{1-\\alpha, r}\\) \n\\[W = (b_0 - D\\hat\\beta)(D X^\\top V^{-1}X D^\\top)^{-1}(b_0 - D\\hat\\beta)\\]","code":""}]
