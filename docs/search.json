[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"poisson-regression.html","id":"poisson-regression","chapter":"3 Poisson Regression","heading":"3 Poisson Regression","text":"","code":""},{"path":"poisson-regression.html","id":"children-ever-born-data","chapter":"3 Poisson Regression","heading":"3.1 Children Ever Born Data","text":"“Children Ever Born” (CEB) dataset consists grouped data number births Fijian women. women described according marriage duration years ordinal levels: (0-4, 5-9, 10-14, 15-19, 20-24, 25-29); place residence (Suva—capital city—Urban, Rural); , level education (none, lower primary, upper primary, secondary greater). count, mean, variance number children ever born, group size, given group women cross-classified factorial level. summaries sufficient model counts children ever born Poisson distribution (individual woman’s count needed).CEB data example observational dataset — characteristics individuals inherent rather set experimenters experimental/controlled trial—, clear context. Several interesting questions may answered using data, : fewer born children associated higher lower education among Fijian women; urban versus rural living location influence number children ever born; , number children ever born steadily increase marrige duration, tend plateau?statistician (student statistician) familiar multiple linear regression /ANOVA factorial experiments may instinctively choose fit Gauss-Markov linear model CEB data, treating responses independent normal random variables. However, since responses counts, Poisson model reasonable. , just one perform Poisson regression? — opposed familiar multiple linear regression described Gauss-Markov model:\n\\[Y = X\\beta+ \\epsilon, \\quad \\epsilon \\sim N_{n}(0_{n\\times 1}, \\sigma^2 I_n).\\]motivation chapter, explore family Generalized Linear Models defining fitting model, performing inference model diagnostics, within context CEB example.","code":"\nceb <- read.table('ceb.dat')\nhead(ceb)##   dur   res  educ mean  var  n     y\n## 1 0-4  Suva  none 0.50 1.14  8  4.00\n## 2 0-4  Suva lower 1.14 0.73 21 23.94\n## 3 0-4  Suva upper 0.90 0.67 42 37.80\n## 4 0-4  Suva  sec+ 0.73 0.48 51 37.23\n## 5 0-4 urban  none 1.17 1.06 12 14.04\n## 6 0-4 urban lower 0.85 1.59 27 22.95"},{"path":"poisson-regression.html","id":"defining-glms","chapter":"3 Poisson Regression","heading":"3.2 Defining GLMs","text":"CEB data naturally want model CEB grouped counts realizations Poisson r.v.’s means \\(n_{j}x^\\top_j\\beta\\) \\(n_j\\) number women \\(j^{\\text{th}}\\) factorial group, \\(x_j\\) vector common covariates, \\(\\beta\\) common regression coefficient vector. , likelihood model \n\\[L(\\beta;\\text{data}) = \\prod_{j=1}^{70}\\frac{(n_{j}x^\\top_j\\beta)^{y_j}e^{-n_{j}x^\\top_j\\beta}}{y_j!}\\]\nloglikelihood given \n\\[\\ell(\\beta;\\text{data}) = \\text{constant} + \\sum_{j=1}^{70}y_j\\log(n_{j}x^\\top_j\\beta) - n_{j}x^\\top_j\\beta.\\]\nPoisson likelihood member Exponential Family, contains distributions PDFs may expressed \n\\[f(y;\\theta,\\phi) = \\exp\\{[y\\theta - b(\\theta)]/(\\phi) + c(y,\\phi)\\}.\\]\nLooking ahead, apply exponential family model independent identically distributed responses, similar data encounter multiple linear regression Gauss-Markov model, allow \\(\\theta\\) well forms \\(\\), \\(b\\), \\(c\\) functions vary observations, fix \\(\\phi\\), loglikelihood sample size \\(n\\) may written follows:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{[y_i\\theta_i - b_i(\\theta_i)]/a_i(\\phi) + c_i(\\phi, y_i)\\}.\\]\nPoisson regression model grouped data fairly simple member family, \\(\\theta = \\log (n_{j}x^\\top_j\\beta)\\), \\(\\phi = (\\phi) = 1\\), \\(b(\\theta) = \\exp(\\theta) =n_{j}x^\\top_j\\beta\\). fact, often case GLMs satisfy \\((\\phi)\\propto \\phi\\) known constant. \ngeneral, GLMs satisfy\n\\[E(Y) = b'(\\theta) \\quad \\text{}\\quad V(Y) = b''(\\theta)(\\phi).\\]\nPoisson regression model, particular, \n\\[\\theta = \\log(n_{j}x^\\top_j\\beta); \\quad b(\\theta) = \\exp(\\theta); \\quad \\text{}\\quad (\\phi) = 1\\]\n\\[E(Y) = b'(\\theta) = \\frac{\\partial}{\\partial \\theta}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta; \\text{ ,}\\]\n\\[V(Y) = b''(\\theta)(\\phi) = \\frac{\\partial^2}{\\partial \\theta^2}\\exp(\\theta) = \\exp(\\theta) = n_{j}x^\\top_j\\beta.\\]","code":""},{"path":"poisson-regression.html","id":"fitting-glms","chapter":"3 Poisson Regression","heading":"3.3 Fitting GLMs","text":"Like model defined likelihood, GLMs may fit maximizing (log)likelihood. , generally case maximizers (MLEs) available closed form. Instead, computed iteratively using Newton’s method similar iterative procedure. Refer exponential family loglikelihood using usual representation \\(a_i(\\phi) = \\phi/w_i\\) \\(w_i\\) known constants:\n\\[\\ell(\\beta;\\text{data}) = \\sum_{=1}^n \\{w_i[y_i\\theta_i - b_i(\\theta_i)]/\\phi + c_i(\\phi, y_i)\\}.\\]\nLet \\(\\mu_i = E(Y_i)\\). , \\(b'(\\theta_i) = \\mu_i\\), , equivalently, \\(g_c(\\mu_i) = \\theta_i\\) \\(g_c\\) termed canonical link; example, \\(g_c := \\log\\) Poisson distribution. Additionally, let \\(g\\) link mean linear function covariates, .e., \\(g(\\mu_i) = \\eta_i = x_i^\\top\\beta\\); e.g., \\(g\\) identity function Poisson model. Since \\(b_i'(\\theta_i)\\) also equal \\(\\mu_i\\) exponential family, may differentiate loglikelihood respect regression parameter \\(\\beta\\) using chain rule:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{=1}^n \\left\\{\\frac{w_i}{\\phi}\\left[y_i\\frac{\\partial \\theta_i}{\\partial\\beta_j} - \\frac{\\partial b_i(\\theta_i)}{\\partial \\beta_j}\\right] + c_i(\\phi, y_i)\\right\\}\\]\nusing\n\\[\\frac{\\partial \\theta_i}{\\partial \\beta_j} = \\frac{\\partial \\theta_i}{\\partial \\mu_i}\\frac{\\partial \\mu_i}{\\partial \\beta_j}.\\]\nSince \\(\\mu_i = b_i'(\\theta_i)\\) \\(\\partial \\theta_i/\\partial \\mu_i = 1/b_i''(\\theta_i)\\). , light \\(\\mu_i = b'(\\theta_i)\\) may always write \\(b_i''(\\theta_i)\\) function \\(\\mu_i\\), .e., \\(V(\\mu_i) = b_i''(\\theta_i)/w\\) \\(V(Y_i) = V(\\mu_i)\\phi\\). Moreover, since \\(\\mu_i = g^{-1}(x_i^\\top \\beta)\\) \\(\\partial\\mu_i/\\partial \\beta_j = x_{ij}/g'[g^{-1}(x_i^\\top \\beta)]\\). Substituting, can write score function using \\(\\mu_i\\) follows:\n\\[\\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{1}{\\phi}\\sum_{=1}^n \\frac{y_i - \\mu_i}{g'(\\mu_i)V(\\mu_i)}x_{ij}.\\]\nsecond (mixed partial) derivative may written\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta_j\\partial\\beta_k} = -\\frac{1}{\\phi}\\sum_{=1}^n \\frac{x_{ij}x_{ik}h(\\mu_i)}{g'(\\mu_i)^2V(\\mu_i)}\\]\n\\(h(\\mu_i) = 1+(y_i-\\mu_i)\\{V'(\\mu_i)/V(\\mu_i) + g''(\\mu_i)/g'(\\mu_i)\\}\\). expectation second derivative (multiplied -1 appears Fisher information matrix) quantity \\(h(\\mu_i)\\) replaced \\(E[h(\\mu_i)]\\), simply equals 1 \\(E(Y_i - \\mu_i) = 0\\).\nHessian loglikelihood clearly quadratic form \\(\\phi^{-1}X^\\top WX\\) \\(X\\) \\(n\\times p\\) design matrix covariates \\(W = [h(\\mu_i)/\\{g'(\\mu_i)^2V(\\mu_i)\\}]\\) \\(n\\times n\\) diagonal matrix “weights”. Less obvious, may define \\(G = \\text{diag}\\{g'(\\mu_i)/h(\\mu_i)\\}\\) gradient loglikelihood equals \\(\\phi^{-1}X^\\top WG(y - \\mu)\\). clever rewriting, Newton’s method updates take form weighted least squares solution:\n\\[\\begin{align*}\n\\beta^{[k+1]} &= \\beta^{[k]} + (X^\\top WX)^{-1}X^\\top WG(y-\\mu)\\\\\n& = (X^\\top WX)^{-1}X^\\top W\\{G(y-\\mu)X+\\beta^{[k]}\\}\\\\\n& = (X^\\top WX)^{-1}X^\\top Wz\n\\end{align*}\\]\n\\(z := G(y-\\mu)+X\\beta^{[k]}\\) sometimes referred “pseudo-data”. Repeating weighted least squares update, iteratively, convergence, termed iteratively re-weighted least squares (IRLS) since, course, weights \\(W\\) updating iteration.Poisson regression based grouped CEB data following likelihood, gradient, Hessian:\n\\[\\begin{align*}\n&\\ell(\\beta;\\text{data}) = \\sum_{j=1}^{70} \\left[y_j x_j^\\top \\beta - n_j e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla_s \\ell = \\sum_{j=1}^{70} \\left[y_j x_{js} - n_j x_{js}e^{x_j^\\top \\beta}\\right]\\\\\n&\\nabla^2_{s,t} \\ell = -\\sum_{j=1}^{70}  n_j x_{js}x_{jt}e^{x_j^\\top \\beta}.\n\\end{align*}\\]Rewriting Hessian gradient general exponential family GLM \n\\[W_{k,k} = n_k\\mu_k\\quad\\text{}\\quad G_k = (n_k\\mu_k)^{-1}\\]\nIRLS updates given \n\\[(X^\\top WX)^{-1}X^\\top Wz\\]\n\\(z_k = (n_k\\mu_k)^{-1}(y_k - n_k\\mu_k) + x_k^\\top \\beta\\).","code":""},{"path":"poisson-regression.html","id":"irls-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.3.1 IRLS for the CEB data","text":"compute MLEs Poisson regression grouped CEB data “hand” using IRLS—, also compare glm function R. calculation initialize elements parameter vector \\(\\mu\\) sample means \\(\\mu_j = y_j/n_j\\). set pseudo data equal \\(z_j = -(1/\\mu_j)(y_j / n_j - \\mu_j) + log(\\mu_j)\\) iterate computation least squares estimates \\(\\hat\\beta\\). Note CEB data contains grouped “counts” computed \\(y_j = \\mu_jn_j\\) \\(\\mu_j\\) values rounded. , result, \\(y_j\\) counts integers. affect “hand” calculation \\(\\hat\\beta\\) whatsoever never use full Poisson PMF computations; glm function R, hand, throw many warnings \\(y_j\\) values rounded, apparently uses PMF via dpois “hood”. differences fitted \\(\\hat\\beta\\) glm’s due rounding \\(y_j\\)’s.","code":"\nn <- nrow(ceb)\ngroup.sizes <- ceb$n\nY <- ceb$y\n# IRLS - factor coding\n# initialize with mu = Y/group.sizes\noptions(contrasts = c('contr.treatment', 'contr.treatment'))\nX <- model.matrix(y~dur+res+educ, data = ceb)\nmu <- Y/group.sizes\nXB <- log(mu)\nW <- diag(as.numeric(mu))\nz <- -(1/mu)*(Y/group.sizes-mu) + XB\nbeta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\ntol <- 0.0001\ndifference <- 1\nmaxiter <- 100\niter <- 1\nwhile((difference > tol) & (iter < maxiter)){\n  XB <- X%*%beta\n  mu <- exp(XB)\n  W <- diag(as.numeric(group.sizes*mu))\n  z <- (Y/diag(W) - rep(1,n)) + XB\n  beta.old <- beta\n  beta <- solve(t(X)%*%W%*%X)%*%t(X)%*%W%*%z\n  difference <- max(abs(beta - beta.old))\n  iter<-iter+1\n}\nbeta##                    [,1]\n## (Intercept)  0.05695417\n## dur10-14     1.37053208\n## dur15-19     1.61423104\n## dur20-24     1.78548879\n## dur25-29     1.97679469\n## dur5-9       0.99765038\n## resSuva     -0.15121728\n## resurban    -0.03895822\n## educnone    -0.02308034\n## educsec+    -0.33266474\n## educupper   -0.12474575\n## the glm function can be used with offset equal to logarithm of the group sizes\nmy.glm <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb, offset = log(n))\nsummary(my.glm)## \n## Call:\n## glm(formula = round(y) ~ dur + res + educ, family = poisson(link = \"log\"), \n##     data = ceb, offset = log(n))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2960  -0.6641   0.0725   0.6336   3.6782  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.05754    0.04803   1.198    0.231    \n## dur10-14     1.36940    0.05107  26.815  < 2e-16 ***\n## dur15-19     1.61376    0.05119  31.522  < 2e-16 ***\n## dur20-24     1.78491    0.05121  34.852  < 2e-16 ***\n## dur25-29     1.97641    0.05003  39.501  < 2e-16 ***\n## dur5-9       0.99693    0.05274  18.902  < 2e-16 ***\n## resSuva     -0.15166    0.02833  -5.353 8.63e-08 ***\n## resurban    -0.03924    0.02463  -1.594    0.111    \n## educnone    -0.02297    0.02266  -1.014    0.311    \n## educsec+    -0.33312    0.05390  -6.180 6.41e-10 ***\n## educupper   -0.12425    0.03000  -4.142 3.44e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 3731.852  on 69  degrees of freedom\n## Residual deviance:   70.665  on 59  degrees of freedom\n## AIC: 522.14\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"poisson-regression.html","id":"inference-on-glms","chapter":"3 Poisson Regression","heading":"3.4 Inference on GLMs","text":"doubt noticed glm function output produces standard errors, “z” values, p-values fitted coefficient, just find accompanying summarized lm output. , justification p-values?Since \\(\\hat\\beta\\) MLE, standard likelihood theory holds \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, ^{-1}(\\beta))\\) “large” \\(n\\) \\(^{-1}(\\beta)\\) Fisher information. discussed , “observed information” (Hessian) equal \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) \\(W\\) weight matrix final iteration IRLS, \\(-\\phi^{-1}(X^\\top WX)^{-1}\\) coincides Fisher Information replace \\(h(\\mu_i)\\) \\(E(h(\\mu_i))=1\\) IRLS (called Fisher scoring) updates. Therefore, \\(\\hat\\beta \\stackrel{\\cdot}{\\sim}N_p(\\beta, \\phi^{-1}(X^\\top W X)^{-1})\\) “large” \\(n\\), useable \\(\\phi\\) known (, equals 1, Poisson Binomial models). Otherwise, replace \\(\\phi\\) MLE use corresponding Student’s \\(t\\) distribution \\(n - p\\) degrees freedom rather standard normal inference \\(\\beta_j\\). upshot may base tests , e.g., \\(H_0:\\beta_j = 0\\), Student’s \\(t\\) \\(n-p\\) df; .e.,\n\\[\\text{Reject }H_0:\\beta_j = 0 \\text{ }\\left|\\frac{\\hat\\beta_j}{\\sqrt{\\hat\\phi^{-1}(X^\\top W X)^{-1}_{j,j}}}\\right| > t_{1-\\alpha/2,n-p}.\\]\nMultivariate Wald simultaneous \\(100(1-\\alpha)\\%\\) confidence regions given eliiptical contours:\n\\[\\phi\\text{ known: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< \\chi^2_{1-\\alpha,p} \\}\\]\n\\[\\phi\\text{ unknown: }\\quad \\{\\beta: (\\hat\\beta - \\beta)^\\top\\hat\\phi^{-1}(X^\\top W X)^{-1}(\\hat\\beta - \\beta)< F_{1-\\alpha,p, n-p} \\}\\]Moreover, approximate \\(95\\%\\) CI mean response \\(\\mu = g^{-1}(x^\\top \\beta)\\) covariate vector \\(x\\) given Delta method interval:\\[g^{-1}(x^\\top \\hat\\beta)\\pm t_{1-\\alpha/2,n-p}\\sqrt{\\hat\\phi^{-1} \\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]^\\top(X^\\top W X)^{-1}\\left[\\nabla_{\\beta} g^{-1}(x^\\top \\beta)\\right]}.\\]\nmultiple linear regression (Gauss-Markov) models use partial F tests (likelihood ratio tests) test significance sets covariates, .e., \\(H_0: \\beta_j = \\beta_{j+1} = \\cdots = \\beta_{j+\\ell} = 0\\). GLMs, similar tests available. models \\(\\phi\\) known, \n\\[-2\\{\\ell(\\hat\\beta_{h_0}) - \\ell(\\hat\\beta)\\}\\stackrel{H_0}{\\sim} F_{\\ell,n-p}\\]\n\\(\\hat\\beta_{h_0}\\) MLE null hypothesis \\(\\ell\\) coefficients set equal 0.several methods estimate \\(\\phi\\) unknown. Pearson’s method observes \n\\[\\phi^{-1}X^2 \\stackrel{\\cdot}{\\sim}\\chi^2_{n-p}\\quad \\text{}\\quad X^2 :=\\sum_{=1}^n \\frac{(Y_i - \\hat\\mu_i)^2}{\\phi V(\\hat\\mu_i)}\\]\nmodel fits data adequately. Hence, \\(\\hat\\phi_P = X^2/(n-p)\\) good estimate \\(\\phi\\). certain data sets, Poisson data low counts, Pearson estimate may behave badly, modified version (Fletcher’s estimator) preferred:\n\\[\\hat\\phi_F = \\frac{\\hat\\phi_P}{1-\\overline s}, \\quad\\text{}\\]\n\\[\\overline s:=n^{-1}\\sum_{=1}^n V'(\\hat\\mu_i)\\frac{(y_i - \\hat\\mu_i)}{V(\\hat\\mu_i)}.\\]Deviance difference models B \\(\\subset B\\) given \\(D_A - D_B = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\phi\\). scaled deviance difference \n\\[D_A^* - D_B^* = -2\\{\\ell(\\hat\\beta_A) - \\ell(\\hat\\beta_B)\\}\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell}\\]\ndifference number fitted parameters \\(\\ell\\). Despite notation, scaled deviance difference depend \\(\\phi\\), whereas deviance difference . Two alternative tests make use scaled deviance compare nested GLMs. first analogous partial F test:\n\\[F = \\frac{(D_A^* - D_B^*)/\\ell}{D_B^* / (n-p)}\\stackrel{\\cdot}{\\sim} F_{\\ell, n-p}\\]\napproximation F distribution rough. Alternatively, can replace scale parameter scaled deviance Pearson (Fletcher) estimator obtain\n\\[\\hat D_A^* - \\hat D_B^*\\stackrel{\\cdot}{\\sim}\\chi^2_{\\ell},\\]\n“hat” scaled deviances indicates dependence \\(\\hat\\phi\\).","code":""},{"path":"poisson-regression.html","id":"inference-and-prediction-for-the-ceb-data-using-poisson-regression","chapter":"3 Poisson Regression","heading":"3.4.1 Inference and prediction for the CEB data using Poisson regression","text":"Next, ’ll demonstrate computations confidence intervals tests significance sets covariates within Poisson regression model CEB data. can either computations “hand” using results IRLS can use built-R functions like glm confint. Wald-type confidence intervals regression coefficients require inverse Hessian (estimate inverse Fisher information, equal Fisher scoring used). compute final iteration IRLS (assuming algorithm converged). GLMs need estimate scale parameter \\(\\phi\\) include Pearson Fletcher estimates —Poisson model \\(\\phi=1\\). Note estimates close 1. p-values included summarized glm output imply intercept significantly different zero, several coefficients different zero, including, e.g., \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\). Wald-type CIs p-values agree glm p-values; example, 95% CI \\(\\beta_0\\) computed “hand” (-0.0372 0.1511), suggesting intercept significantly different zero, 95% CI \\(\\beta_1\\) (1.2704, 1.4706) p-value test \\(\\beta_1 = 0\\) indistinguishable 0. computed deviance difference (minus twice difference loglikelihood) intercept-model full model. deviance difference 3661 ten degrees freedom (difference number fitted coefficients models). intercept model fits, deviance difference comparable Chi-squared r.v. 10 degrees freedom, corresponding p-value basically zero, supporting claim full model fits much better intercept-model. Compare deviance difference calculation output glm: glm includes null deviance residual deviance, difference two gives deviance difference statistic used compare intercept-full models. 3661, agreeing almost exactly “hand” calculation.","code":"\nHessian <- t(X)%*%W%*%X\ninv.Hessian <- solve(Hessian)\np <- length(beta)\n\n# Just for illustration, phi = 1 for Poisson model\nPearson.X2 <- sum(((Y - group.sizes * mu)^2) / (group.sizes * mu))\nPearson.phi <- Pearson.X2 / (n-p)\ns.bar <- mean((Y - group.sizes * mu) / (group.sizes * mu))\nFletcher.phi <- Pearson.phi/(1-s.bar)\nPearson.phi ## [1] 1.211949\nFletcher.phi ## [1] 1.194283\n# CIs for the first 4 regression coefficients\n#   If phi were unknown, it's estimate would appear in the estimated standard error of the \n#   estimated coefficient\n# beta[1] + qt(c(0.025,0.975),n-p)*sqrt((1/Pearson.phi)*inv.Hessian[1,1])\nbeta[1] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[1,1])## [1] -0.03721264  0.15112097\nbeta[2] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[2,2])## [1] 1.270425 1.470639\n2*(1-pnorm(abs(beta[2]/sqrt(inv.Hessian[2,2]))))## [1] 0\nbeta[3] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[3,3])## [1] 1.513870 1.714592\nbeta[4] + qnorm(c(0.025,0.975))*sqrt(inv.Hessian[4,4])## [1] 1.685091 1.885886\n# The R function confint can also be used with GLMs to provide confidence intervals for coefficients\n\nconfint(my.glm, 'dur10-14')## Waiting for profiling to be done...##    2.5 %   97.5 % \n## 1.270141 1.470370\n# \"Model F Test\" - testing that all (non-intercept) coefficients equal zero\n# for Poisson model, since phi is known, we have a LRT equivalent to a partial F test\n# based on deviance difference\nYbar <- sum(Y)/sum(group.sizes)\nD <- -2*(sum(Y*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Y*log(group.sizes*mu))+sum(group.sizes*mu))\nYbar## [1] 3.960497\nD## [1] 3660.872\n1-pchisq(D,p-1)## [1] 0\n# with rounded Ys and using glm output\nYr <- round(Y)\nmu.glm <- exp(X%*%matrix(my.glm$coefficients,p,1))\nYbar <- sum(Yr)/sum(group.sizes)\nD <- -2*(sum(Yr*log(group.sizes*Ybar))-sum(group.sizes*Ybar) - sum(Yr*log(group.sizes*mu.glm))+sum(group.sizes*mu.glm))\nYbar## [1] 3.960403\nD## [1] 3661.186\n1-pchisq(D,p-1)## [1] 0"},{"path":"poisson-regression.html","id":"model-checkingdiagnostics","chapter":"3 Poisson Regression","heading":"3.5 Model Checking/Diagnostics","text":"essential statistical practice check whether model adequately fits data. model fits poorly, inferences/predictions garnered model suspect. multiple linear regression assess model fit analyzing residuals. multiple linear regression model fits, residuals approximately standard normal. Lack fit manifests residuals skewed heavy-tailed, contain outliers, tend increase variability one covariates /predicted responses. Model-checking GLMs can done essentially manner—key find quantity reasonably fills role residuals multiple linear regression. GLMs, two choices, Pearson residuals deviance residuals. Pearson residuals defined \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{V(\\mu_i)}}\\), , sometimes, \\(e^P_i = \\frac{Y_i - \\hat\\mu_i}{\\sqrt{\\hat\\phi V(\\mu_i)}}\\). first definition results quantities approximately zero-mean normal random variates variance \\(\\phi\\) whereas second definition provides standard normal quantities. practitioners prefer deviance residuals Pearson residuals latter often observed asymmetric , hence, “normal” expected. deviance equal \n\\[\\text{Deviance} = -2\\phi\\{\\ell(\\hat\\beta) - \\sup \\ell\\}\\]\n\\(\\ell(\\hat\\beta)\\) loglikelihood evaluated MLEs \\(\\sup \\ell\\) loglikelihood \\(\\mu_i = y_i\\), .e., totally saturated model. Multiplying \\(\\phi\\) removes dependence loglikelihood scale parameter. deviance can written sum terms, say, \\(\\text{Deviance} = \\sum_{=1}^n d_i\\), observation’s contribution \\(d_i\\) deviance used define deviance residuals follows:\n\\[e^D_i = \\text{sign}(y_i - \\mu_i)\\sqrt{d_i}.\\]","code":""},{"path":"poisson-regression.html","id":"residual-analysis-for-ceb-data","chapter":"3 Poisson Regression","heading":"3.5.1 Residual analysis for CEB data","text":"Using either deviance residuals Pearson residuals shows important things. First, residuals approximately standard normal, exception one “outlier”, observation 17. Second, sort observations fitted mean response \\(\\hat\\mu_i\\) least greatest, see absolutely trend , pattern , residuals. implies correctly modeled mean-variance relationship, also correctly modeled mean linear function covariates. obtain similar plots simply running “plot(glm object)”, default differences. Plotting glm object provide plots “residuals” versus “fitted values”, fact, labels plots slightly misleading. residuals vs. fitted values plots uses either Pearson deviance residuals (tell sure plot documentation) versus logarithm fitted responses \\(\\log(\\hat y_j) =\\log(n_j\\hat\\mu_j)\\). ","code":"\ndev <- -2*((Y*log(group.sizes*mu))-(group.sizes*mu) - ((Y*log(Y))-(Y)))\ndeviance.resids <- ifelse((Y-group.sizes*mu) < 0,-1,1)*sqrt(dev)\npearson.resids <- (Y - group.sizes*mu)/sqrt(group.sizes*mu)\n\n# residual plots using deviance residuals\nqqnorm(deviance.resids)\nqqline(deviance.resids)\nshapiro.test(deviance.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  deviance.resids\n## W = 0.96065, p-value = 0.02719\nhist(deviance.resids, freq = FALSE)\ndev.norm <- function(x) dnorm(x,mean(deviance.resids), sd(deviance.resids))\ncurve(dev.norm, -5,5, add = TRUE)\nplot(mu, deviance.resids)\n# residual plots using pearson residuals\nqqnorm(pearson.resids)\nqqline(pearson.resids)\nshapiro.test(pearson.resids)## \n##  Shapiro-Wilk normality test\n## \n## data:  pearson.resids\n## W = 0.94988, p-value = 0.007147\nhist(pearson.resids, freq = FALSE)\npearson.norm <- function(x) dnorm(x,mean(pearson.resids), sd(pearson.resids))\ncurve(pearson.norm, -5,5, add = TRUE)\nplot(mu, pearson.resids)\n# plots are using what look like Pearson (or maybe deviance) residuals versus predicted values under canonical link, i.e. log(Y hat)\nplot(log(group.sizes*mu), pearson.resids)\nplot(my.glm)"},{"path":"poisson-regression.html","id":"outlier-analysis-using-cooks-distance","chapter":"3 Poisson Regression","heading":"3.6 Outlier analysis using Cook’s distance","text":"Outliers observations corresponding large residuals. may occur due chance, , likely, indicate lack model fit specific observation. lack fit may due something innocuous like mistake made recording data; , may observation question different others sample, follow response-covariate relationship.Outliers problem, however, unless inclusion causes fitted model substantially different excluded. Therefore, necessarily care particular residual large, influential model fit. multiple linear regression may measure influence Cook’s distance data point, related magnitude residual leverage, measured hat (influence) matrix. GLMs can define Cook’s distance manner: Cook’s distance data point \\(k\\) given \n\\[C_k = \\frac{1}{(p+1)}\\sum_{=1}^n \\frac{(\\hat\\mu_i^{[k]} - \\hat\\mu_i)^2}{\\hat\\phi V(\\hat\\mu_i)}; \\text{ }\\]\n\\[C_k = \\frac{(e^P_k)^2}{\\hat\\phi (p+1)}\\frac{h_k}{(1-h_k)^2}\\]\n\\(H = W^{1/2}X(X^\\top W X)^{-1}X^\\top W^{1/2}\\) hat matrix \\(h_k\\) \\(k^{th}\\) diagonal entry. Large Cook’s distance implies model predictions change substantially data point question removed.","code":""},{"path":"poisson-regression.html","id":"outlier-analysis-for-the-ceb-data","chapter":"3 Poisson Regression","heading":"3.6.1 Outlier analysis for the CEB data","text":"Typically data points Cook’s distance \\(>1\\) considered highly influential case exclusion considerable. case, apparent outlier influential, even influential observation sampled. computed Cook’s distance “hand” using corresponding built-R function; (slight) difference two seems caused fact built-function uses glm object, fitted rounding responses, whereas “hand” calculation uses Pearson residuals based original (unrounded) responses.","code":"\nceb2 <- ceb[-17,]\nmy.glm2 <- glm(round(y)~dur+res+educ, family = poisson(link = 'log'), data = ceb2, offset = log(n))\n\nW2 <- sqrt(W)\nh <- W2%*%X%*%solve(t(X)%*%W%*%X)%*%t(X)%*%W2\n\n((pearson.resids[17]^2)/p)*(h[17,17]/((1-h[17,17])^2))## [1] 0.0912897\ncd <- cooks.distance(my.glm)\n\ncd[17]##         17 \n## 0.09123744\nsort(cd)##           44           12           16           43           58           26 \n## 0.0001578983 0.0002173653 0.0003181272 0.0004391595 0.0004872775 0.0008276467 \n##           31           48           69            5           40           37 \n## 0.0009604834 0.0010416260 0.0011600877 0.0011813567 0.0015364693 0.0017374273 \n##           63           15           59           64            8           18 \n## 0.0017913034 0.0020157684 0.0022231406 0.0023307859 0.0024333875 0.0027257717 \n##            1           29           54           56           34           62 \n## 0.0027795217 0.0028483882 0.0029171612 0.0029318194 0.0030296972 0.0030322783 \n##           55           36           27           33            9           60 \n## 0.0033869538 0.0034063422 0.0048780994 0.0049440311 0.0052402581 0.0053485413 \n##           47            6           24            3           11           23 \n## 0.0055965679 0.0056512722 0.0056712755 0.0057553959 0.0059235574 0.0061112712 \n##           50           41           14           49            4            2 \n## 0.0062842199 0.0063288779 0.0064608723 0.0067079644 0.0067366794 0.0068678049 \n##           66           25           13            7           61           71 \n## 0.0072889100 0.0074308461 0.0082351514 0.0104259702 0.0110105154 0.0114006236 \n##           70           42           52           20           28           32 \n## 0.0115384460 0.0145603375 0.0149361729 0.0164709118 0.0173145140 0.0189913643 \n##           51           38           53           19           39           67 \n## 0.0191422888 0.0203317679 0.0205784895 0.0220776790 0.0225077753 0.0265192825 \n##           35           10           45           22           46           17 \n## 0.0290870153 0.0375227510 0.0462157923 0.0771290592 0.0847041708 0.0912374357 \n##           21           65           30           57 \n## 0.1058278940 0.1102296604 0.1184297114 0.2448544534"},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"4 Linear Mixed Models","heading":"4 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"anova-with-random-factors","chapter":"4 Linear Mixed Models","heading":"4.1 ANOVA with random factors","text":"simplest linear mixed models used analyze linear models one-way ANOVA, randomized complete block designs, two-way ANOVA. Mixed models opposed fixed models (linear models heretofore studied) needed factors levels random. Random levels occur whenever units making levels behave like random samples population. Two examples given . , discuss perform ANOVA-like tests factors random rather fixed. upshot (least balanced experiments/datasets) tests fixed effects identical random effects, interpretation different (importantly !).","code":""},{"path":"linear-mixed-models.html","id":"strength-of-metallic-bonds","chapter":"4 Linear Mixed Models","heading":"4.1.1 Strength of metallic bonds","text":"dataset , called “Bonds”, contains responses 21 samples metals, 7 iron, nickel, copper, quantify strength metallic bonds. One sample metal extracted 7 ingots. expect ingots act like blocks—differences ingots account substantial amount variability responses, precise block effects inferential/scientific inference. include blocks order reduce residual variance accounting block variance. randomized controlled block design describes data collected, , repeated experiment, blocks (ingots) completely different. , blocks fixed random. Rather estimating block effects surely change experiment experiment, focus estimating amount variability explained blocks, remain experiment experiment. suggests different model used analyze RCBD experiments fixed blocks.usual linear model fixed blocks \n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\\]\n\\(y_{ij}\\) response treatment (metal) \\(\\) block (ingot) \\(j\\); \\(\\alpha_i\\)’s metal (treatment) effects; \\(\\beta_j\\)’s ingot (block) effects; , \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\) random residuals.linear model wrong model data block effects (, hence, also interaction effects) meaningless outside given data set; population-level parameters blocks random rather fixed. appropriate model (given normality independence random residuals reasonable) following mixed effects model:\\[\\begin{equation}\ny_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.1}\n\\end{equation}\\]\n\\(\\beta_j\\stackrel{iid}{\\sim}N(0, \\sigma_b^2)\\) , independently, \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\). balanced experiments (number replicates equal across combination factor levels) can test block treatment effects comparing nested/aggregated models. Let \\(\\overline Y_{\\cdot}\\) denote mean response metal \\(\\) averaged ingots. can write following aggregated model (4.1) \n\\[\\begin{equation}\n\\overline y_{\\cdot} = \\mu + \\alpha_i + \\epsilon_{},\n\\tag{4.2}\n\\end{equation}\\]\n\\(\\epsilon_i = \\frac{1}{J}\\sum_{j=1}^j \\epsilon_{ij}\\). , \\(\\epsilon_j\\) variance \\(\\sigma_b^2 + \\sigma^2/J\\). F statistic\n\\[F = \\frac{J\\cdot MSE_{agg}}{MSE_{full}}\\]\n\\(MSE_{agg}\\) \\(MSE_{full}\\) mean squared errors models (4.2) (4.1) can used test hypothesis \\(H_0:\\sigma_b^2 = 0\\).","code":""},{"path":"linear-mixed-models.html","id":"machine-productivity","chapter":"4 Linear Mixed Models","heading":"4.1.2 Machine productivity","text":"dataset given contains results designed experiment evaluate worker productivity using 3 different industrial machines. goal determine machine productive controlling natural variation worker productivity. observed workers represent random sample population workers (blocks), analogous ingots previous example. difference two examples (besides context) machine treatments replicated wihtin workers, three observations productivity score fore worker type machine. means can fit model interaction terms capable capturing changes machine effects productivity different workers (changes present):\n\\[\\begin{equation}\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\tag{4.3}\n\\end{equation}\\]\n\\(k\\) denotes \\(k^{\\text{th}}\\) replicate within machine \\(\\) worker \\(j\\); \\((\\alpha\\beta)_{ij}\\) denote machine-worker interaction effects. Let \\(\\overline Y_{ij\\cdot}\\) mean response averaging replicates treatment \\(\\) block \\(j\\) combination. ,\n\\[\\begin{align*}\nV(\\overline Y_{ij\\cdot}) &= V\\left(K^{-1}\\sum_{k=1}^K Y_{ijk}\\right) \\\\\n&= \\frac{1}{K^2}V\\left(\\sum_{k=1}^K \\{\\mu+\\alpha_i+\\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\}\\right)\\\\\n& = \\frac{1}{K^2}V\\left(K\\mu + K\\alpha_i + K\\beta_j + K(\\alpha\\beta)_{ij} + \\sum_{k=1}^K \\epsilon_{ijk}\\right)\\\\\n& = \\sigma_b^2 + \\sigma_{ab}^2 + K^{-1}\\sigma^2.\n\\end{align*}\\]\nrewrite model cell mean responses \n\\[\\begin{equation}\n\\overline y_{ij\\cdot} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij},\n\\tag{4.4}\n\\end{equation}\\]\naggregated error term follows \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N(0, \\sigma_{ab}^2 + \\sigma^2/K)\\). residual mean square (called mean squared error) model (4.3) (let’s call \\(MSE_{\\text{full}}\\)) mean \\(\\sigma^2\\) \\(n-p_1\\) degrees freedom \\(n\\) sample size \\(p\\) number coefficients fitted model (\\(p_1\\) equals number crossed factor levels, number blocks times number treatments). residual mean square aggregated model (4.4) (let’s call \\(MSE_{\\text{agg}}\\)) mean \\(\\sigma_{ab}^2 + \\sigma^2/K\\) \\(n/K-p_2\\) degrees freedom \\(p_2\\) number treatments plus number blocks minus 1. unbiased estimate \\(\\sigma_{ab}^2\\) given \\(MSE_{\\text{agg}} - \\frac{1}{K}MSE_{\\text{full}}\\). Consider testing null hypothesis \\(H_0:\\sigma_{ab}^2 = 0\\). statistic\n\\[F := \\frac{K\\cdot MSE_{agg}}{MSE_{full}}\\stackrel{H_0}{\\sim}F_{n/K-p_2, n-p_1},\\]\n, null hypothesis. test rejects \\(H_0\\) \\(F > F_{1-\\alpha,n/K-p_2, n-p_1}\\) exactly equivalent partial F test full model full model without interaction terms (additive model). use R compute ANOVA tables full model, full model without interaction, aggregated model. F test statistic aggregated model 46.13 10 36 degrees freedom, exactly matches partial F test full additive models.","code":"\nlibrary(nlme)\n\n# aggregated model\nMach.agg <- aggregate(score~Machine*Worker, data = Machines, FUN=mean)\n\nm2 <- lm(score~Machine+Worker, data = Mach.agg)\n\nanova(m2)## Analysis of Variance Table\n## \n## Response: score\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 585.09 292.544 20.5761 0.0002855 ***\n## Worker     5 413.97  82.793  5.8232 0.0089495 ** \n## Residuals 10 142.18  14.218                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# full model with interaction\nm0 <- lm(score~Machine*Worker, data = Machines)\n\nanova(m0)## Analysis of Variance Table\n## \n## Response: score\n##                Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine         2 1755.26  877.63  949.17 < 2.2e-16 ***\n## Worker          5 1241.89  248.38  268.63 < 2.2e-16 ***\n## Machine:Worker 10  426.53   42.65   46.13 < 2.2e-16 ***\n## Residuals      36   33.29    0.92                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(142.18*3/10)/(33.29/36)## [1] 46.12628\n1-pf((142.18*3/10)/(33.29/36), 10, 36)## [1] 0\n# additive model (no interaction)\nm1 <- lm(score~Machine+Worker, data = Machines)\n\nanova(m1)## Analysis of Variance Table\n## \n## Response: score\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## Machine    2 1755.26  877.63  87.798 < 2.2e-16 ***\n## Worker     5 1241.89  248.38  24.848 4.867e-12 ***\n## Residuals 46  459.82   10.00                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(m1,m0)## Analysis of Variance Table\n## \n## Model 1: score ~ Machine + Worker\n## Model 2: score ~ Machine * Worker\n##   Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n## 1     46 459.82                                 \n## 2     36  33.29 10    426.53 46.13 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"a-general-linear-mixed-model","chapter":"4 Linear Mixed Models","heading":"4.2 A general linear mixed model","text":"experiments comparing responses factors ANOVA-type analyses sufficient. , general models random effects, e.g., including continuous covariates, general-purpose methods needed. general mixed effects model may written\n\\[Y = X\\beta+ Z\\alpha + \\epsilon\\]\nY \\(n\\times 1\\) response, \\(X\\) \\(n \\times p\\) design matrix fixed (non-random) effects; \\(Z\\) \\(n\\times \\) matrix random effects; \\(\\beta\\) \\(p\\times 1\\) non-random coefficient vector; \\(\\alpha\\sim N_a(0, \\psi_\\theta)\\) \\(\\times 1\\) multivariate normal random coefficient vector mean 0 covariance matrix \\(\\psi_\\theta\\) indexed parameter \\(\\theta\\); \\(\\epsilon\\sim N_n(0, \\Lambda_\\theta)\\) multivariate normal random residual vector covariance matrix \\(\\Lambda_\\theta\\). alternative way writing model (quite succintly) \n\\[\\begin{equation}\n\\tag{4.5}\nY\\sim N_n(X\\beta, Z \\psi_\\theta Z^\\top + \\Lambda_\\theta).\n\\end{equation}\\]\nabbreviation denote \\(\\Sigma = Z \\psi_\\theta Z^\\top + \\Lambda_\\theta\\) often drop \\(\\theta\\) \\(\\psi\\) \\(\\Lambda\\) save pixels.\n### Parameter estimation using PMLThe linear mixed model (4.5) may fit using maximum likelihood estimation (MLE), (potentially) complicated covariance structure poses challenges numerical maximization likelihood function. Rather straightforward MLE, linear mixed models usually fit using either restricted maximum likelihood estimation (REML, also called residual MLE) profile maximum likelihood estimation (PML). approaches aim simplify computation maximum retaining good asymptotic properties maximum likelihood estimators. MLE frequentist concept, turns REML can derived intuitively using Bayesian approach.previously covered general linear models, .e., \\(Y = X\\beta + \\epsilon\\) \\(Cov(\\epsilon) = W\\) known \\(W\\) equals \\(\\sigma^2 W\\) unknown scalar \\(\\sigma^2\\) known matrix \\(W\\), familiar weighted least squares (WLS) estimation. Often weighted least squares employed response observed heteroskedasticity residuals related covariates \\(W\\) depends covariate values. linear mixed model covariance parameter \\(\\theta\\) known, model fit using WLS:\n\\[\\hat\\beta(\\theta) = (X^\\top \\Sigma^{-1}X)^{-1}X^\\top \\Sigma^{-1}Y\\]\njust applications weighted least squares.WLS solution inspires PML technique: maximize likelihood respect \\(\\theta\\) plugging \\(\\beta = \\hat\\beta(\\theta)\\). PML reproduces MLEs exactly; advantage simplified formulation likelihood maximization problem.Nevertheless two problems PML strategy suggested : one computational statistical.first problem PML/WLS requires inverting \\(n\\times n\\) matrix \\(\\Sigma\\). even moderate sample sizes matrix inversion can computationally demanding computationally unstable. Fortunately, clever linear algebra resolves problem. Define matrix \\(:=\\psi^{-1} + Z^\\top \\Lambda^{-1}Z\\). Observe matrix \\(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}\\) equals inverse \\(Z\\psi Z^\\top + \\Lambda\\):\n\\[\\begin{align*}\n&(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})(Z\\psi Z^\\top + \\Lambda) \\\\\n& = \\Lambda^{-1}Z\\psi Z^\\top + - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}Z\\psi Z^\\top - \\Lambda^{-1}ZA^{-1}Z^\\top \\\\\n& = + \\Lambda^{-1}Z(\\psi - ^{-1}(Z^\\top \\Lambda^{-1}Z\\psi + ))Z^\\top \\\\\n& = +\\Lambda^{-1}Z(\\psi - \\psi(Z^\\top \\Lambda^{-1}\\psi + )^{-1}(Z^\\top \\Lambda^{-1}\\psi + ))Z^\\top \\\\\n& = \n\\end{align*}\\]\nkey property inverse requires computing inverse \\(p\\times p\\) \\(\\times \\) matrices, require computation \\(n\\times n\\) matrix inverse. many practical applications \\(p\\) \\(\\) much smaller \\(n\\), matrices \\(\\Lambda\\) \\(\\psi\\) often simple/sparse structures many exact zeroes, matrix inversions may performed quickly reliably.second issue PML easily overcome. Think back Gauss-Markov model \\(Y = X\\beta+\\epsilon\\) \\(Cov(\\epsilon) = \\sigma^2 I_n\\). Recall MLE \\(\\hat\\sigma^2\\) biased, .e., \\(E(\\hat\\sigma^2) = \\frac{n-p}{n}\\sigma^2\\). practice may substantial number covariates relative sample size bias can quite significant; , importantly, bias results underestimate causes -optimism respect inferences concerning \\(\\beta\\). phenomenon occurs ML/PML estimation: PML estimate \\(\\theta\\) biased. desire estimation strategy constructively produces less-biased estimates covariance parameters motivation REML, covered next.","code":""},{"path":"linear-mixed-models.html","id":"reml---frequentist-approach","chapter":"4 Linear Mixed Models","heading":"4.2.1 REML - frequentist approach","text":"two formulations leading residual (also called restricted/reduced) maximum likelihood approach (REML), one frequentist (non-Bayesian) Bayesian. Let’s explore approaches.motivation REML produce unbiased (least less biased MLE) estimates covariance parameters. REML approach may interpreted likelihood approach based residuals, marginal likelihood approach covariance parameter estimation.Begin linear mixed model (4.5). Assume \\(X\\) full rank \\(p\\) let \\(L = [L_1 \\,\\,L_2]\\) denote block matrix \\(n\\times p\\) \\(n\\times (n-p)\\) blocks properties\n\\[L_1^\\top X = I_p \\quad\\text{}\\quad L_2^\\top X = 0_{n-p},\\]\n\\(L\\) full rank \\(n\\).setup bit abstract, helps find least one concrete example \\(L\\). Start projection matrix \\(P_X := X(X^\\top X)^{-1}X^\\top\\). Since \\(P_X\\) symmetric, idempotent matrix rank \\(p\\) eignevalues can zero one, , number eigenvalues (called multiplicity) value one equal rank, exactly \\(p\\). addition, eigenvectors orthonormal. Hence,\n\\[P_X = \\begin{bmatrix}V_1 & V_2\\\\\nV_3 &V_4\\end{bmatrix}\\begin{bmatrix}1_{p\\times p} & 0_{p\\times n-p}\\\\\n0_{n-p \\times p} & 0_{n-p \\times n-p}\\end{bmatrix}\\begin{bmatrix}V_1 & V_2\\\\\nV_3 &V_4\\end{bmatrix}^\\top.\\]\nfollows representation \n\\[P_X = \\begin{bmatrix}V_1 \\\\\nV_3\\end{bmatrix}\\begin{bmatrix}V_1 \\\\\nV_3 \\end{bmatrix}^\\top=:L_1L_1^\\top\\]\nFurthermore, \\(v\\) eigenvector \\(P_X\\) eigenvalue 1, \\(v\\) eigenvector \\(-P_X\\) eigenvalue 0 just definition eigenvalue (\\(Av=\\lambda v\\)). follows \n\\[-P_X = \\begin{bmatrix}V_2 \\\\\nV_4\\end{bmatrix}\\begin{bmatrix}V_2 \\\\\nV_4 \\end{bmatrix}^\\top=:L_2 L_2^\\top.\\]\nconstruction, \\(L_1L_2^\\top = 0\\), \\(L_1^\\top L_1 = I_p\\) \\(L_2^\\top L_2 = I_{n-p}\\). columns \\(L\\) orthogonal, \\(L\\) full rank.follows, note REML estimates invariant choice \\(L\\), long three properties.Now, consider full rank linear transformation \\(Y \\mapsto LY = [Y_1 \\quad Y_2]^\\top\\). properties \\(L\\), \n\\[LY = \\begin{bmatrix} Y_1\\\\Y_2 \\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\beta \\\\ 0\\end{bmatrix}, \\begin{bmatrix} L_1^\\top \\Sigma L_1 &L_1^\\top \\Sigma L_2\\\\ L_2^\\top \\Sigma L_1 & L_2^\\top \\Sigma L_2 \\end{bmatrix}\\right)\\]\n\\(\\Sigma = Cov(Y) = Z \\psi_\\theta Z^\\top + \\Lambda_\\theta\\).next step consider equivalent characterization distribution \\((Y_1, Y_2)\\) product conditional distribution \\(Y_1|Y_2 = y_2\\) marginal distribution \\(Y_2\\). Using general formulas conditional distribution multivariate normal random vector, get\n\\[Y_1|y_2 \\sim N(\\beta - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2, \\, L_1^\\top \\Sigma L_1-L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1} L_2^\\top \\Sigma L_1).\\]\ntedious linear algebra (omitted ) can used show covariance matrix actually equal \\((X^\\top \\Sigma^{-1}X)^{-1}\\), regardless specific choice \\(L\\).Therefore, conditional likelihood given \n\\[\\ell_c := \\text{const.} - \\tfrac12\\log |(X^\\top \\Sigma^{-1}X)^{-1}| -\\tfrac{1}{2}(y_1 - \\beta - y_2^\\star)X^\\top \\Sigma^{-1}X(y_1 - \\beta - y_2^\\star)^\\top, \\]\n\\(y_2^\\star = L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2\\)., tedious linear algebra computations can used show \n\\[L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top = \\Sigma^{-1} - \\Sigma^{-1}X(X^\\top\\Sigma^{-1} X)^{-1} X^\\top\\Sigma^{-1},\\]\nmay computed efficiently using equivalent form \\(\\Sigma^{-1}\\) given . Furthermore, block matrix determinant identity given \n\\[det\\begin{bmatrix}& B\\\\\nB^\\top & C\\end{bmatrix} = |C||- B^\\top C^{-1}B|,\\]\napplied \\(L^\\top\\Sigma L\\) can used show \n\\[\\log |L_2^\\top \\Sigma L_2| = \\log|L^\\top\\Sigma L| + \\log |X^\\top \\Sigma^{-1}X|.\\]\nStandard rules determinants imply\n\\[\\log|L^\\top\\Sigma L| = \\log|L^\\top L\\Sigma| = \\log|L^\\top L|+\\log|\\Sigma|.\\]\nUsing simplified expressions, noting \\(\\log|L^\\top L|\\) constant parameters, may write marginal (residual) likelihood \n\\[\\ell_r:= \\text{const}.-\\tfrac12 \\log |\\Sigma| +\\tfrac12 \\log|X^\\top \\Sigma^{-1} X| -\\tfrac12 y^\\top(\\Sigma^{-1} - \\Sigma^{-1}X(X^\\top\\Sigma^{-1} X)^{-1} X^\\top\\Sigma^{-1})y.\\]reason referring \\(\\ell_r\\) residual loglikelihood? Recall \\(\\ell_r\\) marginal loglikelihood \\(L_2^\\top Y\\) \\(L_2^\\top X = 0\\). Therefore,\n\\[E(L_2^\\top Y) = L_2^\\top X\\beta = (L_2^\\top X)\\beta = 0\\beta = 0,\\]\nshows \\(L_2^\\top Y\\) behaves like residual vector.REML estimate \\(\\theta\\) given maximizer \\(\\ell_r\\) respect \\(\\theta\\). Note also simply MLE \\(\\theta\\) respect marginal likelihood \\(L_2^\\top Y\\), .e., likelihood “part data”.Next, consider maximizing conditional likelihood \\(\\ell_c\\) respect \\(\\beta\\) \\(\\theta=\\hat\\theta\\) fixed REML estimate, .e., \\(\\Sigma = \\hat\\Sigma\\). Take first derivative respect \\(\\beta\\), equate zero, observe\n\\[\\begin{align*}\n\\hat\\beta &= y_1 - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}y_2\\\\\n&= L_1^\\top y - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top y\\\\\n&= L_1^\\top \\Sigma \\Sigma^{-1}y - L_1^\\top \\Sigma L_2(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma\\Sigma^{-1}y\\\\\n& = (L_1^\\top \\Sigma - (L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma)\\Sigma^{-1}y\\\\\n& = (L_1^\\top \\Sigma L_1^\\top X - (L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}L_2^\\top \\Sigma L_1^\\top X)\\Sigma^{-1}y\\\\\n& = [(L_1^\\top \\Sigma L_1)-(L_1^\\top \\Sigma L_2)(L_2^\\top \\Sigma L_2)^{-1}(L_2^\\top \\Sigma L_1)]X\\Sigma^{-1}y\\\\\n& = (X^\\top \\Sigma^{-1}X)^{-1}X^\\top \\Sigma^{-1}y,\n\\end{align*}\\]\nusing \\(L_1^\\top X = \\) equivalence conditional covariance \\(Y_1|y_2\\) \\((X^\\top \\Sigma^{-1}X)^{-1}\\) shown .conclude REML estimator \\(\\beta\\) weighted least squares estimator weight matrix given REML-based plug-estimator \\(\\hat\\Sigma^{-1}\\).","code":""},{"path":"linear-mixed-models.html","id":"reml---bayesian-approach","chapter":"4 Linear Mixed Models","heading":"4.2.2 REML - Bayesian approach","text":"Suppose model (Bayesian sense) parameter \\(\\beta\\) improper constant prior, random component \\(\\alpha\\) multivariate normal prior covariance \\(\\psi\\). whatever parameters define \\(\\theta\\), endow constant/improper priors well. , combining priors multivariate normal likelihood following posterior:\n\\[\\log\\Pi_n(\\beta) = \\text{const.} - \\tfrac12 \\left(y - X\\beta - Z\\alpha\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta - Z\\alpha\\right) - \\tfrac12\\alpha^\\top \\psi^{-1}\\alpha . \\]\nnice (somewhat hard find) factorization posterior due Searle et al. (Variance Components, Section 9.2). Expand exponent:\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12 \\alpha^\\top (\\psi^{-1} + Z^\\top \\Lambda^{-1}Z)\\alpha + \\alpha^\\top Z^\\top \\Lambda^{-1}(y - X\\beta)\n\\end{align*}\\]\nLet \\(:=\\psi^{-1} + Z^\\top \\Lambda^{-1}Z\\) complete square \\(\\alpha\\):\n\\[\\begin{align*}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top \\Lambda^{-1}\\left(y - X\\beta\\right)\\\\\n&-\\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))\\\\\n&+\\tfrac12(y-X\\beta)^\\top\\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1} (y-X\\beta).\n\\end{align*}\\]\nCombine terms quadratic \\(y-X\\beta\\). resulting matrix \\(\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1}\\) equal \\((Z\\psi Z^\\top + \\Lambda)^{-1}\\), fact used frequentist approach REML. Therefore, posterior factorizes \n\\[\\begin{equation}\n\\tag{4.6}\n\\begin{aligned}\n\\log\\Pi_n(\\beta) &= \\text{const.} - \\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta) \\\\\n&- \\tfrac12(\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))^\\top (\\alpha - ^{-1}Z^\\top\\Lambda^{-1}(y-X\\beta))\n\\end{aligned}\n\\end{equation}\\]Integrate \\(\\alpha\\) obtain posterior \\(\\beta\\). Marginally, \\(\\alpha\\) multivariate normal; therefore, obtain\n\\[\\Pi_n(\\beta) = (2\\Pi)^{-n/2}|\\Lambda|^{-1/2}|\\psi|^{-1/2}||^{-1/2}\\exp\\left\\{-\\tfrac12 \\left(y - X\\beta\\right)^\\top (Z\\psi Z^\\top +\\Lambda)^{-1}(y - X\\beta)\\right\\}.\\]can verified \\(|Z\\psi Z^\\top +\\Lambda| = |\\Lambda||\\psi|||\\) using following identity\n\\[|D^{-1} - CA^{-1}B| = |D||^{-1}||- BD^{-1}C|\\]\n\\(Z\\psi Z^\\top +\\Lambda = D^{-1} - CA^{-1}B = (\\Lambda^{-1} - \\Lambda^{-1}ZA^{-1}Z^\\top \\Lambda^{-1})^{-1}\\); , see, e.g., Appendix M equation 31 Searle et al. (Variance Components). result, posterior \\(\\beta\\) multivariate normal covariance \\(Z\\psi Z^\\top +\\Lambda\\).exponent, add subtract \\(X\\hat\\beta = X(X^\\top \\Sigma^{-1} X)^{-1}X^\\top \\Sigma^{-1}y\\) (depends \\(\\theta\\)) quadratic obtain\n\\[\\begin{align*}\n\\left(y - X\\beta\\right)^\\top \\Sigma^{-1}(y - X\\beta) & = \\left(y - X\\hat\\beta\\right)^\\top \\Sigma^{-1}(y - X\\hat\\beta)\\\\\n& -2 \\left(y - X\\hat\\beta\\right)^\\top \\Sigma^{-1}(X\\hat\\beta - X\\beta)\\\\\n& + \\left(X\\hat\\beta - X\\beta\\right)^\\top \\Sigma^{-1}(X\\hat\\beta - X\\beta).\n\\end{align*}\\]\nstraightforward show middle term zero. Therefore, joint posterior \\((\\beta, \\theta)\\) may written\n\\[\\log \\Pi_n(\\beta, \\theta) = \\text{const}. -\\tfrac12\\log|\\Sigma|-\\tfrac12(y-X\\hat\\beta)^\\top \\Sigma^{-1}(y-X\\hat\\beta) -\\tfrac12(X\\hat\\beta - X\\beta)\\Sigma^{-1}(X\\hat\\beta - X\\beta)\\]integrate \\(\\beta\\) (w.r.t. multivariate normal density), obtain following marginal posterior variance parameters:\n\\[\\log\\Pi_n(\\theta) = \\text{const.} - \\tfrac{1}{2}\\log|\\Sigma| + \\tfrac{1}{2}\\log|X^\\top \\Sigma^{-1}X|-\\tfrac12 \\left(y - X\\hat\\beta\\right)^\\top \\Sigma^{-1}(y - X\\hat\\beta).\\]","code":""},{"path":"linear-mixed-models.html","id":"predicting-random-effects-and-responses","chapter":"4 Linear Mixed Models","heading":"4.2.3 Predicting random effects and responses","text":"set \\(\\beta = \\hat\\beta\\) (4.6) see distribution \\(\\alpha|\\beta = \\hat\\beta\\) multivariate normal mean (mode) equal \\(^{-1}Z^\\top\\Lambda^{-1}(y-X\\hat\\beta)\\). quantity predictor \\(\\hat\\alpha\\) \\(\\alpha\\) \\(\\beta\\) set REML estimator, clear Bayesian interpretation posterior mean maximum posteriori point estimate.best linear unbiased predictors \\(\\alpha\\) \\(Y\\) equal \\(\\hat\\alpha\\) \\(X\\hat\\beta + Z\\hat\\alpha\\).","code":""},{"path":"linear-mixed-models.html","id":"example-fitting-a-linear-mixed-model-to-loblolly-pines-data","chapter":"4 Linear Mixed Models","heading":"4.2.4 Example: Fitting a linear mixed model to Loblolly pines data","text":"data set “Loblolly” included nlme package contains longitudinal measurements tree height 14 Loblolly pine trees 6 age points.plot age height trees suggests common concave polynomial behavior—quite linear.model mean height behavior might choose , say, third degree polynomial function age. ways : specify polynomial age, polynomial {age - mean(age)}, basis orthogonal polynomials. second third strategies better first coefficients polynomial elements tend highly correlated unless polynomial elements orthogonal variable centered.model height polynomial age using fixed effects. However, want account variation polynomials trees can introduce tree-specific effects. tree-specific effects fixed random, choice depends sampling mechanism used collect data. specific trees interest sampled intentionally fixed tree effects appropriate. hand, observed trees represent random sample population trees, random tree effects appropriate. also worth pointing , cases, many degrees freedom needed estimate tree-specific fixed effects compared tree-specific random effects. polynomial mixed effects model looks like following\n\\[\\begin{align*}\nY_{ij} &= \\beta_0 + \\beta_1 X_{ij1}+ \\beta_2 X_{ij2}+ \\beta_3 X_{ij3}\\\\\n       &+ \\beta_{i1} X_{ij1}+ \\beta_{i2} X_{ij2}+ \\beta_{i3} X_{ij3}\\\\\n       & + \\epsilon_{ij}\n\\end{align*}\\]\n\\(=1, \\ldots, 14\\) indexes trees, \\(j = 1, \\ldots, 6\\) indexes ages, \\(\\beta_1\\) \\(\\beta_3\\) common coefficients fixed polynomial age effects, \\(\\beta_{i1}\\) \\(\\beta_{i3}\\) varying coefficients age effects tree. covariates \\(X_{ij1}\\) \\(X_{ij3}\\) represent third degree orthogonal polynomials; .e., simply age, age\\(^2\\) age\\(^3\\), shortly. random effects normal covariance \\(\\psi\\), .e., \\((\\beta_{i1},\\beta_{i2},\\beta_{i3})^\\top \\stackrel{iid}{\\sim}N(0,\\psi)\\).Since tree heights recorded time/ages also makes sense consider auto-correlated errors \\(\\epsilon_{,j}\\) non-zero correlation \\(j\\) within \\(\\). now, modeling errors zero-mean, time-dependent normal random variables non-zero auto-correlation lag 1, meaning \\((\\epsilon_{,j}, \\epsilon_{,j+1})\\) non-zero correlation, \\((\\epsilon_{i1},\\ldots, \\epsilon_{i6})\\stackrel{iid}{\\sim} N(0,\\Lambda)\\) \\(\\Lambda\\) positive definite tri-diagonal.specify model using lme function nlme package. default optimization specifications lme fails fit model, lme converges simply increase maximum number iterations function evaluations via lmeControl function.summary function contains everything needed determine fixed effects significant, provide straightforward explanation parameter estimates. little work, can reproduce estimated covariance parameters, estimated covariance fixed effects. getVarCov function provides estimated covariance random effects nested within Seed; call \\(\\hat\\psi_i\\) seeds \\(=1, \\ldots, 14\\). matrix \\(\\psi\\) block-diagonal, identical blocks \\(\\psi_i\\). estimated residual variance tri-diagonal constant main diagonal elements \\(\\hat\\sigma^2\\) given summary(model1)$sigma^2 constant -diagonal elements given coef(model1$modelStruct$corStruct, unconstrained = FALSE).following helper function Matrix package can help constructing \\(\\Psi\\) matrix:can match standard errors approximate p-values fixed effects using following calculations (p-values later). One thing mention: \\(Z\\) design matrix random effects seems present lme object, fit type model using lmer lme4 package easily allows us obtain model matrix. constructed “hand” large, block structure, ’s just easier extract built-functions.may noticed estimated auto-regressive covariance term essentially zero. motivates us fit nested model covariance term zero, .e., residual covariance structure \\(\\epsilon_i\\), \\(=1, \\ldots, n\\) simply iid variance \\(\\sigma^2\\).NLME package fitting linear mixed models. LME4 newer package compared NLME, generally fits models much faster thanks better use efficient linear algebra routines. However, LME4 flexible NLME comes available residual covariance structures one able fit. example, LME4 flexibility fit AR(1) structure fit model1 using NLME package.fit model2 using LME4. lmer model fitting function fails fit model default settings, much like lme function NLME package. main issue causing lack convergence check function evaluates gradient likelihood final parameter estimates requires near zero. violate criteria order get model fit, can using check.conv.grad argument control argument. coerce lmer converge using options see produces essentially estimates lme respect model2.Next, investigate various residual quantities determine model assumptions plausible.","code":"\nlibrary(nlme)\nlibrary(ggplot2)\nhead(Loblolly)## Grouped Data: height ~ age | Seed\n##    height age Seed\n## 1    4.51   3  301\n## 15  10.89   5  301\n## 29  28.72  10  301\n## 43  41.74  15  301\n## 57  52.70  20  301\n## 71  60.92  25  301\nggplot(data = Loblolly) + \n  geom_line(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = height)) + \n  facet_wrap(~ Seed, nrow = 2)\nlmc <- lmeControl(niterEM=1000,msMaxIter=1000,msMaxEval=1000)\n\nmodel1 <- lme(height ~ poly(age,3), data = Loblolly,\nrandom = list(Seed = ~ poly(age,3)),\ncorrelation = corAR1(form = ~ age|Seed), control = lmc)\n\nsummary(model1)## Linear mixed-effects model fit by REML\n##   Data: Loblolly \n##        AIC      BIC    logLik\n##   242.2632 280.3756 -105.1316\n## \n## Random effects:\n##  Formula: ~poly(age, 3) | Seed\n##  Structure: General positive-definite, Log-Cholesky parametrization\n##               StdDev    Corr                \n## (Intercept)   1.4302075 (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1 6.5811089  0.915              \n## poly(age, 3)2 2.6961875 -0.812 -0.509       \n## poly(age, 3)3 0.5943500 -0.125 -0.514 -0.477\n## Residual      0.5906168                     \n## \n## Correlation Structure: ARMA(1,0)\n##  Formula: ~age | Seed \n##  Parameter estimate(s):\n##          Phi1 \n## -2.944641e-07 \n## Fixed effects:  height ~ poly(age, 3) \n##                   Value Std.Error DF   t-value p-value\n## (Intercept)    32.36440 0.3876331 67  83.49237   0.000\n## poly(age, 3)1 186.44570 1.8553896 67 100.48870   0.000\n## poly(age, 3)2 -21.84656 0.9317043 67 -23.44796   0.000\n## poly(age, 3)3   0.05782 0.6116048 67   0.09454   0.925\n##  Correlation: \n##               (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1  0.856              \n## poly(age, 3)2 -0.619 -0.373       \n## poly(age, 3)3 -0.032 -0.126 -0.096\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.84043424 -0.43798962 -0.03581482  0.69326329  1.56774981 \n## \n## Number of Observations: 84\n## Number of Groups: 14\nbdiag_m <- function(lmat) {\n    ## Copyright (C) 2016 Martin Maechler, ETH Zurich\n    if(!length(lmat)) return(new(\"dgCMatrix\"))\n    stopifnot(is.list(lmat), is.matrix(lmat[[1]]),\n              (k <- (d <- dim(lmat[[1]]))[1]) == d[2], # k x k\n              all(vapply(lmat, dim, integer(2)) == k)) # all of them\n    N <- length(lmat)\n    if(N * k > .Machine$integer.max)\n        stop(\"resulting matrix too large; would be  M x M, with M=\", N*k)\n    M <- as.integer(N * k)\n    ## result: an   M x M  matrix\n    new(\"dgCMatrix\", Dim = c(M,M),\n        ## 'i :' maybe there's a faster way (w/o matrix indexing), but elegant?\n        i = as.vector(matrix(0L:(M-1L), nrow=k)[, rep(seq_len(N), each=k)]),\n        p = k * 0L:M,\n        x = as.double(unlist(lmat, recursive=FALSE, use.names=FALSE)))\n}\nlibrary(lme4)## Loading required package: Matrix## \n## Attaching package: 'lme4'## The following object is masked from 'package:nlme':\n## \n##     lmList\nmodel2lmer <- lmer(height ~ poly(age,3) + (poly(age,3) | Seed), data = Loblolly, control = lmerControl(optimizer=\"Nelder_Mead\", optCtrl = list(maxfun = 100000), check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL)))\nZ <- model.matrix(model2lmer, type = 'random')\npsi_i <- getVarCov(model1)\npsi <- bdiag_m(list(psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i))\ninv_psi <- solve(psi)\nphi <- coef(model1$modelStruct$corStruct, unconstrained = FALSE)\nL <- (summary(model1)$sigma^2)*diag(nrow(Loblolly))\nfor(i in 1:nrow(L)){\n  for(j in nrow(L)){\n    if(j==(i+1)||j==(i-1)){\n      L[i,j] <- phi\n    }\n  }\n}\ninvL <- solve(L)\nA <- inv_psi+t(Z)%*%invL%*%Z\ninvA <- solve(A)\nP<-invL-invL%*%Z%*%invA%*%t(Z)%*%invL\nX <- model.matrix(model2lmer, type = 'fixed')\ncov.beta.hat <- solve(t(X)%*%P%*%X)\nY <- Loblolly$height\nbeta.hat <- cov.beta.hat%*%t(X)%*%P%*%Y\nbeta.hat## 4 x 1 Matrix of class \"dgeMatrix\"\n##              [,1]\n## [1,]  32.36440477\n## [2,] 186.44569514\n## [3,] -21.84656380\n## [4,]   0.05781863\ncov.beta.hat## 4 x 4 Matrix of class \"dgeMatrix\"\n##              [,1]       [,2]        [,3]         [,4]\n## [1,]  0.150259390  0.6154960 -0.22367703 -0.007593674\n## [2,]  0.615495986  3.4424706 -0.64470993 -0.143514475\n## [3,] -0.223677034 -0.6447099  0.86807296 -0.054643528\n## [4,] -0.007593673 -0.1435145 -0.05464352  0.374060450\nbeta.hat / sqrt(diag(cov.beta.hat)) # t-values## 4 x 1 Matrix of class \"dgeMatrix\"\n##              [,1]\n## [1,]  83.49237468\n## [2,] 100.48870309\n## [3,] -23.44795779\n## [4,]   0.09453594\n1-pchisq(as.numeric((beta.hat / sqrt(diag(cov.beta.hat)))^2), 1) # rough p-values## [1] 0.0000000 0.0000000 0.0000000 0.9246834\n1-pf(as.numeric((beta.hat / sqrt(diag(cov.beta.hat)))^2), 1, nrow(Loblolly)-4)## [1] 0.0000000 0.0000000 0.0000000 0.9249198\nmodel2 <- lme(height ~ poly(age,3), data = Loblolly,\nrandom = list(Seed = ~ poly(age,3)),\ncorrelation = NULL, control = lmc)\n\nsummary(model2)## Linear mixed-effects model fit by REML\n##   Data: Loblolly \n##        AIC      BIC    logLik\n##   240.2632 275.9936 -105.1316\n## \n## Random effects:\n##  Formula: ~poly(age, 3) | Seed\n##  Structure: General positive-definite, Log-Cholesky parametrization\n##               StdDev    Corr                \n## (Intercept)   1.4301995 (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1 6.5810089  0.915              \n## poly(age, 3)2 2.6961948 -0.812 -0.509       \n## poly(age, 3)3 0.5943572 -0.125 -0.514 -0.477\n## Residual      0.5906184                     \n## \n## Fixed effects:  height ~ poly(age, 3) \n##                   Value Std.Error DF   t-value p-value\n## (Intercept)    32.36440 0.3876310 67  83.49282   0.000\n## poly(age, 3)1 186.44570 1.8553648 67 100.49005   0.000\n## poly(age, 3)2 -21.84656 0.9317069 67 -23.44789   0.000\n## poly(age, 3)3   0.05782 0.6116069 67   0.09454   0.925\n##  Correlation: \n##               (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1  0.856              \n## poly(age, 3)2 -0.619 -0.373       \n## poly(age, 3)3 -0.032 -0.126 -0.096\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.84042613 -0.43799912 -0.03581492  0.69325898  1.56775714 \n## \n## Number of Observations: 84\n## Number of Groups: 14\nlibrary(lme4)\nmodel2lmer <- lmer(height ~ poly(age,3) + (poly(age,3) | Seed), data = Loblolly, control = lmerControl(optimizer=\"Nelder_Mead\", optCtrl = list(maxfun = 100000), check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL)))\nsummary(model2lmer)## Linear mixed model fit by REML ['lmerMod']\n## Formula: height ~ poly(age, 3) + (poly(age, 3) | Seed)\n##    Data: Loblolly\n## Control: \n## lmerControl(optimizer = \"Nelder_Mead\", optCtrl = list(maxfun = 1e+05),  \n##     check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL))\n## \n## REML criterion at convergence: 210.6\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.83857 -0.45408 -0.02587  0.65608  1.54812 \n## \n## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr             \n##  Seed     (Intercept)    1.9497  1.3963                    \n##           poly(age, 3)1 40.8246  6.3894    0.91            \n##           poly(age, 3)2  7.2430  2.6913   -0.82 -0.50      \n##           poly(age, 3)3  0.6555  0.8096   -0.03 -0.37 -0.44\n##  Residual                0.3506  0.5922                    \n## Number of obs: 84, groups:  Seed, 14\n## \n## Fixed effects:\n##                Estimate Std. Error t value\n## (Intercept)    32.36440    0.37873  85.455\n## poly(age, 3)1 186.44570    1.80740 103.157\n## poly(age, 3)2 -21.84656    0.93167 -23.449\n## poly(age, 3)3   0.05782    0.63045   0.092\n## \n## Correlation of Fixed Effects:\n##             (Intr) p(,3)1 p(,3)2\n## poly(ag,3)1  0.846              \n## poly(ag,3)2 -0.621 -0.367       \n## poly(ag,3)3 -0.011 -0.119 -0.117"},{"path":"linear-mixed-models.html","id":"model-diagnostics","chapter":"4 Linear Mixed Models","heading":"4.3 Model diagnostics","text":"pure fixed effects linear models, residuals helpful checking assumptions like linearity normality. mixed effects models, ways define residuals:Marginal residuals residuals formed subtracting fitted fixed effect response, , \\(\\hat e_{ij} := Y_{ij} - x_{ij}^\\top \\hat\\beta\\)Marginal residuals residuals formed subtracting fitted fixed effect response, , \\(\\hat e_{ij} := Y_{ij} - x_{ij}^\\top \\hat\\beta\\)Standardized marginal residuals remove effect correlation. estimated covariance \\(\\hat e\\) \\(\\hat\\Sigma(\\hat e) := \\hat\\Sigma - X(X^\\top \\hat\\Sigma^{-1} X)^{-1}X^\\top\\). Let \\(\\hat\\Sigma_i(\\hat e)\\) denote fitted covariance marginal residuals within group \\(\\), let \\(\\hat\\Sigma_i(\\hat e)^{-1/2}\\) denote lower Cholesky factor \\(\\hat\\Sigma_i(\\hat e)^{-1}\\). , \\(\\hat \\epsilon_{} = \\hat\\Sigma_i(\\hat e)^{-1/2}\\hat e_i\\) denotes vector standardized marginal residuals group \\(\\).Standardized marginal residuals remove effect correlation. estimated covariance \\(\\hat e\\) \\(\\hat\\Sigma(\\hat e) := \\hat\\Sigma - X(X^\\top \\hat\\Sigma^{-1} X)^{-1}X^\\top\\). Let \\(\\hat\\Sigma_i(\\hat e)\\) denote fitted covariance marginal residuals within group \\(\\), let \\(\\hat\\Sigma_i(\\hat e)^{-1/2}\\) denote lower Cholesky factor \\(\\hat\\Sigma_i(\\hat e)^{-1}\\). , \\(\\hat \\epsilon_{} = \\hat\\Sigma_i(\\hat e)^{-1/2}\\hat e_i\\) denotes vector standardized marginal residuals group \\(\\).Conditional residuals also subtract predicted random effect, \\(\\hat \\xi_{ij} = Y_{ij} - x_{ij}^\\top \\hat\\beta - z_{ij}^\\top \\hat \\alpha\\)Conditional residuals also subtract predicted random effect, \\(\\hat \\xi_{ij} = Y_{ij} - x_{ij}^\\top \\hat\\beta - z_{ij}^\\top \\hat \\alpha\\)Standardized conditional residuals \\(\\hat\\phi_{ij}\\) formed standardizing conditional residuals estimated covariance, just case marginal residuals. estimated covariance conditional residuals given \n\\[\\hat\\Sigma(\\hat \\phi) := (-Z\\hat ^{-1}Z^\\top \\hat\\Lambda^{-1})\\hat\\Sigma(\\hat e)(-\\hat\\Lambda^{-1}Z\\hat ^{-1}Z^\\top).\\]Standardized conditional residuals \\(\\hat\\phi_{ij}\\) formed standardizing conditional residuals estimated covariance, just case marginal residuals. estimated covariance conditional residuals given \n\\[\\hat\\Sigma(\\hat \\phi) := (-Z\\hat ^{-1}Z^\\top \\hat\\Lambda^{-1})\\hat\\Sigma(\\hat e)(-\\hat\\Lambda^{-1}Z\\hat ^{-1}Z^\\top).\\]scatterplot marginal residuals versus fixed covariates useful checking assumption linearity. Trends plot indicate important omitted variable /nonlinearity.Quantile-quantile plots standardized marginal residuals within groups match standard normal distribution. , chosen within-group covariance structure good model given data. Comparing quantile-quantile plots group can useful assessing heteroskedasticity, although comparisons become rough groups individuals.Large absolute standardized conditional residuals indicate outliers. Plots standardized conditional residuals versus fitted values \\(X\\hat\\beta + Z\\hat\\alpha\\) may also reveal heteroskedasticity /groups—suggest modifying structure \\(\\Lambda\\) assumed \\(\\sigma^2 I_n\\) common.","code":""},{"path":"linear-mixed-models.html","id":"model-diagnostics-for-loblolly-pines-data","chapter":"4 Linear Mixed Models","heading":"4.3.1 Model diagnostics for Loblolly pines data","text":"begin plotting amrginal residuals cubic model fit LME4 called model2. marginal residuals exhibit pattern age. suggests fitting higher-order polynomial model.5th order polynomial appears fit data best; however, include 5th order polynomial random effects, need make choice random effects structure investigating standardized residuals.fit 5th-order polynomial model random intercepts Seed/tree. means trees age-height simply shifted () version polynomial. Based BLUPs, plotted , random effects structure seem sufficiently flexible model trees 329, 327, 305, example. certainly looks like need add random polynomial effects.Taking poorly-fitting BLUPs account, fit model quadratic random effects. predictions look much accurate including random effects.following calculations produce standardized marginal residuals, can investigate normality overall appropriateness within tree covariance structure.Next, discuss inference fixed effects model comparisons.","code":"\nmarginal.resids <- Loblolly$height - model.matrix(model2lmer, type = 'fixed')%*%fixef(model2lmer)\nplot(Loblolly$age, marginal.resids, xlab = 'age', ylab = 'marginal residual')\nlibrary(lme4)\nmodel3lmer <- lmer(height ~ poly(age,5) + (1 | Seed), data = Loblolly, control = lmerControl(optimizer=\"Nelder_Mead\", optCtrl = list(maxfun = 100000), check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL)))\nmarginal.resids3 <- Loblolly$height - model.matrix(model3lmer, type = 'fixed')%*%fixef(model3lmer)\nplot(Loblolly$age, marginal.resids3, xlab = 'age', ylab = 'marginal residual')\nLoblolly$fitted3 <- fitted(model3lmer)\n\nggplot(data = Loblolly) + \n  geom_line(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = fitted3, color = 'red')) +\n  facet_wrap(~ Seed, nrow = 2)\nmodel4lmer <- lmer(height ~ poly(age,5) + (poly(age,2) | Seed), data = Loblolly, control = lmerControl(optimizer=\"Nelder_Mead\", optCtrl = list(maxfun = 100000), check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL)))## boundary (singular) fit: see help('isSingular')\nmarginal.resids4 <- Loblolly$height - model.matrix(model4lmer, type = 'fixed')%*%fixef(model4lmer)\nLoblolly$fitted4 <- fitted(model4lmer)\nggplot(data = Loblolly) + \n  geom_line(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = height)) + \n  geom_point(mapping = aes(x = age, y = fitted4, color = 'red')) +\n  facet_wrap(~ Seed, nrow = 2)\nZ <- model.matrix(model4lmer, type = 'random')\ndim_cov <- dim(attr(VarCorr(model4lmer)[1]$Seed, 'correlation'))[1]\npsi_i <- as.matrix(VarCorr(model4lmer)[1]$Seed[1:dim_cov,1:dim_cov])\npsi <- bdiag_m(list(psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i,psi_i))\ninv_psi <- solve(psi)\nphi <- coef(model1$modelStruct$corStruct, unconstrained = FALSE)\nL <- (summary(model4lmer)$sigma^2)*diag(nrow(Loblolly))\ninvL <- solve(L)\nA <- inv_psi+t(Z)%*%invL%*%Z\ninvA <- solve(A)\nP<-invL-invL%*%Z%*%invA%*%t(Z)%*%invL\nX <- as.matrix(model.matrix(model4lmer, type = 'fixed'))\ncov.beta.hat <- solve(t(X)%*%P%*%X)\nY <- Loblolly$height\nbeta.hat <- cov.beta.hat%*%t(X)%*%P%*%Y\nPi <- as.matrix(P)[1:6,1:6]\ncholPi <- t(solve(chol(solve(Pi))))\nstandardized_resids <- rep(NA,84)\nseed=1\ncov.check <- 0\nfor(i in 1:14){\n  standardized_resids[(seed-1)*6+(1:6)] <- t(cholPi)%*%marginal.resids4[(seed-1)*6+(1:6)]\n  cov.check <- cov.check + sum((diag(6)-standardized_resids[(seed-1)*6+(1:6)]%*%t(standardized_resids[(seed-1)*6+(1:6)]))^2)\n  seed <- seed + 1\n}\n\n\nplot(Loblolly$Seed, standardized_resids, xlab = 'Seed', ylab = 'standardized marginal residual')\nB <- (diag(nrow(Loblolly)) - Z%*%invA%*%t(Z)%*%invL)\nP2 <- B%*%solve(P)%*%t(B)\ncholP2 <- t(chol(solve(P2)))\nstandardized_conditional_residuals <- cholP2%*%(Y - Loblolly$fitted4)\nqqnorm(standardized_conditional_residuals)\nqqline(standardized_conditional_residuals)\nshapiro.test(as.numeric(standardized_conditional_residuals))## \n##  Shapiro-Wilk normality test\n## \n## data:  as.numeric(standardized_conditional_residuals)\n## W = 0.95891, p-value = 0.008984"},{"path":"linear-mixed-models.html","id":"inference-for-fixed-effects-random-effects-and-model-comparisons","chapter":"4 Linear Mixed Models","heading":"4.4 Inference for fixed effects, random effects, and model comparisons","text":"common inferential questions concern regression coefficient vector \\(\\beta\\). Wald confidence regions \\(\\beta\\) defined sets\n\\[C_\\alpha := \\{b: (b - \\hat\\beta)(X^\\top V^{-1}X)^{-1}(b - \\hat\\beta)<\\chi^2_{1-\\alpha, p}\\}\\]\n\\(\\chi^2_{1-\\alpha, p}\\) upper \\(\\alpha\\) quantile Chi-squared distribution \\(p\\) degrees freedom. Unfortunately, regions exact \\(\\theta\\) known can severely undercover small sample sizes. Software packages typically perform adjustment, Kenward-Rogers scaling, generalization Satterthwaite’s approximation.point null hypothesis \\(H_0: \\beta = \\beta_{0}\\) Wald test statistic \n\\[W = (\\beta_0 - \\hat\\beta)(X^\\top V^{-1}X)^{-1}(\\beta_0 - \\hat\\beta)\\]\n\\(H_0\\) rejected \\(W > \\chi^2_{1-\\alpha, p}\\). generally, let \\(D\\) denote \\(r\\times p\\) matrix \\(r<p\\) full rank. Reject null hypothesis \\(H_0:D\\beta = b_0\\) \\(W > \\chi^2_{1-\\alpha, r}\\) \n\\[W = (b_0 - D\\hat\\beta)(D X^\\top V^{-1}X D^\\top)^{-1}(b_0 - D\\hat\\beta)\\](Generalized) Likelihood ratio tests may used compare nested models. Model nested B may written B B’s parameters equal zero, provided zero boundary value parameter omitted model . example, nested B contains subset B’s fixed effects, subset B’s random effects, nested residual covariance structure like iid versus AR(1) example, combination three. likelihood ratio tests performed compare nested models different fixed effects models fit using maximum likelihood rather REML residual likelihoods two different sets fixed effects comparable.example models nested cubic model called model1 versus model linear, quadratic fourth order fixed effects third order fixed effect. AIC /BIC comparisons (based full likelihoods) useful comparing non-nested models (remember: lower AIC/BIC better).mentioned previously, context linear mixed models, test statistics rarely exact Chi-squared F null distributions—exceptions pertain balanced experiments. fact, approximations may rough, depending model sample sizes. reason, LMER package even report p-values time. , options producing fairly reliable p-values. One option utilize lmerTest package provides functions testing fixed effects using Satterthwaite Kenward-Rogers corrected null distributions. Another option use parametric bootstrap (based sampling multivariate normal distribution) provide bootstrap-based tests confidence intervals. bootstrap method endorsed creators LME4, included bootMer function package facilitate bootstrap-based inference linear mixed models.","code":""},{"path":"linear-mixed-models.html","id":"inference-on-fixed-effects-for-loblolly-pines-data","chapter":"4 Linear Mixed Models","heading":"4.4.1 Inference on fixed effects for Loblolly pines data","text":"first question interest, hinted , whether need model residual error time-dependent, e.g., AR(1), structure. Recall point estimate auto-regressive correlation essentially zero, suggesting played role model fit. NLME package contains anova method comparing models. get habit comparing models fit maximum likelihood, makes little difference use REML comparison models differ covariance structure, fixed-effects structure. Whether compare models fit REML ML, see overwhelming evidence simpler model, one without auto-regressive error structure, just good complicated model.Next investigate whether cubic function simpler quadratic enough model age-height relationship. Using lmerTest can produce corrected p-values fixed effect within model2—model iid residual error structure. ’s clear cubic term provides benefit, omit fit model3. anova method also included lmer, notice lmer automatically recognizes want compare nested models differeing fixed effects refits models ML—delightful. , simpler, quadratic model clearly preferred.alternative Chi-squared/F-based tests performed using NLME lmerTest, provide bootstrap-based analysis using bootMer. downside bootMer slow due need refit linear mixed model bootstrap simulation. perform 100 bootstrap simulations model2—enough precise answers—enough get good idea cubic term ignorable fixed effects significant.","code":"\nanova(model1, model2)##        Model df      AIC      BIC    logLik   Test      L.Ratio p-value\n## model1     1 16 242.2632 280.3756 -105.1316                            \n## model2     2 15 240.2632 275.9936 -105.1316 1 vs 2 7.727313e-06  0.9978\nmodel2ml <- lme(height ~ poly(age,3), data = Loblolly,\nrandom = list(Seed = ~ poly(age,3)),\ncorrelation = NULL, control = lmc, method = 'ML')\nsummary(model2ml)## Linear mixed-effects model fit by maximum likelihood\n##   Data: Loblolly \n##      AIC      BIC   logLik\n##   243.71 280.1723 -106.855\n## \n## Random effects:\n##  Formula: ~poly(age, 3) | Seed\n##  Structure: General positive-definite, Log-Cholesky parametrization\n##               StdDev    Corr                \n## (Intercept)   1.3775085 (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1 6.3313490  0.917              \n## poly(age, 3)2 2.5866452 -0.815 -0.516       \n## poly(age, 3)3 0.5669663 -0.127 -0.512 -0.471\n## Residual      0.5799758                     \n## \n## Fixed effects:  height ~ poly(age, 3) \n##                   Value Std.Error DF   t-value p-value\n## (Intercept)    32.36440 0.3827785 67  84.55126  0.0000\n## poly(age, 3)1 186.44570 1.8329318 67 101.71993  0.0000\n## poly(age, 3)2 -21.84656 0.9246597 67 -23.62660  0.0000\n## poly(age, 3)3   0.05782 0.6142470 67   0.09413  0.9253\n##  Correlation: \n##               (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1  0.855              \n## poly(age, 3)2 -0.615 -0.374       \n## poly(age, 3)3 -0.032 -0.123 -0.091\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.88464862 -0.45304446 -0.03409533  0.70340275  1.60926654 \n## \n## Number of Observations: 84\n## Number of Groups: 14\nmodel1ml <- lme(height ~ poly(age,3), data = Loblolly,\nrandom = list(Seed = ~ poly(age,3)),\ncorrelation = corAR1(form = ~ age|Seed), control = lmc, method = 'ML')\n\nsummary(model1ml)## Linear mixed-effects model fit by maximum likelihood\n##   Data: Loblolly \n##      AIC      BIC   logLik\n##   245.71 284.6031 -106.855\n## \n## Random effects:\n##  Formula: ~poly(age, 3) | Seed\n##  Structure: General positive-definite, Log-Cholesky parametrization\n##               StdDev    Corr                \n## (Intercept)   1.3775459 (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1 6.3314438  0.917              \n## poly(age, 3)2 2.5866196 -0.815 -0.516       \n## poly(age, 3)3 0.5669811 -0.127 -0.512 -0.471\n## Residual      0.5799722                     \n## \n## Correlation Structure: ARMA(1,0)\n##  Formula: ~age | Seed \n##  Parameter estimate(s):\n##         Phi1 \n## 1.981777e-07 \n## Fixed effects:  height ~ poly(age, 3) \n##                   Value Std.Error DF   t-value p-value\n## (Intercept)    32.36440 0.3827885 67  84.54905  0.0000\n## poly(age, 3)1 186.44570 1.8329552 67 101.71863  0.0000\n## poly(age, 3)2 -21.84656 0.9246520 67 -23.62680  0.0000\n## poly(age, 3)3   0.05782 0.6142445 67   0.09413  0.9253\n##  Correlation: \n##               (Intr) p(,3)1 p(,3)2\n## poly(age, 3)1  0.855              \n## poly(age, 3)2 -0.615 -0.374       \n## poly(age, 3)3 -0.032 -0.123 -0.091\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.88463442 -0.45305701 -0.03408891  0.70342611  1.60926441 \n## \n## Number of Observations: 84\n## Number of Groups: 14\nanova(model1ml, model2ml)##          Model df    AIC      BIC   logLik   Test      L.Ratio p-value\n## model1ml     1 16 245.71 284.6031 -106.855                            \n## model2ml     2 15 243.71 280.1723 -106.855 1 vs 2 2.924349e-07  0.9996\nsummary(model2lmer)## Linear mixed model fit by REML ['lmerMod']\n## Formula: height ~ poly(age, 3) + (poly(age, 3) | Seed)\n##    Data: Loblolly\n## Control: \n## lmerControl(optimizer = \"Nelder_Mead\", optCtrl = list(maxfun = 1e+05),  \n##     check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL))\n## \n## REML criterion at convergence: 210.6\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.83857 -0.45408 -0.02587  0.65608  1.54812 \n## \n## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr             \n##  Seed     (Intercept)    1.9497  1.3963                    \n##           poly(age, 3)1 40.8246  6.3894    0.91            \n##           poly(age, 3)2  7.2430  2.6913   -0.82 -0.50      \n##           poly(age, 3)3  0.6555  0.8096   -0.03 -0.37 -0.44\n##  Residual                0.3506  0.5922                    \n## Number of obs: 84, groups:  Seed, 14\n## \n## Fixed effects:\n##                Estimate Std. Error t value\n## (Intercept)    32.36440    0.37873  85.455\n## poly(age, 3)1 186.44570    1.80740 103.157\n## poly(age, 3)2 -21.84656    0.93167 -23.449\n## poly(age, 3)3   0.05782    0.63045   0.092\n## \n## Correlation of Fixed Effects:\n##             (Intr) p(,3)1 p(,3)2\n## poly(ag,3)1  0.846              \n## poly(ag,3)2 -0.621 -0.367       \n## poly(ag,3)3 -0.011 -0.119 -0.117\nlibrary(lmerTest)## Warning: package 'lmerTest' was built under R version 4.2.2## \n## Attaching package: 'lmerTest'## The following object is masked from 'package:lme4':\n## \n##     lmer## The following object is masked from 'package:stats':\n## \n##     step\nX <- poly(Loblolly$age,3)\nLoblolly$linear <- X[,1]\nLoblolly$quadratic <- X[,2]\nLoblolly$cubic <- X[,3]\nmodel2lmer <- lmer(height ~ linear+quadratic+cubic + (linear+quadratic+cubic | Seed), data = Loblolly, control = lmerControl(optimizer=\"Nelder_Mead\", optCtrl = list(maxfun = 100000), check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL)))\nsummary(model2lmer)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: height ~ linear + quadratic + cubic + (linear + quadratic + cubic |  \n##     Seed)\n##    Data: Loblolly\n## Control: \n## lmerControl(optimizer = \"Nelder_Mead\", optCtrl = list(maxfun = 1e+05),  \n##     check.conv.grad = .makeCC(\"warning\", tol = 0.71, relTol = NULL))\n## \n## REML criterion at convergence: 210.6\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.83857 -0.45408 -0.02587  0.65608  1.54812 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr             \n##  Seed     (Intercept)  1.9497  1.3963                    \n##           linear      40.8246  6.3894    0.91            \n##           quadratic    7.2430  2.6913   -0.82 -0.50      \n##           cubic        0.6555  0.8096   -0.03 -0.37 -0.44\n##  Residual              0.3506  0.5922                    \n## Number of obs: 84, groups:  Seed, 14\n## \n## Fixed effects:\n##              Estimate Std. Error        df t value Pr(>|t|)    \n## (Intercept)  32.36440    0.37873  13.91879  85.455  < 2e-16 ***\n## linear      186.44570    1.80740  14.11660 103.157  < 2e-16 ***\n## quadratic   -21.84656    0.93167  14.62281 -23.449 5.21e-13 ***\n## cubic         0.05782    0.63045  26.65583   0.092    0.928    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##           (Intr) linear qudrtc\n## linear     0.846              \n## quadratic -0.621 -0.367       \n## cubic     -0.011 -0.119 -0.117\nanova(model2lmer, type = 'III', ddf = 'lme4')## Analysis of Variance Table\n##           npar Sum Sq Mean Sq    F value\n## linear       1 3702.8  3702.8 10559.8798\n## quadratic    1  195.3   195.3   556.9934\n## cubic        1    0.0     0.0     0.0084\nanova(model2lmer, type = 'III', ddf = 'Satt')## Type III Analysis of Variance Table with Satterthwaite's method\n##           Sum Sq Mean Sq NumDF  DenDF    F value    Pr(>F)    \n## linear    3731.4  3731.4     1 14.117 10641.3510 < 2.2e-16 ***\n## quadratic  192.8   192.8     1 14.623   549.8503 5.208e-13 ***\n## cubic        0.0     0.0     1 26.656     0.0084    0.9276    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(model2lmer, type = 'III', ddf = 'Ken')## Type III Analysis of Variance Table with Kenward-Roger's method\n##           Sum Sq Mean Sq NumDF DenDF    F value    Pr(>F)    \n## linear    3731.4  3731.4     1    13 10641.3510 < 2.2e-16 ***\n## quadratic  192.8   192.8     1    13   549.8503 5.059e-12 ***\n## cubic        0.0     0.0     1    13     0.0084    0.9283    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel3lmer <- lmer(height ~ linear+quadratic + (linear+quadratic | Seed), data = Loblolly)\nsummary(model3lmer)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: height ~ linear + quadratic + (linear + quadratic | Seed)\n##    Data: Loblolly\n## \n## REML criterion at convergence: 211.8\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.84241 -0.45922 -0.07456  0.69007  1.78254 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr       \n##  Seed     (Intercept)  2.045   1.4299              \n##           linear      43.148   6.5687    0.92      \n##           quadratic    7.337   2.7086   -0.81 -0.51\n##  Residual              0.350   0.5916              \n## Number of obs: 84, groups:  Seed, 14\n## \n## Fixed effects:\n##             Estimate Std. Error       df t value Pr(>|t|)    \n## (Intercept)  32.3644     0.3876  13.0062   83.51  < 2e-16 ***\n## linear      186.4457     1.8526  13.0479  100.64  < 2e-16 ***\n## quadratic   -21.8466     0.9349  14.5800  -23.37  5.8e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##           (Intr) linear\n## linear     0.858       \n## quadratic -0.617 -0.372\nanova(model3lmer, type = 'III', ddf = 'lme4')## Analysis of Variance Table\n##           npar Sum Sq Mean Sq F value\n## linear       1 3435.0  3435.0 9813.44\n## quadratic    1  191.1   191.1  546.03\nanova(model3lmer, type = 'III', ddf = 'Satt')## Type III Analysis of Variance Table with Satterthwaite's method\n##           Sum Sq Mean Sq NumDF  DenDF  F value    Pr(>F)    \n## linear    3545.3  3545.3     1 13.048 10128.62 < 2.2e-16 ***\n## quadratic  191.1   191.1     1 14.580   546.03 5.804e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(model3lmer, type = 'III', ddf = 'Ken')## Type III Analysis of Variance Table with Kenward-Roger's method\n##           Sum Sq Mean Sq NumDF DenDF  F value    Pr(>F)    \n## linear    3545.3  3545.3     1    13 10128.62 < 2.2e-16 ***\n## quadratic  191.1   191.1     1    13   546.03 5.289e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(model3lmer, model2lmer)## refitting model(s) with ML (instead of REML)## Data: Loblolly\n## Models:\n## model3lmer: height ~ linear + quadratic + (linear + quadratic | Seed)\n## model2lmer: height ~ linear + quadratic + cubic + (linear + quadratic + cubic | Seed)\n##            npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n## model3lmer   10 234.49 258.80 -107.24   214.49                     \n## model2lmer   15 243.71 280.17 -106.86   213.71 0.7787  5     0.9784\nfun <- function(model) summary(model)$coefficients[,1]\n#booted <- bootMer(model2lmer, fun, 100, type = 'parametric')\n\nlibrary(boot)\n#boot.ci(booted, conf = 0.95, type = 'basic', index = 1)\n#boot.ci(booted, conf = 0.95, type = 'basic', index = 2)\n#boot.ci(booted, conf = 0.95, type = 'basic', index = 3)\n#boot.ci(booted, conf = 0.95, type = 'basic', index = 4)"}]
