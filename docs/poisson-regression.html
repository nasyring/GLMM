<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Poisson Regression | GLMM</title>
<meta name="author" content="Nick Syring">
<meta name="description" content="3.1 Children Ever Born Data The “Children Ever Born” (CEB) dataset consists of grouped data on the number of births of Fijian women. The women are described according to their marriage duration in...">
<meta name="generator" content="bookdown 0.29 with bs4_book()">
<meta property="og:title" content="Chapter 3 Poisson Regression | GLMM">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Children Ever Born Data The “Children Ever Born” (CEB) dataset consists of grouped data on the number of births of Fijian women. The women are described according to their marriage duration in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Poisson Regression | GLMM">
<meta name="twitter:description" content="3.1 Children Ever Born Data The “Children Ever Born” (CEB) dataset consists of grouped data on the number of births of Fijian women. The women are described according to their marriage duration in...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">GLMM</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="active" href="poisson-regression.html"><span class="header-section-number">3</span> Poisson Regression</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">4</span> Linear Mixed Models</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="poisson-regression" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Poisson Regression<a class="anchor" aria-label="anchor" href="#poisson-regression"><i class="fas fa-link"></i></a>
</h1>
<div id="children-ever-born-data" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Children Ever Born Data<a class="anchor" aria-label="anchor" href="#children-ever-born-data"><i class="fas fa-link"></i></a>
</h2>
<p>The “Children Ever Born” (CEB) dataset consists of grouped data on the number of births of Fijian women. The women are described according to their marriage duration in years in ordinal levels: (0-4, 5-9, 10-14, 15-19, 20-24, 25-29); their place of residence (Suva—the capital city—Urban, or Rural); and, their level of education (none, lower primary, upper primary, secondary or greater). The count, mean, and variance of the number of children ever born, and the group size, is given for each group of women by cross-classified factorial level. These summaries are sufficient to model counts of children ever born by a Poisson distribution (each individual woman’s count is not needed).<br><br></p>
<p>The CEB data is an example of an observational dataset — the characteristics of the individuals are inherent rather than set by experimenters as in an experimental/controlled trial—but, that should be clear from the context. Several interesting questions may be answered using this data, such as: are more or fewer born children associated with higher or lower education among Fijian women; does an urban versus rural living location influence the number of children ever born; and, do the number of children ever born steadily increase with marrige duration, or tend to plateau?</p>
<a href="data:application/octet-stream;base64,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" download="ceb.dat">Download ceb.dat</a>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ceb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">'ceb.dat'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">ceb</span><span class="op">)</span></span></code></pre></div>
<pre><code>##   dur   res  educ mean  var  n     y
## 1 0-4  Suva  none 0.50 1.14  8  4.00
## 2 0-4  Suva lower 1.14 0.73 21 23.94
## 3 0-4  Suva upper 0.90 0.67 42 37.80
## 4 0-4  Suva  sec+ 0.73 0.48 51 37.23
## 5 0-4 urban  none 1.17 1.06 12 14.04
## 6 0-4 urban lower 0.85 1.59 27 22.95</code></pre>
<p>A statistician (or student statistician) familiar with multiple linear regression and/or ANOVA for factorial experiments may instinctively choose to fit a Gauss-Markov linear model to the CEB data, treating the responses as independent normal random variables. However, since the responses are counts, a Poisson model is more reasonable. But, just how does one perform <em>Poisson regression</em>? — as opposed to the familiar multiple linear regression described by the Gauss-Markov model:
<span class="math display">\[Y = X\beta+ \epsilon, \quad \epsilon \sim N_{n}(0_{n\times 1}, \sigma^2 I_n).\]</span></p>
<p>That is the motivation for this chapter, in which we will explore the family of <em>Generalized Linear Models</em> from defining and fitting the model, to performing inference and model diagnostics, all within the context of the CEB example.</p>
</div>
<div id="defining-glms" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Defining GLMs<a class="anchor" aria-label="anchor" href="#defining-glms"><i class="fas fa-link"></i></a>
</h2>
<p>For the CEB data we naturally want to model the CEB grouped counts as realizations of Poisson r.v.’s with means <span class="math inline">\(n_{j}x^\top_j\beta\)</span> where <span class="math inline">\(n_j\)</span> is the number of women in the <span class="math inline">\(j^{\text{th}}\)</span> factorial group, <span class="math inline">\(x_j\)</span> is the vector of their common covariates, and <span class="math inline">\(\beta\)</span> is the common regression coefficient vector. Then, the likelihood of the model is
<span class="math display">\[L(\beta;\text{data}) = \prod_{j=1}^{70}\frac{(n_{j}x^\top_j\beta)^{y_j}e^{-n_{j}x^\top_j\beta}}{y_j!}\]</span>
and the loglikelihood is given by
<span class="math display">\[\ell(\beta;\text{data}) = \text{constant} + \sum_{j=1}^{70}y_j\log(n_{j}x^\top_j\beta) - n_{j}x^\top_j\beta.\]</span>
The Poisson likelihood is a member of the Exponential Family, which contains all distributions with PDFs that may be expressed as
<span class="math display">\[f(y;\theta,\phi) = \exp\{[y\theta - b(\theta)]/a(\phi) + c(y,\phi)\}.\]</span>
Looking ahead, we will apply the exponential family model above to independent but not identically distributed responses, similar to the data we encounter in multiple linear regression and the Gauss-Markov model, so we will allow <span class="math inline">\(\theta\)</span> as well as the forms of the <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> functions to vary over observations, but we will fix <span class="math inline">\(\phi\)</span>, so that the loglikelihood for a sample of size <span class="math inline">\(n\)</span> may be written as follows:
<span class="math display">\[\ell(\beta;\text{data}) = \sum_{i=1}^n \{[y_i\theta_i - b_i(\theta_i)]/a_i(\phi) + c_i(\phi, y_i)\}.\]</span>
The Poisson regression model for grouped data is a fairly simple member of this family, having <span class="math inline">\(\theta = \log (n_{j}x^\top_j\beta)\)</span>, <span class="math inline">\(\phi = a(\phi) = 1\)</span>, and <span class="math inline">\(b(\theta) = \exp(\theta) =n_{j}x^\top_j\beta\)</span>. In fact, it is very often the case that GLMs satisfy <span class="math inline">\(a(\phi)\propto \phi\)</span> up to a known constant. <br><br>
In general, GLMs satisfy
<span class="math display">\[E(Y) = b'(\theta) \quad \text{and}\quad V(Y) = b''(\theta)a(\phi).\]</span>
For the Poisson regression model, in particular, we have
<span class="math display">\[\theta = \log(n_{j}x^\top_j\beta); \quad b(\theta) = \exp(\theta); \quad \text{and}\quad a(\phi) = 1\]</span>
<span class="math display">\[E(Y) = b'(\theta) = \frac{\partial}{\partial \theta}\exp(\theta) = \exp(\theta) = n_{j}x^\top_j\beta; \text{ and,}\]</span>
<span class="math display">\[V(Y) = b''(\theta)a(\phi) = \frac{\partial^2}{\partial \theta^2}\exp(\theta) = \exp(\theta) = n_{j}x^\top_j\beta.\]</span></p>
</div>
<div id="fitting-glms" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Fitting GLMs<a class="anchor" aria-label="anchor" href="#fitting-glms"><i class="fas fa-link"></i></a>
</h2>
<p>Like any other model defined by a likelihood, GLMs may be fit by maximizing the (log)likelihood. But, it is generally not the case that the maximizers (MLEs) are available in closed form. Instead, they are computed iteratively using Newton’s method or a similar iterative procedure. Refer again to the exponential family loglikelihood using the usual representation <span class="math inline">\(a_i(\phi) = \phi/w_i\)</span> where <span class="math inline">\(w_i\)</span> are known constants:
<span class="math display">\[\ell(\beta;\text{data}) = \sum_{i=1}^n \{w_i[y_i\theta_i - b_i(\theta_i)]/\phi + c_i(\phi, y_i)\}.\]</span>
Let <span class="math inline">\(\mu_i = E(Y_i)\)</span>. Then, <span class="math inline">\(b'(\theta_i) = \mu_i\)</span>, or, equivalently, <span class="math inline">\(g_c(\mu_i) = \theta_i\)</span> where <span class="math inline">\(g_c\)</span> is termed the <em>canonical link</em>; for example, <span class="math inline">\(g_c := \log\)</span> for the Poisson distribution. Additionally, let <span class="math inline">\(g\)</span> link the mean to the linear function of covariates, i.e., <span class="math inline">\(g(\mu_i) = \eta_i = x_i^\top\beta\)</span>; e.g., <span class="math inline">\(g\)</span> is the identity function for the Poisson model. Since <span class="math inline">\(b_i'(\theta_i)\)</span> is also equal to <span class="math inline">\(\mu_i\)</span> in the exponential family, we may differentiate the loglikelihood with respect to the regression parameter <span class="math inline">\(\beta\)</span> using the chain rule:
<span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \left\{\frac{w_i}{\phi}\left[y_i\frac{\partial \theta_i}{\partial\beta_j} - \frac{\partial b_i(\theta_i)}{\partial \beta_j}\right] + c_i(\phi, y_i)\right\}\]</span>
using
<span class="math display">\[\frac{\partial \theta_i}{\partial \beta_j} = \frac{\partial \theta_i}{\partial \mu_i}\frac{\partial \mu_i}{\partial \beta_j}.\]</span>
Since <span class="math inline">\(\mu_i = b_i'(\theta_i)\)</span> we have <span class="math inline">\(\partial \theta_i/\partial \mu_i = 1/b_i''(\theta_i)\)</span>. But, in light of <span class="math inline">\(\mu_i = b'(\theta_i)\)</span> we may always write <span class="math inline">\(b_i''(\theta_i)\)</span> as a function of <span class="math inline">\(\mu_i\)</span>, i.e., <span class="math inline">\(V(\mu_i) = b_i''(\theta_i)/w\)</span> so that <span class="math inline">\(V(Y_i) = V(\mu_i)\phi\)</span>. Moreover, since <span class="math inline">\(\mu_i = g^{-1}(x_i^\top \beta)\)</span> we have <span class="math inline">\(\partial\mu_i/\partial \beta_j = x_{ij}/g'[g^{-1}(x_i^\top \beta)]\)</span>. Substituting, we can write the score function using only <span class="math inline">\(\mu_i\)</span> as follows:
<span class="math display">\[\frac{\partial \ell}{\partial \beta_j} = \frac{1}{\phi}\sum_{i=1}^n \frac{y_i - \mu_i}{g'(\mu_i)V(\mu_i)}x_{ij}.\]</span>
The second (mixed partial) derivative may be written
<span class="math display">\[\frac{\partial^2 \ell}{\partial \beta_j\partial\beta_k} = -\frac{1}{\phi}\sum_{i=1}^n \frac{x_{ij}x_{ik}h(\mu_i)}{g'(\mu_i)^2V(\mu_i)}\]</span>
where <span class="math inline">\(h(\mu_i) = 1+(y_i-\mu_i)\{V'(\mu_i)/V(\mu_i) + g''(\mu_i)/g'(\mu_i)\}\)</span>. The expectation of the second derivative (which when multiplied by -1 appears in the Fisher information matrix) is the same quantity with <span class="math inline">\(h(\mu_i)\)</span> replaced by <span class="math inline">\(E[h(\mu_i)]\)</span>, which simply equals 1 because <span class="math inline">\(E(Y_i - \mu_i) = 0\)</span>.<br>
The Hessian of the loglikelihood is clearly a quadratic form <span class="math inline">\(\phi^{-1}X^\top WX\)</span> where <span class="math inline">\(X\)</span> is the <span class="math inline">\(n\times p\)</span> design matrix of covariates and <span class="math inline">\(W = [h(\mu_i)/\{g'(\mu_i)^2V(\mu_i)\}]\)</span> is an <span class="math inline">\(n\times n\)</span> diagonal matrix of “weights”. Less obvious, we may define <span class="math inline">\(G = \text{diag}\{g'(\mu_i)/h(\mu_i)\}\)</span> so that the gradient of the loglikelihood equals <span class="math inline">\(\phi^{-1}X^\top WG(y - \mu)\)</span>. With this clever rewriting, Newton’s method updates take on the form of a weighted least squares solution:
<span class="math display">\[\begin{align*}
\beta^{[k+1]} &amp;= \beta^{[k]} + (X^\top WX)^{-1}X^\top WG(y-\mu)\\
&amp; = (X^\top WX)^{-1}X^\top W\{G(y-\mu)X+\beta^{[k]}\}\\
&amp; = (X^\top WX)^{-1}X^\top Wz
\end{align*}\]</span>
where <span class="math inline">\(z := G(y-\mu)+X\beta^{[k]}\)</span> is sometimes referred to as the “pseudo-data”. Repeating the weighted least squares update, iteratively, until convergence, is termed <em>iteratively re-weighted least squares</em> (IRLS) since, of course, the weights in <span class="math inline">\(W\)</span> are updating with each iteration.</p>
<p><br><br></p>
<p>For our Poisson regression based on the grouped CEB data we have the following likelihood, gradient, and Hessian:
<span class="math display">\[\begin{align*}
&amp;\ell(\beta;\text{data}) = \sum_{j=1}^{70} \left[y_j x_j^\top \beta - n_j e^{x_j^\top \beta}\right]\\
&amp;\nabla_s \ell = \sum_{j=1}^{70} \left[y_j x_{js} - n_j x_{js}e^{x_j^\top \beta}\right]\\
&amp;\nabla^2_{s,t} \ell = -\sum_{j=1}^{70}  n_j x_{js}x_{jt}e^{x_j^\top \beta}.
\end{align*}\]</span></p>
<p>Rewriting the Hessian and gradient as above for the general exponential family GLM we have
<span class="math display">\[W_{k,k} = n_k\mu_k\quad\text{and}\quad G_k = (n_k\mu_k)^{-1}\]</span>
so that the IRLS updates are given by
<span class="math display">\[(X^\top WX)^{-1}X^\top Wz\]</span>
with <span class="math inline">\(z_k = (n_k\mu_k)^{-1}(y_k - n_k\mu_k) + x_k^\top \beta\)</span>.</p>
<div id="irls-for-the-ceb-data" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> IRLS for the CEB data<a class="anchor" aria-label="anchor" href="#irls-for-the-ceb-data"><i class="fas fa-link"></i></a>
</h3>
<p>Below we compute the MLEs for the Poisson regression of the grouped CEB data “by hand” using IRLS—and, also compare to the glm function in R. For our calculation we initialize the elements of the parameter vector <span class="math inline">\(\mu\)</span> by the sample means <span class="math inline">\(\mu_j = y_j/n_j\)</span>. We set the pseudo data equal to <span class="math inline">\(z_j = -(1/\mu_j)(y_j / n_j - \mu_j) + log(\mu_j)\)</span> and iterate the computation of least squares estimates <span class="math inline">\(\hat\beta\)</span>. <br><br></p>
<p>Note that the CEB data contains grouped “counts” computed as <span class="math inline">\(y_j = \mu_jn_j\)</span> where the <span class="math inline">\(\mu_j\)</span> values are rounded. And, as a result, the <span class="math inline">\(y_j\)</span> counts are not integers. This does not affect our “by hand” calculation of <span class="math inline">\(\hat\beta\)</span> whatsoever because we never use the full Poisson PMF in our computations; the glm function in R, on the other hand, will throw many warnings if the <span class="math inline">\(y_j\)</span> values are not rounded, apparently because it uses the PMF via dpois “under the hood”. The only differences between our fitted <span class="math inline">\(\hat\beta\)</span> and glm’s are due to rounding <span class="math inline">\(y_j\)</span>’s.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">ceb</span><span class="op">)</span></span>
<span><span class="va">group.sizes</span> <span class="op">&lt;-</span> <span class="va">ceb</span><span class="op">$</span><span class="va">n</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">ceb</span><span class="op">$</span><span class="va">y</span></span>
<span><span class="co"># IRLS - factor coding</span></span>
<span><span class="co"># initialize with mu = Y/group.sizes</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>contrasts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'contr.treatment'</span>, <span class="st">'contr.treatment'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">dur</span><span class="op">+</span><span class="va">res</span><span class="op">+</span><span class="va">educ</span>, data <span class="op">=</span> <span class="va">ceb</span><span class="op">)</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">Y</span><span class="op">/</span><span class="va">group.sizes</span></span>
<span><span class="va">XB</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">mu</span><span class="op">)</span></span>
<span><span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">mu</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="va">Y</span><span class="op">/</span><span class="va">group.sizes</span><span class="op">-</span><span class="va">mu</span><span class="op">)</span> <span class="op">+</span> <span class="va">XB</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">z</span></span>
<span><span class="va">tol</span> <span class="op">&lt;-</span> <span class="fl">0.0001</span></span>
<span><span class="va">difference</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">maxiter</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">iter</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">while</span><span class="op">(</span><span class="op">(</span><span class="va">difference</span> <span class="op">&gt;</span> <span class="va">tol</span><span class="op">)</span> <span class="op">&amp;</span> <span class="op">(</span><span class="va">iter</span> <span class="op">&lt;</span> <span class="va">maxiter</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">XB</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">beta</span></span>
<span>  <span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">XB</span><span class="op">)</span></span>
<span>  <span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">z</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">Y</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">W</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="va">XB</span></span>
<span>  <span class="va">beta.old</span> <span class="op">&lt;-</span> <span class="va">beta</span></span>
<span>  <span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">z</span></span>
<span>  <span class="va">difference</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">beta</span> <span class="op">-</span> <span class="va">beta.old</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">iter</span><span class="op">&lt;-</span><span class="va">iter</span><span class="op">+</span><span class="fl">1</span></span>
<span><span class="op">}</span></span>
<span><span class="va">beta</span></span></code></pre></div>
<pre><code>##                    [,1]
## (Intercept)  0.05695417
## dur10-14     1.37053208
## dur15-19     1.61423104
## dur20-24     1.78548879
## dur25-29     1.97679469
## dur5-9       0.99765038
## resSuva     -0.15121728
## resurban    -0.03895822
## educnone    -0.02308034
## educsec+    -0.33266474
## educupper   -0.12474575</code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## the glm function can be used with offset equal to logarithm of the group sizes</span></span>
<span><span class="va">my.glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">~</span><span class="va">dur</span><span class="op">+</span><span class="va">res</span><span class="op">+</span><span class="va">educ</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">poisson</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">'log'</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">ceb</span>, offset <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">my.glm</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = round(y) ~ dur + res + educ, family = poisson(link = "log"), 
##     data = ceb, offset = log(n))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2960  -0.6641   0.0725   0.6336   3.6782  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.05754    0.04803   1.198    0.231    
## dur10-14     1.36940    0.05107  26.815  &lt; 2e-16 ***
## dur15-19     1.61376    0.05119  31.522  &lt; 2e-16 ***
## dur20-24     1.78491    0.05121  34.852  &lt; 2e-16 ***
## dur25-29     1.97641    0.05003  39.501  &lt; 2e-16 ***
## dur5-9       0.99693    0.05274  18.902  &lt; 2e-16 ***
## resSuva     -0.15166    0.02833  -5.353 8.63e-08 ***
## resurban    -0.03924    0.02463  -1.594    0.111    
## educnone    -0.02297    0.02266  -1.014    0.311    
## educsec+    -0.33312    0.05390  -6.180 6.41e-10 ***
## educupper   -0.12425    0.03000  -4.142 3.44e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 3731.852  on 69  degrees of freedom
## Residual deviance:   70.665  on 59  degrees of freedom
## AIC: 522.14
## 
## Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<div id="inference-on-glms" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Inference on GLMs<a class="anchor" aria-label="anchor" href="#inference-on-glms"><i class="fas fa-link"></i></a>
</h2>
<p>No doubt you noticed the glm function output produces standard errors, “z” values, and p-values for each fitted coefficient, just as you would find accompanying summarized lm output. But, what is the justification for these p-values?<br><br></p>
<p>Since <span class="math inline">\(\hat\beta\)</span> is an MLE, standard likelihood theory holds that <span class="math inline">\(\hat\beta \stackrel{\cdot}{\sim}N_p(\beta, I^{-1}(\beta))\)</span> for “large” <span class="math inline">\(n\)</span> where <span class="math inline">\(I^{-1}(\beta)\)</span> is the Fisher information. As discussed above, the “observed information” (which is the Hessian) is equal to <span class="math inline">\(-\phi^{-1}(X^\top WX)^{-1}\)</span> where <span class="math inline">\(W\)</span> is the weight matrix in the final iteration of IRLS, and <span class="math inline">\(-\phi^{-1}(X^\top WX)^{-1}\)</span> coincides with the Fisher Information when we replace <span class="math inline">\(h(\mu_i)\)</span> by <span class="math inline">\(E(h(\mu_i))=1\)</span> in the IRLS (then called Fisher scoring) updates. Therefore, <span class="math inline">\(\hat\beta \stackrel{\cdot}{\sim}N_p(\beta, \phi^{-1}(X^\top W X)^{-1})\)</span> for “large” <span class="math inline">\(n\)</span>, but this is only useable if <span class="math inline">\(\phi\)</span> is known (which it is, and equals 1, for the Poisson and Binomial models). Otherwise, we replace <span class="math inline">\(\phi\)</span> by its MLE and use the corresponding Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n - p\)</span> degrees of freedom rather than the standard normal for inference on <span class="math inline">\(\beta_j\)</span>. <br><br></p>
<p>The upshot is that we may base tests for, e.g., <span class="math inline">\(H_0:\beta_j = 0\)</span>, on Student’s <span class="math inline">\(t\)</span> with <span class="math inline">\(n-p\)</span> df; i.e.,
<span class="math display">\[\text{Reject }H_0:\beta_j = 0 \text{ if }\left|\frac{\hat\beta_j}{\sqrt{\hat\phi^{-1}(X^\top W X)^{-1}_{j,j}}}\right| &gt; t_{1-\alpha/2,n-p}.\]</span>
Multivariate Wald simultaneous <span class="math inline">\(100(1-\alpha)\%\)</span> confidence regions are given by the eliiptical contours:
<span class="math display">\[\phi\text{ known: }\quad \{\beta: (\hat\beta - \beta)^\top\phi^{-1}(X^\top W X)^{-1}(\hat\beta - \beta)&lt; \chi^2_{1-\alpha,p} \}\]</span>
<span class="math display">\[\phi\text{ unknown: }\quad \{\beta: (\hat\beta - \beta)^\top\hat\phi^{-1}(X^\top W X)^{-1}(\hat\beta - \beta)&lt; F_{1-\alpha,p, n-p} \}\]</span></p>
<p>Moreover, an approximate <span class="math inline">\(95\%\)</span> CI for the mean response <span class="math inline">\(\mu = g^{-1}(x^\top \beta)\)</span> for covariate vector <span class="math inline">\(x\)</span> is given by the Delta method interval:</p>
<p><span class="math display">\[g^{-1}(x^\top \hat\beta)\pm t_{1-\alpha/2,n-p}\sqrt{\hat\phi^{-1} \left[\nabla_{\beta} g^{-1}(x^\top \beta)\right]^\top(X^\top W X)^{-1}\left[\nabla_{\beta} g^{-1}(x^\top \beta)\right]}.\]</span></p>
<p><br>
In multiple linear regression (Gauss-Markov) models we use partial F tests (which are likelihood ratio tests) to test for significance of sets of covariates, i.e., <span class="math inline">\(H_0: \beta_j = \beta_{j+1} = \cdots = \beta_{j+\ell} = 0\)</span>. For GLMs, similar tests are available. For models where <span class="math inline">\(\phi\)</span> is known, we have
<span class="math display">\[-2\{\ell(\hat\beta_{h_0}) - \ell(\hat\beta)\}\stackrel{H_0}{\sim} F_{\ell,n-p}\]</span>
where <span class="math inline">\(\hat\beta_{h_0}\)</span> is the MLE under the null hypothesis where <span class="math inline">\(\ell\)</span> coefficients are set equal to 0.<br><br><br></p>
<p>There are several methods to estimate <span class="math inline">\(\phi\)</span> when unknown. Pearson’s method observes that
<span class="math display">\[\phi^{-1}X^2 \stackrel{\cdot}{\sim}\chi^2_{n-p}\quad \text{where}\quad X^2 :=\sum_{i=1}^n \frac{(Y_i - \hat\mu_i)^2}{\phi V(\hat\mu_i)}\]</span>
if the model fits the data adequately. Hence, <span class="math inline">\(\hat\phi_P = X^2/(n-p)\)</span> ought to be a good estimate of <span class="math inline">\(\phi\)</span>. For certain data sets, such as Poisson data with low counts, the Pearson estimate may behave badly, and a modified version (Fletcher’s estimator) is preferred:
<span class="math display">\[\hat\phi_F = \frac{\hat\phi_P}{1-\overline s}, \quad\text{where}\]</span>
<span class="math display">\[\overline s:=n^{-1}\sum_{i=1}^n V'(\hat\mu_i)\frac{(y_i - \hat\mu_i)}{V(\hat\mu_i)}.\]</span></p>
<p>The <em>Deviance difference</em> for models A and B where <span class="math inline">\(A\subset B\)</span> is given by <span class="math inline">\(D_A - D_B = -2\{\ell(\hat\beta_A) - \ell(\hat\beta_B)\}\phi\)</span>. The scaled deviance difference is
<span class="math display">\[D_A^* - D_B^* = -2\{\ell(\hat\beta_A) - \ell(\hat\beta_B)\}\stackrel{\cdot}{\sim}\chi^2_{\ell}\]</span>
where the difference in number of fitted parameters is <span class="math inline">\(\ell\)</span>. Despite the notation, the scaled deviance difference does depend on <span class="math inline">\(\phi\)</span>, whereas the deviance difference does not. Two alternative tests make use of the scaled deviance to compare nested GLMs. The first is analogous to the partial F test:
<span class="math display">\[F = \frac{(D_A^* - D_B^*)/\ell}{D_B^* / (n-p)}\stackrel{\cdot}{\sim} F_{\ell, n-p}\]</span>
but the approximation to the F distribution is very rough. Alternatively, we can replace the scale parameter in the scaled deviance by its Pearson (or Fletcher) estimator and obtain
<span class="math display">\[\hat D_A^* - \hat D_B^*\stackrel{\cdot}{\sim}\chi^2_{\ell},\]</span>
where the “hat” on the scaled deviances indicates their dependence on <span class="math inline">\(\hat\phi\)</span>.</p>
<div id="inference-and-prediction-for-the-ceb-data-using-poisson-regression" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> Inference and prediction for the CEB data using Poisson regression<a class="anchor" aria-label="anchor" href="#inference-and-prediction-for-the-ceb-data-using-poisson-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Next, we’ll demonstrate computations of confidence intervals and tests for significance of sets of covariates within the Poisson regression model for the CEB data. We can either do these computations “by hand” using the results of our IRLS or we can use built-in R functions like glm and confint. <br><br></p>
<p>For Wald-type confidence intervals for regression coefficients we require the inverse Hessian (the estimate of the inverse Fisher information, equal when Fisher scoring is used). We compute this from the final iteration of IRLS (assuming the algorithm converged). For most GLMs we will need to estimate the scale parameter <span class="math inline">\(\phi\)</span> and we include the Pearson and Fletcher estimates below—but in the Poisson model <span class="math inline">\(\phi=1\)</span>. Note both estimates are close to 1. The p-values included in the summarized glm output imply the intercept is not significantly different from zero, but that several other coefficients are different from zero, including, e.g., <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span>. Our Wald-type CIs and p-values agree with the glm p-values; for example, our 95% CI for <span class="math inline">\(\beta_0\)</span> computed “by hand” is (-0.0372 0.1511), suggesting the intercept is not significantly different from zero, and our 95% CI for <span class="math inline">\(\beta_1\)</span> is (1.2704, 1.4706) with a p-value for the test of <span class="math inline">\(\beta_1 = 0\)</span> that is indistinguishable from 0. <br><br></p>
<p>We computed the deviance difference (which is minus twice the difference in loglikelihood) between the intercept-only model and the full model. The deviance difference is about 3661 on ten degrees of freedom (the difference in number of fitted coefficients between the models). If the intercept only model fits, then this deviance difference should be comparable to a Chi-squared r.v. with 10 degrees of freedom, but the corresponding p-value is basically zero, supporting the claim that the full model fits much better than the intercept-only model. Compare our deviance difference calculation to the output of glm: glm includes the null deviance and residual deviance, the difference of these two gives the deviance difference statistic used to compare the intercept-only and full models. It is about 3661, agreeing almost exactly with our “by hand” calculation.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Hessian</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span></span>
<span><span class="va">inv.Hessian</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">Hessian</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">beta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Just for illustration, phi = 1 for Poisson model</span></span>
<span><span class="va">Pearson.X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="op">(</span><span class="va">Y</span> <span class="op">-</span> <span class="va">group.sizes</span> <span class="op">*</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">group.sizes</span> <span class="op">*</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Pearson.phi</span> <span class="op">&lt;-</span> <span class="va">Pearson.X2</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span><span class="op">-</span><span class="va">p</span><span class="op">)</span></span>
<span><span class="va">s.bar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y</span> <span class="op">-</span> <span class="va">group.sizes</span> <span class="op">*</span> <span class="va">mu</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">group.sizes</span> <span class="op">*</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Fletcher.phi</span> <span class="op">&lt;-</span> <span class="va">Pearson.phi</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">s.bar</span><span class="op">)</span></span>
<span><span class="va">Pearson.phi</span> </span></code></pre></div>
<pre><code>## [1] 1.211949</code></pre>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Fletcher.phi</span> </span></code></pre></div>
<pre><code>## [1] 1.194283</code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># CIs for the first 4 regression coefficients</span></span>
<span><span class="co">#   If phi were unknown, it's estimate would appear in the estimated standard error of the </span></span>
<span><span class="co">#   estimated coefficient</span></span>
<span><span class="co"># beta[1] + qt(c(0.025,0.975),n-p)*sqrt((1/Pearson.phi)*inv.Hessian[1,1])</span></span>
<span><span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="fl">0.975</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">inv.Hessian</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] -0.03721264  0.15112097</code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="fl">0.975</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">inv.Hessian</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1.270425 1.470639</code></pre>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">inv.Hessian</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="fl">0.975</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">inv.Hessian</span><span class="op">[</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1.513870 1.714592</code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">beta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="fl">0.975</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">inv.Hessian</span><span class="op">[</span><span class="fl">4</span>,<span class="fl">4</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1.685091 1.885886</code></pre>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The R function confint can also be used with GLMs to provide confidence intervals for coefficients</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">my.glm</span>, <span class="st">'dur10-14'</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##    2.5 %   97.5 % 
## 1.270141 1.470370</code></pre>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># "Model F Test" - testing that all (non-intercept) coefficients equal zero</span></span>
<span><span class="co"># for Poisson model, since phi is known, we have a LRT equivalent to a partial F test</span></span>
<span><span class="co"># based on deviance difference</span></span>
<span><span class="va">Ybar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">)</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Y</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">Ybar</span><span class="op">)</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">Ybar</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Y</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Ybar</span></span></code></pre></div>
<pre><code>## [1] 3.960497</code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">D</span></span></code></pre></div>
<pre><code>## [1] 3660.872</code></pre>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">D</span>,<span class="va">p</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># with rounded Ys and using glm output</span></span>
<span><span class="va">Yr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">mu.glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">X</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">my.glm</span><span class="op">$</span><span class="va">coefficients</span>,<span class="va">p</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Ybar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Yr</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">)</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Yr</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">Ybar</span><span class="op">)</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">Ybar</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Yr</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu.glm</span><span class="op">)</span><span class="op">)</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu.glm</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Ybar</span></span></code></pre></div>
<pre><code>## [1] 3.960403</code></pre>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">D</span></span></code></pre></div>
<pre><code>## [1] 3661.186</code></pre>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">D</span>,<span class="va">p</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
</div>
</div>
<div id="model-checkingdiagnostics" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Model Checking/Diagnostics<a class="anchor" aria-label="anchor" href="#model-checkingdiagnostics"><i class="fas fa-link"></i></a>
</h2>
<p>It is essential statistical practice to check whether the model adequately fits the data. If the model fits poorly, then inferences/predictions garnered from the model are suspect. In multiple linear regression we assess model fit by analyzing residuals. When the multiple linear regression model fits, the residuals should be approximately standard normal. Lack of fit manifests in residuals that are skewed or heavy-tailed, contain outliers, or tend to increase in variability with one or more covariates and/or the predicted responses. <br><br></p>
<p>Model-checking for GLMs can be done in essentially the same manner—the key is to find a quantity that reasonably fills the role of residuals in multiple linear regression. For GLMs, there are two choices, the Pearson residuals and the deviance residuals. <br><br></p>
<p>Pearson residuals are defined <span class="math inline">\(e^P_i = \frac{Y_i - \hat\mu_i}{\sqrt{V(\mu_i)}}\)</span>, or, sometimes, <span class="math inline">\(e^P_i = \frac{Y_i - \hat\mu_i}{\sqrt{\hat\phi V(\mu_i)}}\)</span>. The first definition results in quantities that should be approximately zero-mean normal random variates with variance <span class="math inline">\(\phi\)</span> whereas the second definition provides standard normal quantities. Some practitioners prefer the deviance residuals to the Pearson residuals as the latter are often observed to be asymmetric and, hence, not as “normal” as expected. The deviance is equal to
<span class="math display">\[\text{Deviance} = -2\phi\{\ell(\hat\beta) - \sup \ell\}\]</span>
where <span class="math inline">\(\ell(\hat\beta)\)</span> is the loglikelihood evaluated at the MLEs and <span class="math inline">\(\sup \ell\)</span> is the loglikelihood with <span class="math inline">\(\mu_i = y_i\)</span>, i.e., a totally saturated model. Multiplying by <span class="math inline">\(\phi\)</span> removes the dependence of the loglikelihood on the scale parameter. The deviance can be written as a sum of terms, say, <span class="math inline">\(\text{Deviance} = \sum_{i=1}^n d_i\)</span>, and each observation’s contribution <span class="math inline">\(d_i\)</span> to the deviance is used to define the deviance residuals as follows:
<span class="math display">\[e^D_i = \text{sign}(y_i - \mu_i)\sqrt{d_i}.\]</span></p>
<div id="residual-analysis-for-ceb-data" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Residual analysis for CEB data<a class="anchor" aria-label="anchor" href="#residual-analysis-for-ceb-data"><i class="fas fa-link"></i></a>
</h3>
<p>Using either the deviance residuals or the Pearson residuals shows a few important things. First, the residuals are approximately standard normal, with the exception of one “outlier”, observation 17. Second, if we sort the observations by fitted mean response <span class="math inline">\(\hat\mu_i\)</span> from least to greatest, we see absolutely no trend up or down, or any pattern at all, in the residuals. This implies we have correctly modeled the mean-variance relationship, and that we have also correctly modeled the mean as a linear function of the covariates. <br><br></p>
<p>We obtain similar plots by simply running “plot(glm object)”, but there are some default differences. Plotting a glm object will provide plots of “residuals” versus “fitted values”, but in fact, the labels on these plots are slightly misleading. The residuals vs. fitted values plots uses either Pearson or deviance residuals (cannot tell for sure from the plot or the documentation) versus the logarithm of the fitted responses <span class="math inline">\(\log(\hat y_j) =\log(n_j\hat\mu_j)\)</span>. <br><br></p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dev</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="op">(</span><span class="va">Y</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span><span class="op">-</span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span> <span class="op">-</span> <span class="op">(</span><span class="op">(</span><span class="va">Y</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span><span class="op">-</span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">deviance.resids</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="op">(</span><span class="va">Y</span><span class="op">-</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fl">0</span>,<span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">dev</span><span class="op">)</span></span>
<span><span class="va">pearson.resids</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">Y</span> <span class="op">-</span> <span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># residual plots using deviance residuals</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/qqnorm.html">qqnorm</a></span><span class="op">(</span><span class="va">deviance.resids</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/qqnorm.html">qqline</a></span><span class="op">(</span><span class="va">deviance.resids</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">deviance.resids</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  deviance.resids
## W = 0.96065, p-value = 0.02719</code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">deviance.resids</span>, freq <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">dev.norm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">deviance.resids</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">deviance.resids</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="va">dev.norm</span>, <span class="op">-</span><span class="fl">5</span>,<span class="fl">5</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-2.png" width="672"></div>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mu</span>, <span class="va">deviance.resids</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-3.png" width="672"></div>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># residual plots using pearson residuals</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/qqnorm.html">qqnorm</a></span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/qqnorm.html">qqline</a></span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-4.png" width="672"></div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  pearson.resids
## W = 0.94988, p-value = 0.007147</code></pre>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">pearson.resids</span>, freq <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">pearson.norm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>,<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="va">pearson.norm</span>, <span class="op">-</span><span class="fl">5</span>,<span class="fl">5</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-5.png" width="672"></div>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mu</span>, <span class="va">pearson.resids</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-6.png" width="672"></div>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># plots are using what look like Pearson (or maybe deviance) residuals versus predicted values under canonical link, i.e. log(Y hat)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">group.sizes</span><span class="op">*</span><span class="va">mu</span><span class="op">)</span>, <span class="va">pearson.resids</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="GLMM_files/figure-html/unnamed-chunk-5-7.png" width="672"></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">my.glm</span><span class="op">)</span></span></code></pre></div>
<p><img src="GLMM_files/figure-html/unnamed-chunk-5-8.png" width="672"><img src="GLMM_files/figure-html/unnamed-chunk-5-9.png" width="672"><img src="GLMM_files/figure-html/unnamed-chunk-5-10.png" width="672"><img src="GLMM_files/figure-html/unnamed-chunk-5-11.png" width="672"></p>
</div>
</div>
<div id="outlier-analysis-using-cooks-distance" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Outlier analysis using Cook’s distance<a class="anchor" aria-label="anchor" href="#outlier-analysis-using-cooks-distance"><i class="fas fa-link"></i></a>
</h2>
<p>Outliers are observations corresponding to large residuals. They may occur due to chance, or, more likely, indicate a lack of model fit for a specific observation. The lack of fit may be due to something innocuous like a mistake made in recording data; or, it may be that the observation in question is very different from the others in the sample, and does not follow the same response-covariate relationship.<br><br></p>
<p>Outliers are not a problem, however, unless their inclusion causes the fitted model to be substantially different than had they been excluded. Therefore, we do not necessarily care that a particular residual is large, but that it is influential on the model fit. <br><br></p>
<p>In multiple linear regression we may measure this influence by Cook’s distance of the data point, which is related to both the magnitude of its residual and its leverage, as measured by the hat (or influence) matrix. For GLMs we can define the Cook’s distance in the same manner: the Cook’s distance of data point <span class="math inline">\(k\)</span> is given by
<span class="math display">\[C_k = \frac{1}{(p+1)}\sum_{i=1}^n \frac{(\hat\mu_i^{[k]} - \hat\mu_i)^2}{\hat\phi V(\hat\mu_i)}; \text{ or}\]</span>
<span class="math display">\[C_k = \frac{(e^P_k)^2}{\hat\phi (p+1)}\frac{h_k}{(1-h_k)^2}\]</span>
where <span class="math inline">\(H = W^{1/2}X(X^\top W X)^{-1}X^\top W^{1/2}\)</span> is the hat matrix and <span class="math inline">\(h_k\)</span> is its <span class="math inline">\(k^{th}\)</span> diagonal entry. Large Cook’s distance implies the model predictions change substantially when the data point in question is removed.</p>
<div id="outlier-analysis-for-the-ceb-data" class="section level3" number="3.6.1">
<h3>
<span class="header-section-number">3.6.1</span> Outlier analysis for the CEB data<a class="anchor" aria-label="anchor" href="#outlier-analysis-for-the-ceb-data"><i class="fas fa-link"></i></a>
</h3>
<p>Typically data points with Cook’s distance <span class="math inline">\(&gt;1\)</span> are considered highly influential and the case for their exclusion is considerable. In this case, our apparent outlier is not influential, nor is it even the most influential observation sampled. <br><br></p>
<p>Below we have computed Cook’s distance “by hand” and by using the corresponding built-in R function; the only (very slight) difference between the two seems to be caused by the fact the built-in function uses the glm object, which we fitted after rounding the responses, whereas our “by hand” calculation uses Pearson residuals based on the original (unrounded) responses.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ceb2</span> <span class="op">&lt;-</span> <span class="va">ceb</span><span class="op">[</span><span class="op">-</span><span class="fl">17</span>,<span class="op">]</span></span>
<span><span class="va">my.glm2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">~</span><span class="va">dur</span><span class="op">+</span><span class="va">res</span><span class="op">+</span><span class="va">educ</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">poisson</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">'log'</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">ceb2</span>, offset <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">W2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">W</span><span class="op">)</span></span>
<span><span class="va">h</span> <span class="op">&lt;-</span> <span class="va">W2</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">W2</span></span>
<span></span>
<span><span class="op">(</span><span class="op">(</span><span class="va">pearson.resids</span><span class="op">[</span><span class="fl">17</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">/</span><span class="va">p</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="va">h</span><span class="op">[</span><span class="fl">17</span>,<span class="fl">17</span><span class="op">]</span><span class="op">/</span><span class="op">(</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">h</span><span class="op">[</span><span class="fl">17</span>,<span class="fl">17</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.0912897</code></pre>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">cooks.distance</a></span><span class="op">(</span><span class="va">my.glm</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cd</span><span class="op">[</span><span class="fl">17</span><span class="op">]</span></span></code></pre></div>
<pre><code>##         17 
## 0.09123744</code></pre>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="va">cd</span><span class="op">)</span></span></code></pre></div>
<pre><code>##           44           12           16           43           58           26 
## 0.0001578983 0.0002173653 0.0003181272 0.0004391595 0.0004872775 0.0008276467 
##           31           48           69            5           40           37 
## 0.0009604834 0.0010416260 0.0011600877 0.0011813567 0.0015364693 0.0017374273 
##           63           15           59           64            8           18 
## 0.0017913034 0.0020157684 0.0022231406 0.0023307859 0.0024333875 0.0027257717 
##            1           29           54           56           34           62 
## 0.0027795217 0.0028483882 0.0029171612 0.0029318194 0.0030296972 0.0030322783 
##           55           36           27           33            9           60 
## 0.0033869538 0.0034063422 0.0048780994 0.0049440311 0.0052402581 0.0053485413 
##           47            6           24            3           11           23 
## 0.0055965679 0.0056512722 0.0056712755 0.0057553959 0.0059235574 0.0061112712 
##           50           41           14           49            4            2 
## 0.0062842199 0.0063288779 0.0064608723 0.0067079644 0.0067366794 0.0068678049 
##           66           25           13            7           61           71 
## 0.0072889100 0.0074308461 0.0082351514 0.0104259702 0.0110105154 0.0114006236 
##           70           42           52           20           28           32 
## 0.0115384460 0.0145603375 0.0149361729 0.0164709118 0.0173145140 0.0189913643 
##           51           38           53           19           39           67 
## 0.0191422888 0.0203317679 0.0205784895 0.0220776790 0.0225077753 0.0265192825 
##           35           10           45           22           46           17 
## 0.0290870153 0.0375227510 0.0462157923 0.0771290592 0.0847041708 0.0912374357 
##           21           65           30           57 
## 0.1058278940 0.1102296604 0.1184297114 0.2448544534</code></pre>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Introduction</a></div>
<div class="next"><a href="linear-mixed-models.html"><span class="header-section-number">4</span> Linear Mixed Models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#poisson-regression"><span class="header-section-number">3</span> Poisson Regression</a></li>
<li><a class="nav-link" href="#children-ever-born-data"><span class="header-section-number">3.1</span> Children Ever Born Data</a></li>
<li><a class="nav-link" href="#defining-glms"><span class="header-section-number">3.2</span> Defining GLMs</a></li>
<li>
<a class="nav-link" href="#fitting-glms"><span class="header-section-number">3.3</span> Fitting GLMs</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#irls-for-the-ceb-data"><span class="header-section-number">3.3.1</span> IRLS for the CEB data</a></li></ul>
</li>
<li>
<a class="nav-link" href="#inference-on-glms"><span class="header-section-number">3.4</span> Inference on GLMs</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#inference-and-prediction-for-the-ceb-data-using-poisson-regression"><span class="header-section-number">3.4.1</span> Inference and prediction for the CEB data using Poisson regression</a></li></ul>
</li>
<li>
<a class="nav-link" href="#model-checkingdiagnostics"><span class="header-section-number">3.5</span> Model Checking/Diagnostics</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#residual-analysis-for-ceb-data"><span class="header-section-number">3.5.1</span> Residual analysis for CEB data</a></li></ul>
</li>
<li>
<a class="nav-link" href="#outlier-analysis-using-cooks-distance"><span class="header-section-number">3.6</span> Outlier analysis using Cook’s distance</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#outlier-analysis-for-the-ceb-data"><span class="header-section-number">3.6.1</span> Outlier analysis for the CEB data</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>GLMM</strong>" was written by Nick Syring. It was last built on 2023-02-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
